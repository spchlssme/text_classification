{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "#import itertools\n",
    "#from collections import Counter\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import learn\n",
    "import os\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# improvements\n",
    "# L2, dropout, CV, more layers, TF records\n",
    "# remove stuff that isn't being used\n",
    "# word embeddings... \n",
    "# data cleaning...\n",
    "# cleaner way to tun parameters...\n",
    "# talk about algors: CNNs, LSTMs, maybe fasttext\n",
    "# conditional autostop training...\n",
    "# check/prevent over fitting...\n",
    "# tensorboard(launch???)\n",
    "### dopeee i can watch training in real time with tensorboard!!!\n",
    "### 1515401822, loss curve shows location of over fit... 4200 step lowest loss before incline\n",
    "# post to github\n",
    "# update python code so it works with local file structure\n",
    "# remove flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can only run this block of code once due to flags...\n",
    "## better to use global variables vs FLAGS??\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"spooky_data_file\", \"./data/spooky_data/spooky.data.csv\", \"Data source for the spooky data.\")\n",
    "tf.flags.DEFINE_string(\"spooky_test_file\", \"./data/spooky_data/spooky.test.csv\", \"Data source for the spooky data.\")\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 100, \"Batch Size (default: 64)\")\n",
    "# 25 overshoots with batch 64, \n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20 , \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "tf.flags.DEFINE_boolean(\"eval_test\", True, \"Evaluate on all training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def load_test_data(spooky_test_file):\n",
    "    # reallllllly un organized\n",
    "    # Load data from files\n",
    "    spooky_data = pd.read_csv(spooky_test_file, skiprows=1, \n",
    "                              names=['id', 'text'])\n",
    "\n",
    "    test_examples = spooky_data['text']\n",
    "\n",
    "    datasets = dict()\n",
    "    datasets['data'] = test_examples\n",
    "\n",
    "    x_text = datasets['data']\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    #x_text = [clean_stop(sent) for sent in x_text]\n",
    "    # ids, odd transformations from dfs and dicts\n",
    "    ids = dict()\n",
    "    x_ids = spooky_data['id']\n",
    "    ids['data'] = x_ids\n",
    "\n",
    "    x_ids = ids['data']\n",
    "    return [x_text, x_ids]\n",
    "\n",
    "def get_datasets_spooky(spooky_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    spooky_data = pd.read_csv(spooky_data_file, skiprows=1, \n",
    "                              names=['id', 'text', 'author']).set_index('id')\n",
    "    # double stripping???\n",
    "    eap_examples = spooky_data[spooky_data['author'] == 'EAP']['text']\n",
    "    eap_examples = [s.strip() for s in eap_examples]\n",
    "    hpl_examples = spooky_data[spooky_data['author'] == 'HPL']['text']\n",
    "    hpl_examples = [s.strip() for s in hpl_examples]\n",
    "    mws_examples = spooky_data[spooky_data['author'] == 'MWS']['text']\n",
    "    mws_examples = [s.strip() for s in mws_examples]\n",
    "\n",
    "    datasets = dict()\n",
    "    datasets['data'] = eap_examples + hpl_examples + mws_examples\n",
    "    target = [0 for x in eap_examples] + [1 for x in hpl_examples] + [2 for x in mws_examples]\n",
    "    datasets['target'] = target\n",
    "    datasets['target_names'] = ['eap_examples', 'hpl_examples', 'mws_examples']\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_data_labels(datasets):\n",
    "    \"\"\"\n",
    "    Load data and labels\n",
    "    :param datasets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Split by words\n",
    "    x_text = datasets['data']\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    labels = []\n",
    "    for i in range(len(x_text)):\n",
    "        label = [0 for j in datasets['target_names']]\n",
    "        label[datasets['target'][i]] = 1\n",
    "        labels.append(label)\n",
    "    y = np.array(labels)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "#==================================================\n",
    "# added for probabilities\n",
    "#==================================================\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape((1, -1))\n",
    "    max_x = np.max(x, axis=1).reshape((-1, 1))\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_DIR=\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVAL_TEST=True\n",
      "EVAL_TRAIN=False\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=128\n",
      "SPOOKY_DATA_FILE=./data/spooky_data/spooky.data.csv\n",
      "SPOOKY_TEST_FILE=./data/spooky_data/spooky.test.csv\n",
      "\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Total number of test examples: 8392\n",
      "Loading data...\n",
      "Vocabulary Size: 25428\n",
      "Train/Dev split: 17622/1957\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# CHANGE THIS: Load data. Load your own data here\n",
    "if FLAGS.eval_test:\n",
    "    x_raw, x_ids = load_test_data(FLAGS.spooky_test_file)\n",
    "    print(\"Total number of test examples: {}\".format(len(x_raw)))\n",
    "else:\n",
    "    x_raw = [\"i wish i were\", \"an oscar meyer\"]\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "# x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "datasets = get_datasets_spooky(FLAGS.spooky_data_file)\n",
    "x_text, y = load_data_labels(datasets)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "\n",
    "        self.h_pool = tf.sqrt(tf.reduce_sum(tf.square(self.embedded_chars), 1))\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_pool, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            preds = tf.sigmoid(self.scores)\n",
    "            self.loss = tf.losses.log_loss(labels=self.input_y, predictions=preds)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745\n",
      "\n",
      "2018-01-08T04:45:46.181954: step 1, loss 5.69721, acc 0.25\n",
      "2018-01-08T04:45:46.319815: step 2, loss 4.04224, acc 0.35\n",
      "2018-01-08T04:45:46.462743: step 3, loss 3.45145, acc 0.36\n",
      "2018-01-08T04:45:46.608299: step 4, loss 3.30845, acc 0.28\n",
      "2018-01-08T04:45:46.746337: step 5, loss 2.51854, acc 0.27\n",
      "2018-01-08T04:45:46.890179: step 6, loss 1.81286, acc 0.25\n",
      "2018-01-08T04:45:47.032980: step 7, loss 1.42786, acc 0.36\n",
      "2018-01-08T04:45:47.180788: step 8, loss 1.32876, acc 0.32\n",
      "2018-01-08T04:45:47.324574: step 9, loss 1.16827, acc 0.28\n",
      "2018-01-08T04:45:47.466900: step 10, loss 0.982326, acc 0.46\n",
      "2018-01-08T04:45:47.608425: step 11, loss 1.22426, acc 0.41\n",
      "2018-01-08T04:45:47.741983: step 12, loss 1.47643, acc 0.42\n",
      "2018-01-08T04:45:47.881566: step 13, loss 1.77732, acc 0.35\n",
      "2018-01-08T04:45:48.020934: step 14, loss 1.57628, acc 0.44\n",
      "2018-01-08T04:45:48.163456: step 15, loss 1.38334, acc 0.42\n",
      "2018-01-08T04:45:48.305081: step 16, loss 1.29007, acc 0.39\n",
      "2018-01-08T04:45:48.448002: step 17, loss 1.0832, acc 0.41\n",
      "2018-01-08T04:45:48.591073: step 18, loss 1.13633, acc 0.41\n",
      "2018-01-08T04:45:48.739250: step 19, loss 0.972131, acc 0.37\n",
      "2018-01-08T04:45:48.881259: step 20, loss 0.920503, acc 0.35\n",
      "2018-01-08T04:45:49.014896: step 21, loss 0.821362, acc 0.38\n",
      "2018-01-08T04:45:49.157693: step 22, loss 0.817117, acc 0.32\n",
      "2018-01-08T04:45:49.311106: step 23, loss 0.775279, acc 0.41\n",
      "2018-01-08T04:45:49.452986: step 24, loss 0.839978, acc 0.29\n",
      "2018-01-08T04:45:49.597922: step 25, loss 0.901712, acc 0.29\n",
      "2018-01-08T04:45:49.736723: step 26, loss 0.908824, acc 0.29\n",
      "2018-01-08T04:45:49.893125: step 27, loss 0.842704, acc 0.28\n",
      "2018-01-08T04:45:50.046000: step 28, loss 0.804095, acc 0.29\n",
      "2018-01-08T04:45:50.222288: step 29, loss 0.728588, acc 0.28\n",
      "2018-01-08T04:45:50.380163: step 30, loss 0.656008, acc 0.5\n",
      "2018-01-08T04:45:50.524429: step 31, loss 0.709456, acc 0.39\n",
      "2018-01-08T04:45:50.670926: step 32, loss 0.755418, acc 0.35\n",
      "2018-01-08T04:45:50.831808: step 33, loss 0.663604, acc 0.45\n",
      "2018-01-08T04:45:50.979077: step 34, loss 0.785144, acc 0.35\n",
      "2018-01-08T04:45:51.128331: step 35, loss 0.739884, acc 0.36\n",
      "2018-01-08T04:45:51.264062: step 36, loss 0.69533, acc 0.4\n",
      "2018-01-08T04:45:51.414344: step 37, loss 0.739899, acc 0.34\n",
      "2018-01-08T04:45:51.568521: step 38, loss 0.680587, acc 0.41\n",
      "2018-01-08T04:45:51.718999: step 39, loss 0.684814, acc 0.4\n",
      "2018-01-08T04:45:51.865266: step 40, loss 0.687953, acc 0.39\n",
      "2018-01-08T04:45:52.010343: step 41, loss 0.645561, acc 0.4\n",
      "2018-01-08T04:45:52.149362: step 42, loss 0.644195, acc 0.43\n",
      "2018-01-08T04:45:52.287734: step 43, loss 0.651774, acc 0.34\n",
      "2018-01-08T04:45:52.427308: step 44, loss 0.659553, acc 0.35\n",
      "2018-01-08T04:45:52.575956: step 45, loss 0.691753, acc 0.29\n",
      "2018-01-08T04:45:52.717696: step 46, loss 0.668254, acc 0.38\n",
      "2018-01-08T04:45:52.859161: step 47, loss 0.665972, acc 0.38\n",
      "2018-01-08T04:45:52.996918: step 48, loss 0.632713, acc 0.46\n",
      "2018-01-08T04:45:53.136195: step 49, loss 0.627073, acc 0.42\n",
      "2018-01-08T04:45:53.273213: step 50, loss 0.691495, acc 0.34\n",
      "2018-01-08T04:45:53.422648: step 51, loss 0.665033, acc 0.37\n",
      "2018-01-08T04:45:53.567444: step 52, loss 0.638352, acc 0.45\n",
      "2018-01-08T04:45:53.718426: step 53, loss 0.640919, acc 0.43\n",
      "2018-01-08T04:45:53.862057: step 54, loss 0.656794, acc 0.39\n",
      "2018-01-08T04:45:54.000638: step 55, loss 0.603761, acc 0.54\n",
      "2018-01-08T04:45:54.142037: step 56, loss 0.660564, acc 0.34\n",
      "2018-01-08T04:45:54.276903: step 57, loss 0.663931, acc 0.29\n",
      "2018-01-08T04:45:54.420815: step 58, loss 0.629585, acc 0.42\n",
      "2018-01-08T04:45:54.560545: step 59, loss 0.623632, acc 0.46\n",
      "2018-01-08T04:45:54.703186: step 60, loss 0.697196, acc 0.35\n",
      "2018-01-08T04:45:54.847558: step 61, loss 0.628966, acc 0.42\n",
      "2018-01-08T04:45:54.992181: step 62, loss 0.65139, acc 0.48\n",
      "2018-01-08T04:45:55.134838: step 63, loss 0.642722, acc 0.43\n",
      "2018-01-08T04:45:55.266764: step 64, loss 0.631757, acc 0.45\n",
      "2018-01-08T04:45:55.404867: step 65, loss 0.667612, acc 0.33\n",
      "2018-01-08T04:45:55.540489: step 66, loss 0.663457, acc 0.29\n",
      "2018-01-08T04:45:55.682296: step 67, loss 0.639517, acc 0.38\n",
      "2018-01-08T04:45:55.817051: step 68, loss 0.637283, acc 0.37\n",
      "2018-01-08T04:45:55.956735: step 69, loss 0.629747, acc 0.44\n",
      "2018-01-08T04:45:56.100663: step 70, loss 0.650684, acc 0.41\n",
      "2018-01-08T04:45:56.243511: step 71, loss 0.647541, acc 0.46\n",
      "2018-01-08T04:45:56.387170: step 72, loss 0.626704, acc 0.42\n",
      "2018-01-08T04:45:56.521900: step 73, loss 0.639283, acc 0.35\n",
      "2018-01-08T04:45:56.663025: step 74, loss 0.638335, acc 0.38\n",
      "2018-01-08T04:45:56.808684: step 75, loss 0.627668, acc 0.42\n",
      "2018-01-08T04:45:56.952321: step 76, loss 0.635927, acc 0.38\n",
      "2018-01-08T04:45:57.105672: step 77, loss 0.625103, acc 0.4\n",
      "2018-01-08T04:45:57.247352: step 78, loss 0.591964, acc 0.54\n",
      "2018-01-08T04:45:57.392309: step 79, loss 0.609005, acc 0.5\n",
      "2018-01-08T04:45:57.525104: step 80, loss 0.608385, acc 0.48\n",
      "2018-01-08T04:45:57.670618: step 81, loss 0.634923, acc 0.39\n",
      "2018-01-08T04:45:57.808426: step 82, loss 0.622875, acc 0.47\n",
      "2018-01-08T04:45:57.944872: step 83, loss 0.616011, acc 0.44\n",
      "2018-01-08T04:45:58.088309: step 84, loss 0.63243, acc 0.33\n",
      "2018-01-08T04:45:58.223991: step 85, loss 0.621949, acc 0.38\n",
      "2018-01-08T04:45:58.363426: step 86, loss 0.618252, acc 0.43\n",
      "2018-01-08T04:45:58.499576: step 87, loss 0.603643, acc 0.5\n",
      "2018-01-08T04:45:58.641941: step 88, loss 0.607636, acc 0.52\n",
      "2018-01-08T04:45:58.799082: step 89, loss 0.624042, acc 0.43\n",
      "2018-01-08T04:45:58.940014: step 90, loss 0.617376, acc 0.43\n",
      "2018-01-08T04:45:59.081346: step 91, loss 0.616402, acc 0.41\n",
      "2018-01-08T04:45:59.220019: step 92, loss 0.626272, acc 0.39\n",
      "2018-01-08T04:45:59.362099: step 93, loss 0.615571, acc 0.48\n",
      "2018-01-08T04:45:59.502593: step 94, loss 0.620093, acc 0.41\n",
      "2018-01-08T04:45:59.640597: step 95, loss 0.621068, acc 0.44\n",
      "2018-01-08T04:45:59.774979: step 96, loss 0.630089, acc 0.39\n",
      "2018-01-08T04:45:59.921265: step 97, loss 0.618438, acc 0.5\n",
      "2018-01-08T04:46:00.062793: step 98, loss 0.618683, acc 0.52\n",
      "2018-01-08T04:46:00.208182: step 99, loss 0.617131, acc 0.48\n",
      "2018-01-08T04:46:00.349326: step 100, loss 0.62988, acc 0.4\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:46:00.757131: step 100, loss 0.614977, acc 0.447113\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-100\n",
      "\n",
      "2018-01-08T04:46:00.956108: step 101, loss 0.598328, acc 0.53\n",
      "2018-01-08T04:46:01.113834: step 102, loss 0.61826, acc 0.44\n",
      "2018-01-08T04:46:01.256105: step 103, loss 0.585122, acc 0.53\n",
      "2018-01-08T04:46:01.390180: step 104, loss 0.621216, acc 0.41\n",
      "2018-01-08T04:46:01.527632: step 105, loss 0.614572, acc 0.45\n",
      "2018-01-08T04:46:01.668849: step 106, loss 0.620521, acc 0.43\n",
      "2018-01-08T04:46:01.811275: step 107, loss 0.620397, acc 0.4\n",
      "2018-01-08T04:46:01.951749: step 108, loss 0.614902, acc 0.46\n",
      "2018-01-08T04:46:02.092834: step 109, loss 0.601382, acc 0.51\n",
      "2018-01-08T04:46:02.239496: step 110, loss 0.634866, acc 0.43\n",
      "2018-01-08T04:46:02.384886: step 111, loss 0.63086, acc 0.37\n",
      "2018-01-08T04:46:02.528653: step 112, loss 0.620284, acc 0.4\n",
      "2018-01-08T04:46:02.669974: step 113, loss 0.597538, acc 0.49\n",
      "2018-01-08T04:46:02.813114: step 114, loss 0.592729, acc 0.48\n",
      "2018-01-08T04:46:02.951015: step 115, loss 0.63038, acc 0.45\n",
      "2018-01-08T04:46:03.093088: step 116, loss 0.620215, acc 0.45\n",
      "2018-01-08T04:46:03.241244: step 117, loss 0.627463, acc 0.4\n",
      "2018-01-08T04:46:03.375086: step 118, loss 0.633972, acc 0.39\n",
      "2018-01-08T04:46:03.513219: step 119, loss 0.627974, acc 0.4\n",
      "2018-01-08T04:46:03.654652: step 120, loss 0.59288, acc 0.57\n",
      "2018-01-08T04:46:03.793272: step 121, loss 0.619841, acc 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:46:03.935035: step 122, loss 0.632398, acc 0.42\n",
      "2018-01-08T04:46:04.074904: step 123, loss 0.622931, acc 0.39\n",
      "2018-01-08T04:46:04.214136: step 124, loss 0.602859, acc 0.47\n",
      "2018-01-08T04:46:04.353707: step 125, loss 0.594796, acc 0.57\n",
      "2018-01-08T04:46:04.495768: step 126, loss 0.612372, acc 0.42\n",
      "2018-01-08T04:46:04.642213: step 127, loss 0.5863, acc 0.53\n",
      "2018-01-08T04:46:04.782787: step 128, loss 0.610025, acc 0.43\n",
      "2018-01-08T04:46:04.920514: step 129, loss 0.608125, acc 0.4\n",
      "2018-01-08T04:46:05.061228: step 130, loss 0.617853, acc 0.39\n",
      "2018-01-08T04:46:05.197514: step 131, loss 0.609605, acc 0.53\n",
      "2018-01-08T04:46:05.339490: step 132, loss 0.603268, acc 0.58\n",
      "2018-01-08T04:46:05.476678: step 133, loss 0.6044, acc 0.51\n",
      "2018-01-08T04:46:05.619077: step 134, loss 0.613717, acc 0.49\n",
      "2018-01-08T04:46:05.764228: step 135, loss 0.605786, acc 0.45\n",
      "2018-01-08T04:46:05.909191: step 136, loss 0.607665, acc 0.49\n",
      "2018-01-08T04:46:06.054723: step 137, loss 0.600324, acc 0.42\n",
      "2018-01-08T04:46:06.203183: step 138, loss 0.59028, acc 0.49\n",
      "2018-01-08T04:46:06.342421: step 139, loss 0.630317, acc 0.38\n",
      "2018-01-08T04:46:06.479871: step 140, loss 0.612494, acc 0.39\n",
      "2018-01-08T04:46:06.619388: step 141, loss 0.597786, acc 0.43\n",
      "2018-01-08T04:46:06.761501: step 142, loss 0.592528, acc 0.59\n",
      "2018-01-08T04:46:06.899038: step 143, loss 0.608787, acc 0.59\n",
      "2018-01-08T04:46:07.034417: step 144, loss 0.619273, acc 0.52\n",
      "2018-01-08T04:46:07.176405: step 145, loss 0.607961, acc 0.48\n",
      "2018-01-08T04:46:07.314271: step 146, loss 0.601236, acc 0.57\n",
      "2018-01-08T04:46:07.450267: step 147, loss 0.597148, acc 0.47\n",
      "2018-01-08T04:46:07.588875: step 148, loss 0.612301, acc 0.39\n",
      "2018-01-08T04:46:07.727357: step 149, loss 0.610933, acc 0.41\n",
      "2018-01-08T04:46:07.874413: step 150, loss 0.594583, acc 0.51\n",
      "2018-01-08T04:46:08.017051: step 151, loss 0.591445, acc 0.51\n",
      "2018-01-08T04:46:08.159508: step 152, loss 0.610133, acc 0.45\n",
      "2018-01-08T04:46:08.296678: step 153, loss 0.607245, acc 0.49\n",
      "2018-01-08T04:46:08.435968: step 154, loss 0.592172, acc 0.47\n",
      "2018-01-08T04:46:08.583119: step 155, loss 0.600245, acc 0.49\n",
      "2018-01-08T04:46:08.725773: step 156, loss 0.59247, acc 0.53\n",
      "2018-01-08T04:46:08.874859: step 157, loss 0.603906, acc 0.47\n",
      "2018-01-08T04:46:09.041438: step 158, loss 0.609683, acc 0.44\n",
      "2018-01-08T04:46:09.180818: step 159, loss 0.599726, acc 0.52\n",
      "2018-01-08T04:46:09.325375: step 160, loss 0.600945, acc 0.52\n",
      "2018-01-08T04:46:09.463389: step 161, loss 0.599187, acc 0.51\n",
      "2018-01-08T04:46:09.603323: step 162, loss 0.577879, acc 0.62\n",
      "2018-01-08T04:46:09.744181: step 163, loss 0.612939, acc 0.41\n",
      "2018-01-08T04:46:09.885097: step 164, loss 0.582833, acc 0.56\n",
      "2018-01-08T04:46:10.026014: step 165, loss 0.584377, acc 0.57\n",
      "2018-01-08T04:46:10.170190: step 166, loss 0.628579, acc 0.45\n",
      "2018-01-08T04:46:10.307007: step 167, loss 0.608438, acc 0.47\n",
      "2018-01-08T04:46:10.449889: step 168, loss 0.596036, acc 0.55\n",
      "2018-01-08T04:46:10.592368: step 169, loss 0.598236, acc 0.56\n",
      "2018-01-08T04:46:10.738479: step 170, loss 0.596653, acc 0.63\n",
      "2018-01-08T04:46:10.884434: step 171, loss 0.59184, acc 0.61\n",
      "2018-01-08T04:46:11.026443: step 172, loss 0.585215, acc 0.57\n",
      "2018-01-08T04:46:11.166872: step 173, loss 0.598427, acc 0.51\n",
      "2018-01-08T04:46:11.304400: step 174, loss 0.616116, acc 0.42\n",
      "2018-01-08T04:46:11.439859: step 175, loss 0.594108, acc 0.48\n",
      "2018-01-08T04:46:11.576783: step 176, loss 0.580436, acc 0.53\n",
      "2018-01-08T04:46:11.664784: step 177, loss 0.584909, acc 0.545455\n",
      "2018-01-08T04:46:11.807812: step 178, loss 0.586771, acc 0.53\n",
      "2018-01-08T04:46:11.946359: step 179, loss 0.608592, acc 0.48\n",
      "2018-01-08T04:46:12.086929: step 180, loss 0.58883, acc 0.48\n",
      "2018-01-08T04:46:12.221545: step 181, loss 0.594542, acc 0.47\n",
      "2018-01-08T04:46:12.361866: step 182, loss 0.580536, acc 0.53\n",
      "2018-01-08T04:46:12.505190: step 183, loss 0.586504, acc 0.51\n",
      "2018-01-08T04:46:12.648829: step 184, loss 0.586575, acc 0.58\n",
      "2018-01-08T04:46:12.788447: step 185, loss 0.589519, acc 0.5\n",
      "2018-01-08T04:46:12.927531: step 186, loss 0.571074, acc 0.59\n",
      "2018-01-08T04:46:13.065111: step 187, loss 0.57346, acc 0.57\n",
      "2018-01-08T04:46:13.209104: step 188, loss 0.58122, acc 0.49\n",
      "2018-01-08T04:46:13.352918: step 189, loss 0.591256, acc 0.46\n",
      "2018-01-08T04:46:13.515420: step 190, loss 0.594348, acc 0.45\n",
      "2018-01-08T04:46:13.660997: step 191, loss 0.584455, acc 0.51\n",
      "2018-01-08T04:46:13.813785: step 192, loss 0.595201, acc 0.55\n",
      "2018-01-08T04:46:13.964296: step 193, loss 0.57671, acc 0.55\n",
      "2018-01-08T04:46:14.112343: step 194, loss 0.620076, acc 0.42\n",
      "2018-01-08T04:46:14.256132: step 195, loss 0.585357, acc 0.6\n",
      "2018-01-08T04:46:14.404658: step 196, loss 0.57941, acc 0.63\n",
      "2018-01-08T04:46:14.553009: step 197, loss 0.582069, acc 0.59\n",
      "2018-01-08T04:46:14.702089: step 198, loss 0.594477, acc 0.46\n",
      "2018-01-08T04:46:14.851817: step 199, loss 0.605452, acc 0.47\n",
      "2018-01-08T04:46:15.006786: step 200, loss 0.587803, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:46:15.299232: step 200, loss 0.588027, acc 0.519673\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-200\n",
      "\n",
      "2018-01-08T04:46:15.525242: step 201, loss 0.597357, acc 0.47\n",
      "2018-01-08T04:46:15.672206: step 202, loss 0.611908, acc 0.45\n",
      "2018-01-08T04:46:15.827660: step 203, loss 0.586482, acc 0.51\n",
      "2018-01-08T04:46:15.982836: step 204, loss 0.591331, acc 0.58\n",
      "2018-01-08T04:46:16.133587: step 205, loss 0.574984, acc 0.69\n",
      "2018-01-08T04:46:16.284796: step 206, loss 0.591335, acc 0.6\n",
      "2018-01-08T04:46:16.428893: step 207, loss 0.578158, acc 0.64\n",
      "2018-01-08T04:46:16.581701: step 208, loss 0.583525, acc 0.54\n",
      "2018-01-08T04:46:16.728545: step 209, loss 0.596679, acc 0.41\n",
      "2018-01-08T04:46:16.881548: step 210, loss 0.58061, acc 0.55\n",
      "2018-01-08T04:46:17.027653: step 211, loss 0.571267, acc 0.6\n",
      "2018-01-08T04:46:17.175025: step 212, loss 0.607187, acc 0.41\n",
      "2018-01-08T04:46:17.322824: step 213, loss 0.569574, acc 0.66\n",
      "2018-01-08T04:46:17.457458: step 214, loss 0.579188, acc 0.65\n",
      "2018-01-08T04:46:17.597339: step 215, loss 0.570795, acc 0.67\n",
      "2018-01-08T04:46:17.738839: step 216, loss 0.610782, acc 0.4\n",
      "2018-01-08T04:46:17.886269: step 217, loss 0.562707, acc 0.63\n",
      "2018-01-08T04:46:18.025265: step 218, loss 0.589316, acc 0.54\n",
      "2018-01-08T04:46:18.168695: step 219, loss 0.58445, acc 0.52\n",
      "2018-01-08T04:46:18.316107: step 220, loss 0.581567, acc 0.6\n",
      "2018-01-08T04:46:18.454358: step 221, loss 0.581257, acc 0.61\n",
      "2018-01-08T04:46:18.593621: step 222, loss 0.569566, acc 0.63\n",
      "2018-01-08T04:46:18.730890: step 223, loss 0.566536, acc 0.61\n",
      "2018-01-08T04:46:18.873047: step 224, loss 0.570954, acc 0.54\n",
      "2018-01-08T04:46:19.017905: step 225, loss 0.596463, acc 0.41\n",
      "2018-01-08T04:46:19.198858: step 226, loss 0.572336, acc 0.63\n",
      "2018-01-08T04:46:19.338463: step 227, loss 0.577369, acc 0.58\n",
      "2018-01-08T04:46:19.475984: step 228, loss 0.592499, acc 0.36\n",
      "2018-01-08T04:46:19.623752: step 229, loss 0.58774, acc 0.48\n",
      "2018-01-08T04:46:19.759979: step 230, loss 0.581923, acc 0.57\n",
      "2018-01-08T04:46:19.897733: step 231, loss 0.569236, acc 0.57\n",
      "2018-01-08T04:46:20.039050: step 232, loss 0.55255, acc 0.52\n",
      "2018-01-08T04:46:20.180582: step 233, loss 0.564539, acc 0.52\n",
      "2018-01-08T04:46:20.318454: step 234, loss 0.616376, acc 0.37\n",
      "2018-01-08T04:46:20.462365: step 235, loss 0.620592, acc 0.37\n",
      "2018-01-08T04:46:20.601721: step 236, loss 0.565677, acc 0.61\n",
      "2018-01-08T04:46:20.743722: step 237, loss 0.603646, acc 0.36\n",
      "2018-01-08T04:46:20.884654: step 238, loss 0.591503, acc 0.34\n",
      "2018-01-08T04:46:21.027373: step 239, loss 0.557418, acc 0.68\n",
      "2018-01-08T04:46:21.170365: step 240, loss 0.560632, acc 0.66\n",
      "2018-01-08T04:46:21.311150: step 241, loss 0.58218, acc 0.44\n",
      "2018-01-08T04:46:21.451978: step 242, loss 0.548203, acc 0.51\n",
      "2018-01-08T04:46:21.598101: step 243, loss 0.602121, acc 0.41\n",
      "2018-01-08T04:46:21.740851: step 244, loss 0.58065, acc 0.47\n",
      "2018-01-08T04:46:21.886801: step 245, loss 0.578849, acc 0.56\n",
      "2018-01-08T04:46:22.030622: step 246, loss 0.556756, acc 0.68\n",
      "2018-01-08T04:46:22.179620: step 247, loss 0.576845, acc 0.64\n",
      "2018-01-08T04:46:22.325875: step 248, loss 0.558766, acc 0.67\n",
      "2018-01-08T04:46:22.470342: step 249, loss 0.561838, acc 0.62\n",
      "2018-01-08T04:46:22.617947: step 250, loss 0.555099, acc 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:46:22.767441: step 251, loss 0.560789, acc 0.59\n",
      "2018-01-08T04:46:22.912142: step 252, loss 0.575114, acc 0.55\n",
      "2018-01-08T04:46:23.057435: step 253, loss 0.578513, acc 0.55\n",
      "2018-01-08T04:46:23.200711: step 254, loss 0.551244, acc 0.61\n",
      "2018-01-08T04:46:23.349969: step 255, loss 0.552101, acc 0.61\n",
      "2018-01-08T04:46:23.491577: step 256, loss 0.590507, acc 0.5\n",
      "2018-01-08T04:46:23.642796: step 257, loss 0.572668, acc 0.61\n",
      "2018-01-08T04:46:23.788714: step 258, loss 0.587411, acc 0.53\n",
      "2018-01-08T04:46:23.938556: step 259, loss 0.579235, acc 0.61\n",
      "2018-01-08T04:46:24.086053: step 260, loss 0.556055, acc 0.65\n",
      "2018-01-08T04:46:24.235319: step 261, loss 0.558251, acc 0.5\n",
      "2018-01-08T04:46:24.385869: step 262, loss 0.608281, acc 0.41\n",
      "2018-01-08T04:46:24.538797: step 263, loss 0.605321, acc 0.41\n",
      "2018-01-08T04:46:24.683963: step 264, loss 0.577528, acc 0.46\n",
      "2018-01-08T04:46:24.831555: step 265, loss 0.566051, acc 0.6\n",
      "2018-01-08T04:46:24.979331: step 266, loss 0.575111, acc 0.6\n",
      "2018-01-08T04:46:25.130455: step 267, loss 0.583264, acc 0.55\n",
      "2018-01-08T04:46:25.280060: step 268, loss 0.569224, acc 0.61\n",
      "2018-01-08T04:46:25.428881: step 269, loss 0.58119, acc 0.59\n",
      "2018-01-08T04:46:25.569616: step 270, loss 0.554924, acc 0.67\n",
      "2018-01-08T04:46:25.705554: step 271, loss 0.546237, acc 0.55\n",
      "2018-01-08T04:46:25.849067: step 272, loss 0.595365, acc 0.42\n",
      "2018-01-08T04:46:25.990841: step 273, loss 0.562615, acc 0.48\n",
      "2018-01-08T04:46:26.137245: step 274, loss 0.562062, acc 0.53\n",
      "2018-01-08T04:46:26.277364: step 275, loss 0.579008, acc 0.48\n",
      "2018-01-08T04:46:26.417874: step 276, loss 0.585022, acc 0.58\n",
      "2018-01-08T04:46:26.561551: step 277, loss 0.568355, acc 0.65\n",
      "2018-01-08T04:46:26.706771: step 278, loss 0.581936, acc 0.59\n",
      "2018-01-08T04:46:26.845135: step 279, loss 0.56918, acc 0.55\n",
      "2018-01-08T04:46:26.982396: step 280, loss 0.583093, acc 0.54\n",
      "2018-01-08T04:46:27.130859: step 281, loss 0.551341, acc 0.71\n",
      "2018-01-08T04:46:27.271174: step 282, loss 0.574283, acc 0.51\n",
      "2018-01-08T04:46:27.413627: step 283, loss 0.554515, acc 0.61\n",
      "2018-01-08T04:46:27.549286: step 284, loss 0.565225, acc 0.55\n",
      "2018-01-08T04:46:27.695650: step 285, loss 0.539567, acc 0.66\n",
      "2018-01-08T04:46:27.840676: step 286, loss 0.563222, acc 0.51\n",
      "2018-01-08T04:46:27.982701: step 287, loss 0.552805, acc 0.56\n",
      "2018-01-08T04:46:28.120097: step 288, loss 0.575195, acc 0.54\n",
      "2018-01-08T04:46:28.267316: step 289, loss 0.552792, acc 0.68\n",
      "2018-01-08T04:46:28.407393: step 290, loss 0.566491, acc 0.6\n",
      "2018-01-08T04:46:28.548215: step 291, loss 0.555157, acc 0.66\n",
      "2018-01-08T04:46:28.688902: step 292, loss 0.585675, acc 0.59\n",
      "2018-01-08T04:46:28.835499: step 293, loss 0.54436, acc 0.68\n",
      "2018-01-08T04:46:28.976847: step 294, loss 0.552165, acc 0.61\n",
      "2018-01-08T04:46:29.117395: step 295, loss 0.551534, acc 0.53\n",
      "2018-01-08T04:46:29.256900: step 296, loss 0.584782, acc 0.46\n",
      "2018-01-08T04:46:29.397757: step 297, loss 0.569063, acc 0.53\n",
      "2018-01-08T04:46:29.538051: step 298, loss 0.555618, acc 0.7\n",
      "2018-01-08T04:46:29.681567: step 299, loss 0.562497, acc 0.73\n",
      "2018-01-08T04:46:29.831245: step 300, loss 0.570272, acc 0.67\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:46:30.101728: step 300, loss 0.557689, acc 0.639244\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-300\n",
      "\n",
      "2018-01-08T04:46:30.302601: step 301, loss 0.571946, acc 0.55\n",
      "2018-01-08T04:46:30.445559: step 302, loss 0.531616, acc 0.68\n",
      "2018-01-08T04:46:30.595230: step 303, loss 0.549077, acc 0.58\n",
      "2018-01-08T04:46:30.739280: step 304, loss 0.543649, acc 0.62\n",
      "2018-01-08T04:46:30.891647: step 305, loss 0.568411, acc 0.53\n",
      "2018-01-08T04:46:31.040277: step 306, loss 0.58555, acc 0.44\n",
      "2018-01-08T04:46:31.190660: step 307, loss 0.537495, acc 0.73\n",
      "2018-01-08T04:46:31.339028: step 308, loss 0.564058, acc 0.61\n",
      "2018-01-08T04:46:31.489346: step 309, loss 0.560859, acc 0.74\n",
      "2018-01-08T04:46:31.630857: step 310, loss 0.539811, acc 0.73\n",
      "2018-01-08T04:46:31.774129: step 311, loss 0.551802, acc 0.67\n",
      "2018-01-08T04:46:31.921492: step 312, loss 0.549011, acc 0.54\n",
      "2018-01-08T04:46:32.071294: step 313, loss 0.555392, acc 0.48\n",
      "2018-01-08T04:46:32.216982: step 314, loss 0.543739, acc 0.56\n",
      "2018-01-08T04:46:32.366625: step 315, loss 0.579879, acc 0.48\n",
      "2018-01-08T04:46:32.515762: step 316, loss 0.532813, acc 0.66\n",
      "2018-01-08T04:46:32.669302: step 317, loss 0.545731, acc 0.67\n",
      "2018-01-08T04:46:32.815252: step 318, loss 0.527043, acc 0.66\n",
      "2018-01-08T04:46:32.965026: step 319, loss 0.571164, acc 0.59\n",
      "2018-01-08T04:46:33.110001: step 320, loss 0.567768, acc 0.56\n",
      "2018-01-08T04:46:33.254566: step 321, loss 0.56495, acc 0.58\n",
      "2018-01-08T04:46:33.400474: step 322, loss 0.56809, acc 0.56\n",
      "2018-01-08T04:46:33.620552: step 323, loss 0.543578, acc 0.67\n",
      "2018-01-08T04:46:33.843691: step 324, loss 0.596308, acc 0.47\n",
      "2018-01-08T04:46:34.070397: step 325, loss 0.56025, acc 0.6\n",
      "2018-01-08T04:46:34.293752: step 326, loss 0.589165, acc 0.44\n",
      "2018-01-08T04:46:34.523762: step 327, loss 0.548318, acc 0.57\n",
      "2018-01-08T04:46:34.749482: step 328, loss 0.549052, acc 0.63\n",
      "2018-01-08T04:46:34.980824: step 329, loss 0.581315, acc 0.51\n",
      "2018-01-08T04:46:35.206824: step 330, loss 0.568629, acc 0.58\n",
      "2018-01-08T04:46:35.424003: step 331, loss 0.541817, acc 0.7\n",
      "2018-01-08T04:46:35.641181: step 332, loss 0.556458, acc 0.6\n",
      "2018-01-08T04:46:35.876920: step 333, loss 0.576366, acc 0.52\n",
      "2018-01-08T04:46:36.098930: step 334, loss 0.540505, acc 0.63\n",
      "2018-01-08T04:46:36.320383: step 335, loss 0.550423, acc 0.55\n",
      "2018-01-08T04:46:36.550881: step 336, loss 0.553856, acc 0.58\n",
      "2018-01-08T04:46:36.778014: step 337, loss 0.568185, acc 0.61\n",
      "2018-01-08T04:46:37.008981: step 338, loss 0.559902, acc 0.58\n",
      "2018-01-08T04:46:37.234061: step 339, loss 0.532725, acc 0.76\n",
      "2018-01-08T04:46:37.443186: step 340, loss 0.545241, acc 0.65\n",
      "2018-01-08T04:46:37.605588: step 341, loss 0.558225, acc 0.61\n",
      "2018-01-08T04:46:37.763723: step 342, loss 0.566861, acc 0.53\n",
      "2018-01-08T04:46:37.928192: step 343, loss 0.571955, acc 0.5\n",
      "2018-01-08T04:46:38.092516: step 344, loss 0.53968, acc 0.59\n",
      "2018-01-08T04:46:38.244241: step 345, loss 0.54828, acc 0.64\n",
      "2018-01-08T04:46:38.397900: step 346, loss 0.540454, acc 0.69\n",
      "2018-01-08T04:46:38.551039: step 347, loss 0.540305, acc 0.69\n",
      "2018-01-08T04:46:38.708942: step 348, loss 0.54957, acc 0.71\n",
      "2018-01-08T04:46:38.862806: step 349, loss 0.553611, acc 0.67\n",
      "2018-01-08T04:46:39.022240: step 350, loss 0.556624, acc 0.68\n",
      "2018-01-08T04:46:39.187151: step 351, loss 0.547768, acc 0.66\n",
      "2018-01-08T04:46:39.348360: step 352, loss 0.53394, acc 0.62\n",
      "2018-01-08T04:46:39.501869: step 353, loss 0.579291, acc 0.47\n",
      "2018-01-08T04:46:39.598555: step 354, loss 0.604294, acc 0.5\n",
      "2018-01-08T04:46:39.761187: step 355, loss 0.536729, acc 0.58\n",
      "2018-01-08T04:46:39.922562: step 356, loss 0.530747, acc 0.68\n",
      "2018-01-08T04:46:40.081226: step 357, loss 0.520404, acc 0.8\n",
      "2018-01-08T04:46:40.239569: step 358, loss 0.542196, acc 0.67\n",
      "2018-01-08T04:46:40.397675: step 359, loss 0.534102, acc 0.74\n",
      "2018-01-08T04:46:40.561823: step 360, loss 0.554681, acc 0.66\n",
      "2018-01-08T04:46:40.722923: step 361, loss 0.550284, acc 0.62\n",
      "2018-01-08T04:46:40.888082: step 362, loss 0.535606, acc 0.61\n",
      "2018-01-08T04:46:41.044170: step 363, loss 0.539008, acc 0.63\n",
      "2018-01-08T04:46:41.200768: step 364, loss 0.55364, acc 0.58\n",
      "2018-01-08T04:46:41.356171: step 365, loss 0.559388, acc 0.52\n",
      "2018-01-08T04:46:41.500534: step 366, loss 0.542096, acc 0.66\n",
      "2018-01-08T04:46:41.645053: step 367, loss 0.546351, acc 0.68\n",
      "2018-01-08T04:46:41.792768: step 368, loss 0.5279, acc 0.71\n",
      "2018-01-08T04:46:41.942000: step 369, loss 0.534535, acc 0.72\n",
      "2018-01-08T04:46:42.087618: step 370, loss 0.524354, acc 0.77\n",
      "2018-01-08T04:46:42.232512: step 371, loss 0.543062, acc 0.7\n",
      "2018-01-08T04:46:42.374321: step 372, loss 0.546177, acc 0.54\n",
      "2018-01-08T04:46:42.523364: step 373, loss 0.545766, acc 0.55\n",
      "2018-01-08T04:46:42.675703: step 374, loss 0.537978, acc 0.57\n",
      "2018-01-08T04:46:42.827240: step 375, loss 0.515472, acc 0.66\n",
      "2018-01-08T04:46:42.974286: step 376, loss 0.513594, acc 0.68\n",
      "2018-01-08T04:46:43.119841: step 377, loss 0.553756, acc 0.59\n",
      "2018-01-08T04:46:43.267286: step 378, loss 0.538287, acc 0.73\n",
      "2018-01-08T04:46:43.412887: step 379, loss 0.537319, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:46:43.561390: step 380, loss 0.503616, acc 0.8\n",
      "2018-01-08T04:46:43.707624: step 381, loss 0.524239, acc 0.65\n",
      "2018-01-08T04:46:43.852118: step 382, loss 0.525132, acc 0.51\n",
      "2018-01-08T04:46:43.999737: step 383, loss 0.586978, acc 0.44\n",
      "2018-01-08T04:46:44.147179: step 384, loss 0.52007, acc 0.57\n",
      "2018-01-08T04:46:44.292687: step 385, loss 0.527412, acc 0.67\n",
      "2018-01-08T04:46:44.446764: step 386, loss 0.527858, acc 0.67\n",
      "2018-01-08T04:46:44.590572: step 387, loss 0.519164, acc 0.77\n",
      "2018-01-08T04:46:44.741969: step 388, loss 0.519517, acc 0.8\n",
      "2018-01-08T04:46:44.893596: step 389, loss 0.528122, acc 0.66\n",
      "2018-01-08T04:46:45.043983: step 390, loss 0.524354, acc 0.62\n",
      "2018-01-08T04:46:45.190007: step 391, loss 0.507787, acc 0.76\n",
      "2018-01-08T04:46:45.339199: step 392, loss 0.540341, acc 0.63\n",
      "2018-01-08T04:46:45.478157: step 393, loss 0.518898, acc 0.71\n",
      "2018-01-08T04:46:45.617566: step 394, loss 0.534054, acc 0.63\n",
      "2018-01-08T04:46:45.762099: step 395, loss 0.525567, acc 0.71\n",
      "2018-01-08T04:46:45.900251: step 396, loss 0.525738, acc 0.67\n",
      "2018-01-08T04:46:46.035592: step 397, loss 0.535982, acc 0.55\n",
      "2018-01-08T04:46:46.178302: step 398, loss 0.540303, acc 0.54\n",
      "2018-01-08T04:46:46.318106: step 399, loss 0.521627, acc 0.6\n",
      "2018-01-08T04:46:46.457404: step 400, loss 0.545592, acc 0.57\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:46:46.716220: step 400, loss 0.538434, acc 0.658661\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-400\n",
      "\n",
      "2018-01-08T04:46:46.905779: step 401, loss 0.539145, acc 0.71\n",
      "2018-01-08T04:46:47.052240: step 402, loss 0.539641, acc 0.7\n",
      "2018-01-08T04:46:47.199289: step 403, loss 0.532876, acc 0.73\n",
      "2018-01-08T04:46:47.339683: step 404, loss 0.543269, acc 0.6\n",
      "2018-01-08T04:46:47.478617: step 405, loss 0.545665, acc 0.71\n",
      "2018-01-08T04:46:47.621158: step 406, loss 0.535593, acc 0.63\n",
      "2018-01-08T04:46:47.759042: step 407, loss 0.566659, acc 0.52\n",
      "2018-01-08T04:46:47.901693: step 408, loss 0.523872, acc 0.66\n",
      "2018-01-08T04:46:48.042601: step 409, loss 0.552848, acc 0.56\n",
      "2018-01-08T04:46:48.186144: step 410, loss 0.5422, acc 0.6\n",
      "2018-01-08T04:46:48.328114: step 411, loss 0.512974, acc 0.79\n",
      "2018-01-08T04:46:48.469902: step 412, loss 0.528065, acc 0.76\n",
      "2018-01-08T04:46:48.618430: step 413, loss 0.54189, acc 0.67\n",
      "2018-01-08T04:46:48.762150: step 414, loss 0.530122, acc 0.65\n",
      "2018-01-08T04:46:48.901172: step 415, loss 0.529305, acc 0.59\n",
      "2018-01-08T04:46:49.041007: step 416, loss 0.535295, acc 0.63\n",
      "2018-01-08T04:46:49.174170: step 417, loss 0.552817, acc 0.58\n",
      "2018-01-08T04:46:49.313566: step 418, loss 0.522954, acc 0.72\n",
      "2018-01-08T04:46:49.455328: step 419, loss 0.533626, acc 0.71\n",
      "2018-01-08T04:46:49.597551: step 420, loss 0.521588, acc 0.67\n",
      "2018-01-08T04:46:49.750654: step 421, loss 0.506773, acc 0.62\n",
      "2018-01-08T04:46:49.904530: step 422, loss 0.534693, acc 0.52\n",
      "2018-01-08T04:46:50.052340: step 423, loss 0.522452, acc 0.56\n",
      "2018-01-08T04:46:50.198343: step 424, loss 0.553386, acc 0.55\n",
      "2018-01-08T04:46:50.344515: step 425, loss 0.540651, acc 0.63\n",
      "2018-01-08T04:46:50.494592: step 426, loss 0.530418, acc 0.72\n",
      "2018-01-08T04:46:50.644698: step 427, loss 0.512377, acc 0.81\n",
      "2018-01-08T04:46:50.787350: step 428, loss 0.535496, acc 0.65\n",
      "2018-01-08T04:46:50.934980: step 429, loss 0.515884, acc 0.64\n",
      "2018-01-08T04:46:51.082756: step 430, loss 0.51853, acc 0.6\n",
      "2018-01-08T04:46:51.231361: step 431, loss 0.523282, acc 0.65\n",
      "2018-01-08T04:46:51.374311: step 432, loss 0.51275, acc 0.64\n",
      "2018-01-08T04:46:51.518902: step 433, loss 0.544448, acc 0.49\n",
      "2018-01-08T04:46:51.667323: step 434, loss 0.511603, acc 0.63\n",
      "2018-01-08T04:46:51.812460: step 435, loss 0.517948, acc 0.7\n",
      "2018-01-08T04:46:51.957167: step 436, loss 0.516095, acc 0.8\n",
      "2018-01-08T04:46:52.104587: step 437, loss 0.529706, acc 0.7\n",
      "2018-01-08T04:46:52.245332: step 438, loss 0.518368, acc 0.76\n",
      "2018-01-08T04:46:52.396878: step 439, loss 0.538929, acc 0.77\n",
      "2018-01-08T04:46:52.539171: step 440, loss 0.515052, acc 0.71\n",
      "2018-01-08T04:46:52.681907: step 441, loss 0.498367, acc 0.71\n",
      "2018-01-08T04:46:52.827930: step 442, loss 0.507759, acc 0.61\n",
      "2018-01-08T04:46:52.976880: step 443, loss 0.492127, acc 0.65\n",
      "2018-01-08T04:46:53.120953: step 444, loss 0.5398, acc 0.51\n",
      "2018-01-08T04:46:53.271673: step 445, loss 0.514677, acc 0.65\n",
      "2018-01-08T04:46:53.418268: step 446, loss 0.517796, acc 0.65\n",
      "2018-01-08T04:46:53.554731: step 447, loss 0.51647, acc 0.72\n",
      "2018-01-08T04:46:53.700206: step 448, loss 0.531542, acc 0.74\n",
      "2018-01-08T04:46:53.839704: step 449, loss 0.54741, acc 0.68\n",
      "2018-01-08T04:46:53.980276: step 450, loss 0.500262, acc 0.74\n",
      "2018-01-08T04:46:54.125155: step 451, loss 0.520885, acc 0.55\n",
      "2018-01-08T04:46:54.266676: step 452, loss 0.544124, acc 0.46\n",
      "2018-01-08T04:46:54.413492: step 453, loss 0.553655, acc 0.45\n",
      "2018-01-08T04:46:54.553166: step 454, loss 0.535493, acc 0.62\n",
      "2018-01-08T04:46:54.687331: step 455, loss 0.502054, acc 0.8\n",
      "2018-01-08T04:46:54.830995: step 456, loss 0.514852, acc 0.71\n",
      "2018-01-08T04:46:54.973216: step 457, loss 0.531047, acc 0.72\n",
      "2018-01-08T04:46:55.115978: step 458, loss 0.508995, acc 0.69\n",
      "2018-01-08T04:46:55.254105: step 459, loss 0.545541, acc 0.59\n",
      "2018-01-08T04:46:55.396616: step 460, loss 0.504019, acc 0.74\n",
      "2018-01-08T04:46:55.535118: step 461, loss 0.514435, acc 0.73\n",
      "2018-01-08T04:46:55.677991: step 462, loss 0.513159, acc 0.73\n",
      "2018-01-08T04:46:55.818810: step 463, loss 0.491098, acc 0.72\n",
      "2018-01-08T04:46:55.958789: step 464, loss 0.504296, acc 0.58\n",
      "2018-01-08T04:46:56.099100: step 465, loss 0.576995, acc 0.47\n",
      "2018-01-08T04:46:56.246662: step 466, loss 0.546277, acc 0.52\n",
      "2018-01-08T04:46:56.387822: step 467, loss 0.504567, acc 0.71\n",
      "2018-01-08T04:46:56.530437: step 468, loss 0.505667, acc 0.76\n",
      "2018-01-08T04:46:56.673326: step 469, loss 0.496434, acc 0.71\n",
      "2018-01-08T04:46:56.817090: step 470, loss 0.526138, acc 0.65\n",
      "2018-01-08T04:46:56.957743: step 471, loss 0.513097, acc 0.79\n",
      "2018-01-08T04:46:57.097604: step 472, loss 0.507049, acc 0.71\n",
      "2018-01-08T04:46:57.243779: step 473, loss 0.516482, acc 0.59\n",
      "2018-01-08T04:46:57.385324: step 474, loss 0.532876, acc 0.51\n",
      "2018-01-08T04:46:57.529904: step 475, loss 0.503747, acc 0.6\n",
      "2018-01-08T04:46:57.675421: step 476, loss 0.522252, acc 0.59\n",
      "2018-01-08T04:46:57.822865: step 477, loss 0.509699, acc 0.69\n",
      "2018-01-08T04:46:57.971772: step 478, loss 0.497423, acc 0.73\n",
      "2018-01-08T04:46:58.122108: step 479, loss 0.545378, acc 0.61\n",
      "2018-01-08T04:46:58.275714: step 480, loss 0.512242, acc 0.77\n",
      "2018-01-08T04:46:58.418834: step 481, loss 0.511422, acc 0.71\n",
      "2018-01-08T04:46:58.582223: step 482, loss 0.530669, acc 0.66\n",
      "2018-01-08T04:46:58.736055: step 483, loss 0.518068, acc 0.6\n",
      "2018-01-08T04:46:58.899108: step 484, loss 0.486487, acc 0.68\n",
      "2018-01-08T04:46:59.050975: step 485, loss 0.52245, acc 0.68\n",
      "2018-01-08T04:46:59.203797: step 486, loss 0.497852, acc 0.69\n",
      "2018-01-08T04:46:59.349420: step 487, loss 0.499037, acc 0.7\n",
      "2018-01-08T04:46:59.488773: step 488, loss 0.496822, acc 0.72\n",
      "2018-01-08T04:46:59.630463: step 489, loss 0.507057, acc 0.64\n",
      "2018-01-08T04:46:59.775974: step 490, loss 0.502502, acc 0.69\n",
      "2018-01-08T04:46:59.924102: step 491, loss 0.503005, acc 0.72\n",
      "2018-01-08T04:47:00.087411: step 492, loss 0.487188, acc 0.79\n",
      "2018-01-08T04:47:00.236631: step 493, loss 0.511511, acc 0.75\n",
      "2018-01-08T04:47:00.378229: step 494, loss 0.500429, acc 0.75\n",
      "2018-01-08T04:47:00.523083: step 495, loss 0.530896, acc 0.6\n",
      "2018-01-08T04:47:00.669316: step 496, loss 0.495308, acc 0.65\n",
      "2018-01-08T04:47:00.821625: step 497, loss 0.543772, acc 0.53\n",
      "2018-01-08T04:47:00.970332: step 498, loss 0.508628, acc 0.62\n",
      "2018-01-08T04:47:01.116372: step 499, loss 0.499976, acc 0.74\n",
      "2018-01-08T04:47:01.259557: step 500, loss 0.537958, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:47:01.545892: step 500, loss 0.519565, acc 0.698518\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-500\n",
      "\n",
      "2018-01-08T04:47:01.743313: step 501, loss 0.495295, acc 0.78\n",
      "2018-01-08T04:47:01.888169: step 502, loss 0.492662, acc 0.76\n",
      "2018-01-08T04:47:02.030622: step 503, loss 0.524023, acc 0.56\n",
      "2018-01-08T04:47:02.175189: step 504, loss 0.518362, acc 0.59\n",
      "2018-01-08T04:47:02.312017: step 505, loss 0.471172, acc 0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:47:02.451942: step 506, loss 0.499791, acc 0.58\n",
      "2018-01-08T04:47:02.593502: step 507, loss 0.48927, acc 0.66\n",
      "2018-01-08T04:47:02.734753: step 508, loss 0.495199, acc 0.78\n",
      "2018-01-08T04:47:02.873227: step 509, loss 0.506723, acc 0.72\n",
      "2018-01-08T04:47:03.014942: step 510, loss 0.515711, acc 0.76\n",
      "2018-01-08T04:47:03.164246: step 511, loss 0.501933, acc 0.76\n",
      "2018-01-08T04:47:03.300970: step 512, loss 0.493809, acc 0.73\n",
      "2018-01-08T04:47:03.436861: step 513, loss 0.521367, acc 0.56\n",
      "2018-01-08T04:47:03.582372: step 514, loss 0.499555, acc 0.61\n",
      "2018-01-08T04:47:03.727855: step 515, loss 0.508694, acc 0.61\n",
      "2018-01-08T04:47:03.867769: step 516, loss 0.508263, acc 0.63\n",
      "2018-01-08T04:47:04.013115: step 517, loss 0.521915, acc 0.7\n",
      "2018-01-08T04:47:04.159015: step 518, loss 0.49776, acc 0.74\n",
      "2018-01-08T04:47:04.302243: step 519, loss 0.504728, acc 0.77\n",
      "2018-01-08T04:47:04.443133: step 520, loss 0.481818, acc 0.81\n",
      "2018-01-08T04:47:04.590306: step 521, loss 0.508093, acc 0.64\n",
      "2018-01-08T04:47:04.729550: step 522, loss 0.506392, acc 0.63\n",
      "2018-01-08T04:47:04.873024: step 523, loss 0.530278, acc 0.6\n",
      "2018-01-08T04:47:05.008658: step 524, loss 0.500776, acc 0.6\n",
      "2018-01-08T04:47:05.148100: step 525, loss 0.512741, acc 0.56\n",
      "2018-01-08T04:47:05.286641: step 526, loss 0.511948, acc 0.69\n",
      "2018-01-08T04:47:05.424503: step 527, loss 0.509373, acc 0.69\n",
      "2018-01-08T04:47:05.569122: step 528, loss 0.535069, acc 0.65\n",
      "2018-01-08T04:47:05.712578: step 529, loss 0.506571, acc 0.78\n",
      "2018-01-08T04:47:05.861110: step 530, loss 0.484701, acc 0.74\n",
      "2018-01-08T04:47:05.952299: step 531, loss 0.528712, acc 0.545455\n",
      "2018-01-08T04:47:06.099309: step 532, loss 0.525694, acc 0.59\n",
      "2018-01-08T04:47:06.240966: step 533, loss 0.50854, acc 0.58\n",
      "2018-01-08T04:47:06.390428: step 534, loss 0.507319, acc 0.57\n",
      "2018-01-08T04:47:06.534636: step 535, loss 0.48787, acc 0.65\n",
      "2018-01-08T04:47:06.681735: step 536, loss 0.504542, acc 0.71\n",
      "2018-01-08T04:47:06.832831: step 537, loss 0.510842, acc 0.72\n",
      "2018-01-08T04:47:06.974381: step 538, loss 0.51368, acc 0.69\n",
      "2018-01-08T04:47:07.126309: step 539, loss 0.54646, acc 0.61\n",
      "2018-01-08T04:47:07.271373: step 540, loss 0.493205, acc 0.76\n",
      "2018-01-08T04:47:07.410378: step 541, loss 0.464518, acc 0.71\n",
      "2018-01-08T04:47:07.560257: step 542, loss 0.550753, acc 0.47\n",
      "2018-01-08T04:47:07.710466: step 543, loss 0.515362, acc 0.59\n",
      "2018-01-08T04:47:07.859042: step 544, loss 0.467848, acc 0.77\n",
      "2018-01-08T04:47:08.001108: step 545, loss 0.46487, acc 0.79\n",
      "2018-01-08T04:47:08.149406: step 546, loss 0.453383, acc 0.84\n",
      "2018-01-08T04:47:08.295130: step 547, loss 0.516158, acc 0.59\n",
      "2018-01-08T04:47:08.443894: step 548, loss 0.48874, acc 0.73\n",
      "2018-01-08T04:47:08.584510: step 549, loss 0.473487, acc 0.77\n",
      "2018-01-08T04:47:08.730563: step 550, loss 0.480025, acc 0.75\n",
      "2018-01-08T04:47:08.882486: step 551, loss 0.495194, acc 0.65\n",
      "2018-01-08T04:47:09.025988: step 552, loss 0.497988, acc 0.71\n",
      "2018-01-08T04:47:09.173040: step 553, loss 0.493187, acc 0.73\n",
      "2018-01-08T04:47:09.311286: step 554, loss 0.481224, acc 0.76\n",
      "2018-01-08T04:47:09.461680: step 555, loss 0.476874, acc 0.71\n",
      "2018-01-08T04:47:09.681558: step 556, loss 0.465722, acc 0.7\n",
      "2018-01-08T04:47:09.907044: step 557, loss 0.467581, acc 0.73\n",
      "2018-01-08T04:47:10.136046: step 558, loss 0.489982, acc 0.78\n",
      "2018-01-08T04:47:10.375913: step 559, loss 0.487377, acc 0.72\n",
      "2018-01-08T04:47:10.606019: step 560, loss 0.520822, acc 0.55\n",
      "2018-01-08T04:47:10.833395: step 561, loss 0.526061, acc 0.62\n",
      "2018-01-08T04:47:11.065180: step 562, loss 0.487284, acc 0.69\n",
      "2018-01-08T04:47:11.287728: step 563, loss 0.50733, acc 0.65\n",
      "2018-01-08T04:47:11.503593: step 564, loss 0.514293, acc 0.64\n",
      "2018-01-08T04:47:11.726580: step 565, loss 0.501428, acc 0.63\n",
      "2018-01-08T04:47:11.962542: step 566, loss 0.474552, acc 0.8\n",
      "2018-01-08T04:47:12.192066: step 567, loss 0.481286, acc 0.73\n",
      "2018-01-08T04:47:12.422382: step 568, loss 0.487011, acc 0.69\n",
      "2018-01-08T04:47:12.655320: step 569, loss 0.525312, acc 0.6\n",
      "2018-01-08T04:47:12.883881: step 570, loss 0.471018, acc 0.73\n",
      "2018-01-08T04:47:13.112359: step 571, loss 0.466353, acc 0.81\n",
      "2018-01-08T04:47:13.348663: step 572, loss 0.493082, acc 0.68\n",
      "2018-01-08T04:47:13.535116: step 573, loss 0.514516, acc 0.65\n",
      "2018-01-08T04:47:13.701357: step 574, loss 0.471544, acc 0.66\n",
      "2018-01-08T04:47:13.858133: step 575, loss 0.464027, acc 0.77\n",
      "2018-01-08T04:47:14.016767: step 576, loss 0.473231, acc 0.67\n",
      "2018-01-08T04:47:14.175591: step 577, loss 0.494285, acc 0.65\n",
      "2018-01-08T04:47:14.330675: step 578, loss 0.472018, acc 0.75\n",
      "2018-01-08T04:47:14.499295: step 579, loss 0.480971, acc 0.73\n",
      "2018-01-08T04:47:14.657398: step 580, loss 0.484524, acc 0.72\n",
      "2018-01-08T04:47:14.819813: step 581, loss 0.465344, acc 0.72\n",
      "2018-01-08T04:47:14.989942: step 582, loss 0.474762, acc 0.74\n",
      "2018-01-08T04:47:15.150439: step 583, loss 0.477642, acc 0.68\n",
      "2018-01-08T04:47:15.314388: step 584, loss 0.456841, acc 0.73\n",
      "2018-01-08T04:47:15.476864: step 585, loss 0.459669, acc 0.8\n",
      "2018-01-08T04:47:15.637469: step 586, loss 0.48268, acc 0.76\n",
      "2018-01-08T04:47:15.795913: step 587, loss 0.452629, acc 0.78\n",
      "2018-01-08T04:47:15.954244: step 588, loss 0.511207, acc 0.6\n",
      "2018-01-08T04:47:16.107534: step 589, loss 0.489661, acc 0.73\n",
      "2018-01-08T04:47:16.263440: step 590, loss 0.485533, acc 0.77\n",
      "2018-01-08T04:47:16.420567: step 591, loss 0.484147, acc 0.75\n",
      "2018-01-08T04:47:16.574540: step 592, loss 0.472954, acc 0.77\n",
      "2018-01-08T04:47:16.729057: step 593, loss 0.483481, acc 0.72\n",
      "2018-01-08T04:47:16.892396: step 594, loss 0.46078, acc 0.76\n",
      "2018-01-08T04:47:17.053607: step 595, loss 0.453107, acc 0.73\n",
      "2018-01-08T04:47:17.213078: step 596, loss 0.523626, acc 0.55\n",
      "2018-01-08T04:47:17.372750: step 597, loss 0.49712, acc 0.66\n",
      "2018-01-08T04:47:17.523625: step 598, loss 0.46936, acc 0.8\n",
      "2018-01-08T04:47:17.672750: step 599, loss 0.484197, acc 0.74\n",
      "2018-01-08T04:47:17.822495: step 600, loss 0.503164, acc 0.63\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:47:18.090753: step 600, loss 0.484265, acc 0.715381\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-600\n",
      "\n",
      "2018-01-08T04:47:18.354624: step 601, loss 0.46475, acc 0.78\n",
      "2018-01-08T04:47:18.498419: step 602, loss 0.488269, acc 0.69\n",
      "2018-01-08T04:47:18.645317: step 603, loss 0.458723, acc 0.69\n",
      "2018-01-08T04:47:18.793345: step 604, loss 0.449381, acc 0.79\n",
      "2018-01-08T04:47:18.942212: step 605, loss 0.467102, acc 0.7\n",
      "2018-01-08T04:47:19.086888: step 606, loss 0.475973, acc 0.75\n",
      "2018-01-08T04:47:19.231306: step 607, loss 0.473671, acc 0.74\n",
      "2018-01-08T04:47:19.369559: step 608, loss 0.463899, acc 0.78\n",
      "2018-01-08T04:47:19.515126: step 609, loss 0.456687, acc 0.8\n",
      "2018-01-08T04:47:19.657934: step 610, loss 0.474145, acc 0.84\n",
      "2018-01-08T04:47:19.806119: step 611, loss 0.458572, acc 0.8\n",
      "2018-01-08T04:47:19.948150: step 612, loss 0.447737, acc 0.77\n",
      "2018-01-08T04:47:20.096835: step 613, loss 0.493988, acc 0.64\n",
      "2018-01-08T04:47:20.240018: step 614, loss 0.442162, acc 0.67\n",
      "2018-01-08T04:47:20.381754: step 615, loss 0.48189, acc 0.7\n",
      "2018-01-08T04:47:20.523661: step 616, loss 0.477393, acc 0.67\n",
      "2018-01-08T04:47:20.671086: step 617, loss 0.471902, acc 0.76\n",
      "2018-01-08T04:47:20.813191: step 618, loss 0.448615, acc 0.79\n",
      "2018-01-08T04:47:20.957941: step 619, loss 0.481141, acc 0.78\n",
      "2018-01-08T04:47:21.107539: step 620, loss 0.457797, acc 0.84\n",
      "2018-01-08T04:47:21.254094: step 621, loss 0.47011, acc 0.76\n",
      "2018-01-08T04:47:21.401694: step 622, loss 0.468919, acc 0.8\n",
      "2018-01-08T04:47:21.537158: step 623, loss 0.447843, acc 0.79\n",
      "2018-01-08T04:47:21.679330: step 624, loss 0.487988, acc 0.64\n",
      "2018-01-08T04:47:21.822462: step 625, loss 0.449134, acc 0.76\n",
      "2018-01-08T04:47:21.961905: step 626, loss 0.474985, acc 0.72\n",
      "2018-01-08T04:47:22.100162: step 627, loss 0.434831, acc 0.85\n",
      "2018-01-08T04:47:22.237234: step 628, loss 0.483406, acc 0.79\n",
      "2018-01-08T04:47:22.374878: step 629, loss 0.484734, acc 0.74\n",
      "2018-01-08T04:47:22.515361: step 630, loss 0.45844, acc 0.85\n",
      "2018-01-08T04:47:22.652002: step 631, loss 0.473425, acc 0.72\n",
      "2018-01-08T04:47:22.784684: step 632, loss 0.463696, acc 0.67\n",
      "2018-01-08T04:47:22.925302: step 633, loss 0.465124, acc 0.72\n",
      "2018-01-08T04:47:23.065735: step 634, loss 0.452117, acc 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:47:23.209442: step 635, loss 0.465462, acc 0.68\n",
      "2018-01-08T04:47:23.342939: step 636, loss 0.480499, acc 0.7\n",
      "2018-01-08T04:47:23.473895: step 637, loss 0.464867, acc 0.74\n",
      "2018-01-08T04:47:23.614692: step 638, loss 0.457405, acc 0.76\n",
      "2018-01-08T04:47:23.752774: step 639, loss 0.456526, acc 0.8\n",
      "2018-01-08T04:47:23.893902: step 640, loss 0.476772, acc 0.73\n",
      "2018-01-08T04:47:24.032968: step 641, loss 0.50612, acc 0.64\n",
      "2018-01-08T04:47:24.174100: step 642, loss 0.479767, acc 0.67\n",
      "2018-01-08T04:47:24.316288: step 643, loss 0.468818, acc 0.69\n",
      "2018-01-08T04:47:24.455682: step 644, loss 0.470211, acc 0.73\n",
      "2018-01-08T04:47:24.602336: step 645, loss 0.458803, acc 0.68\n",
      "2018-01-08T04:47:24.742698: step 646, loss 0.516362, acc 0.61\n",
      "2018-01-08T04:47:24.887103: step 647, loss 0.484212, acc 0.75\n",
      "2018-01-08T04:47:25.022764: step 648, loss 0.451415, acc 0.77\n",
      "2018-01-08T04:47:25.168540: step 649, loss 0.438035, acc 0.78\n",
      "2018-01-08T04:47:25.311616: step 650, loss 0.468396, acc 0.69\n",
      "2018-01-08T04:47:25.443636: step 651, loss 0.461256, acc 0.71\n",
      "2018-01-08T04:47:25.586175: step 652, loss 0.468715, acc 0.68\n",
      "2018-01-08T04:47:25.739314: step 653, loss 0.464262, acc 0.7\n",
      "2018-01-08T04:47:25.884569: step 654, loss 0.459911, acc 0.8\n",
      "2018-01-08T04:47:26.031412: step 655, loss 0.465525, acc 0.78\n",
      "2018-01-08T04:47:26.174864: step 656, loss 0.454646, acc 0.76\n",
      "2018-01-08T04:47:26.320197: step 657, loss 0.460666, acc 0.81\n",
      "2018-01-08T04:47:26.462995: step 658, loss 0.450966, acc 0.7\n",
      "2018-01-08T04:47:26.613875: step 659, loss 0.500672, acc 0.53\n",
      "2018-01-08T04:47:26.757346: step 660, loss 0.523683, acc 0.5\n",
      "2018-01-08T04:47:26.908169: step 661, loss 0.445494, acc 0.74\n",
      "2018-01-08T04:47:27.055821: step 662, loss 0.460473, acc 0.76\n",
      "2018-01-08T04:47:27.205054: step 663, loss 0.449686, acc 0.8\n",
      "2018-01-08T04:47:27.352309: step 664, loss 0.484571, acc 0.74\n",
      "2018-01-08T04:47:27.492842: step 665, loss 0.45681, acc 0.87\n",
      "2018-01-08T04:47:27.638799: step 666, loss 0.453704, acc 0.73\n",
      "2018-01-08T04:47:27.781980: step 667, loss 0.510941, acc 0.6\n",
      "2018-01-08T04:47:27.934523: step 668, loss 0.468905, acc 0.67\n",
      "2018-01-08T04:47:28.079142: step 669, loss 0.469543, acc 0.7\n",
      "2018-01-08T04:47:28.222255: step 670, loss 0.456467, acc 0.71\n",
      "2018-01-08T04:47:28.369832: step 671, loss 0.477249, acc 0.74\n",
      "2018-01-08T04:47:28.511396: step 672, loss 0.441048, acc 0.83\n",
      "2018-01-08T04:47:28.659474: step 673, loss 0.450285, acc 0.76\n",
      "2018-01-08T04:47:28.807338: step 674, loss 0.471415, acc 0.69\n",
      "2018-01-08T04:47:28.954907: step 675, loss 0.46406, acc 0.79\n",
      "2018-01-08T04:47:29.105841: step 676, loss 0.464074, acc 0.78\n",
      "2018-01-08T04:47:29.247131: step 677, loss 0.463169, acc 0.73\n",
      "2018-01-08T04:47:29.393024: step 678, loss 0.456081, acc 0.76\n",
      "2018-01-08T04:47:29.609970: step 679, loss 0.441517, acc 0.79\n",
      "2018-01-08T04:47:29.844304: step 680, loss 0.472186, acc 0.77\n",
      "2018-01-08T04:47:30.079913: step 681, loss 0.467935, acc 0.74\n",
      "2018-01-08T04:47:30.308940: step 682, loss 0.44768, acc 0.81\n",
      "2018-01-08T04:47:30.536217: step 683, loss 0.429475, acc 0.85\n",
      "2018-01-08T04:47:30.762825: step 684, loss 0.464337, acc 0.67\n",
      "2018-01-08T04:47:30.988308: step 685, loss 0.433317, acc 0.75\n",
      "2018-01-08T04:47:31.213736: step 686, loss 0.439535, acc 0.73\n",
      "2018-01-08T04:47:31.436776: step 687, loss 0.428966, acc 0.75\n",
      "2018-01-08T04:47:31.654939: step 688, loss 0.470655, acc 0.71\n",
      "2018-01-08T04:47:31.884764: step 689, loss 0.455993, acc 0.68\n",
      "2018-01-08T04:47:32.122028: step 690, loss 0.484328, acc 0.64\n",
      "2018-01-08T04:47:32.340244: step 691, loss 0.488379, acc 0.7\n",
      "2018-01-08T04:47:32.570360: step 692, loss 0.479817, acc 0.69\n",
      "2018-01-08T04:47:32.794997: step 693, loss 0.473001, acc 0.71\n",
      "2018-01-08T04:47:33.017141: step 694, loss 0.452313, acc 0.77\n",
      "2018-01-08T04:47:33.236383: step 695, loss 0.413096, acc 0.78\n",
      "2018-01-08T04:47:33.453318: step 696, loss 0.493058, acc 0.64\n",
      "2018-01-08T04:47:33.592352: step 697, loss 0.474392, acc 0.69\n",
      "2018-01-08T04:47:33.740079: step 698, loss 0.53499, acc 0.5\n",
      "2018-01-08T04:47:33.880096: step 699, loss 0.479055, acc 0.67\n",
      "2018-01-08T04:47:34.021860: step 700, loss 0.4793, acc 0.73\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:47:34.287902: step 700, loss 0.489522, acc 0.66326\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-700\n",
      "\n",
      "2018-01-08T04:47:34.494035: step 701, loss 0.468801, acc 0.72\n",
      "2018-01-08T04:47:34.630887: step 702, loss 0.462303, acc 0.75\n",
      "2018-01-08T04:47:34.771970: step 703, loss 0.457063, acc 0.76\n",
      "2018-01-08T04:47:34.920531: step 704, loss 0.432246, acc 0.89\n",
      "2018-01-08T04:47:35.064420: step 705, loss 0.437365, acc 0.83\n",
      "2018-01-08T04:47:35.199470: step 706, loss 0.411152, acc 0.81\n",
      "2018-01-08T04:47:35.335885: step 707, loss 0.476502, acc 0.62\n",
      "2018-01-08T04:47:35.420008: step 708, loss 0.584275, acc 0.363636\n",
      "2018-01-08T04:47:35.571889: step 709, loss 0.469779, acc 0.67\n",
      "2018-01-08T04:47:35.711871: step 710, loss 0.469846, acc 0.72\n",
      "2018-01-08T04:47:35.862468: step 711, loss 0.49811, acc 0.57\n",
      "2018-01-08T04:47:35.999347: step 712, loss 0.484861, acc 0.57\n",
      "2018-01-08T04:47:36.147350: step 713, loss 0.43197, acc 0.81\n",
      "2018-01-08T04:47:36.289103: step 714, loss 0.42334, acc 0.81\n",
      "2018-01-08T04:47:36.428907: step 715, loss 0.416403, acc 0.73\n",
      "2018-01-08T04:47:36.571934: step 716, loss 0.44139, acc 0.67\n",
      "2018-01-08T04:47:36.712222: step 717, loss 0.444745, acc 0.66\n",
      "2018-01-08T04:47:36.847610: step 718, loss 0.455019, acc 0.68\n",
      "2018-01-08T04:47:36.996413: step 719, loss 0.415452, acc 0.78\n",
      "2018-01-08T04:47:37.135148: step 720, loss 0.428553, acc 0.78\n",
      "2018-01-08T04:47:37.281520: step 721, loss 0.454539, acc 0.81\n",
      "2018-01-08T04:47:37.419575: step 722, loss 0.428412, acc 0.86\n",
      "2018-01-08T04:47:37.568193: step 723, loss 0.447146, acc 0.83\n",
      "2018-01-08T04:47:37.716336: step 724, loss 0.429577, acc 0.76\n",
      "2018-01-08T04:47:37.866854: step 725, loss 0.451683, acc 0.74\n",
      "2018-01-08T04:47:38.012581: step 726, loss 0.432451, acc 0.79\n",
      "2018-01-08T04:47:38.158791: step 727, loss 0.453135, acc 0.74\n",
      "2018-01-08T04:47:38.308930: step 728, loss 0.435975, acc 0.72\n",
      "2018-01-08T04:47:38.451343: step 729, loss 0.452728, acc 0.66\n",
      "2018-01-08T04:47:38.596828: step 730, loss 0.41992, acc 0.7\n",
      "2018-01-08T04:47:38.743973: step 731, loss 0.383804, acc 0.88\n",
      "2018-01-08T04:47:38.890846: step 732, loss 0.44539, acc 0.75\n",
      "2018-01-08T04:47:39.037197: step 733, loss 0.426703, acc 0.82\n",
      "2018-01-08T04:47:39.178549: step 734, loss 0.435318, acc 0.84\n",
      "2018-01-08T04:47:39.325957: step 735, loss 0.436767, acc 0.83\n",
      "2018-01-08T04:47:39.463796: step 736, loss 0.454712, acc 0.75\n",
      "2018-01-08T04:47:39.614341: step 737, loss 0.421808, acc 0.78\n",
      "2018-01-08T04:47:39.756185: step 738, loss 0.397233, acc 0.8\n",
      "2018-01-08T04:47:39.902748: step 739, loss 0.44218, acc 0.68\n",
      "2018-01-08T04:47:40.048804: step 740, loss 0.452407, acc 0.66\n",
      "2018-01-08T04:47:40.195916: step 741, loss 0.449282, acc 0.7\n",
      "2018-01-08T04:47:40.340109: step 742, loss 0.439474, acc 0.79\n",
      "2018-01-08T04:47:40.491758: step 743, loss 0.452366, acc 0.72\n",
      "2018-01-08T04:47:40.644414: step 744, loss 0.444063, acc 0.79\n",
      "2018-01-08T04:47:40.793128: step 745, loss 0.460014, acc 0.74\n",
      "2018-01-08T04:47:40.933481: step 746, loss 0.423351, acc 0.82\n",
      "2018-01-08T04:47:41.077331: step 747, loss 0.444516, acc 0.72\n",
      "2018-01-08T04:47:41.222452: step 748, loss 0.463032, acc 0.67\n",
      "2018-01-08T04:47:41.360590: step 749, loss 0.417189, acc 0.73\n",
      "2018-01-08T04:47:41.542814: step 750, loss 0.413515, acc 0.79\n",
      "2018-01-08T04:47:41.781138: step 751, loss 0.461999, acc 0.7\n",
      "2018-01-08T04:47:42.013513: step 752, loss 0.460912, acc 0.72\n",
      "2018-01-08T04:47:42.248031: step 753, loss 0.429807, acc 0.83\n",
      "2018-01-08T04:47:42.473813: step 754, loss 0.445012, acc 0.79\n",
      "2018-01-08T04:47:42.709808: step 755, loss 0.457347, acc 0.74\n",
      "2018-01-08T04:47:42.939643: step 756, loss 0.437947, acc 0.8\n",
      "2018-01-08T04:47:43.175198: step 757, loss 0.444065, acc 0.73\n",
      "2018-01-08T04:47:43.393932: step 758, loss 0.446492, acc 0.7\n",
      "2018-01-08T04:47:43.621310: step 759, loss 0.454594, acc 0.71\n",
      "2018-01-08T04:47:43.846690: step 760, loss 0.455294, acc 0.7\n",
      "2018-01-08T04:47:44.066909: step 761, loss 0.432702, acc 0.78\n",
      "2018-01-08T04:47:44.293037: step 762, loss 0.432463, acc 0.74\n",
      "2018-01-08T04:47:44.511730: step 763, loss 0.436913, acc 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:47:44.739368: step 764, loss 0.424431, acc 0.8\n",
      "2018-01-08T04:47:44.968806: step 765, loss 0.464305, acc 0.76\n",
      "2018-01-08T04:47:45.190001: step 766, loss 0.460362, acc 0.74\n",
      "2018-01-08T04:47:45.412244: step 767, loss 0.449065, acc 0.72\n",
      "2018-01-08T04:47:45.587004: step 768, loss 0.446408, acc 0.74\n",
      "2018-01-08T04:47:45.728238: step 769, loss 0.435862, acc 0.79\n",
      "2018-01-08T04:47:45.867556: step 770, loss 0.413265, acc 0.8\n",
      "2018-01-08T04:47:46.013926: step 771, loss 0.44771, acc 0.71\n",
      "2018-01-08T04:47:46.151812: step 772, loss 0.439754, acc 0.77\n",
      "2018-01-08T04:47:46.289877: step 773, loss 0.430851, acc 0.76\n",
      "2018-01-08T04:47:46.428975: step 774, loss 0.440759, acc 0.68\n",
      "2018-01-08T04:47:46.569735: step 775, loss 0.436914, acc 0.69\n",
      "2018-01-08T04:47:46.713701: step 776, loss 0.453978, acc 0.79\n",
      "2018-01-08T04:47:46.853595: step 777, loss 0.456196, acc 0.77\n",
      "2018-01-08T04:47:46.991543: step 778, loss 0.427106, acc 0.86\n",
      "2018-01-08T04:47:47.129484: step 779, loss 0.41275, acc 0.82\n",
      "2018-01-08T04:47:47.268960: step 780, loss 0.457671, acc 0.72\n",
      "2018-01-08T04:47:47.405485: step 781, loss 0.427879, acc 0.76\n",
      "2018-01-08T04:47:47.544897: step 782, loss 0.428457, acc 0.72\n",
      "2018-01-08T04:47:47.682798: step 783, loss 0.460374, acc 0.65\n",
      "2018-01-08T04:47:47.822324: step 784, loss 0.407616, acc 0.81\n",
      "2018-01-08T04:47:47.959578: step 785, loss 0.445941, acc 0.72\n",
      "2018-01-08T04:47:48.100470: step 786, loss 0.438642, acc 0.76\n",
      "2018-01-08T04:47:48.244518: step 787, loss 0.437512, acc 0.82\n",
      "2018-01-08T04:47:48.384368: step 788, loss 0.403739, acc 0.87\n",
      "2018-01-08T04:47:48.517736: step 789, loss 0.472922, acc 0.65\n",
      "2018-01-08T04:47:48.660832: step 790, loss 0.454415, acc 0.67\n",
      "2018-01-08T04:47:48.803150: step 791, loss 0.462208, acc 0.67\n",
      "2018-01-08T04:47:48.954728: step 792, loss 0.444757, acc 0.73\n",
      "2018-01-08T04:47:49.105768: step 793, loss 0.439311, acc 0.79\n",
      "2018-01-08T04:47:49.244856: step 794, loss 0.427443, acc 0.8\n",
      "2018-01-08T04:47:49.380613: step 795, loss 0.501085, acc 0.6\n",
      "2018-01-08T04:47:49.523104: step 796, loss 0.42603, acc 0.81\n",
      "2018-01-08T04:47:49.670399: step 797, loss 0.407242, acc 0.83\n",
      "2018-01-08T04:47:49.813694: step 798, loss 0.434491, acc 0.71\n",
      "2018-01-08T04:47:49.958929: step 799, loss 0.409566, acc 0.79\n",
      "2018-01-08T04:47:50.105532: step 800, loss 0.434342, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:47:50.383072: step 800, loss 0.448455, acc 0.696985\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-800\n",
      "\n",
      "2018-01-08T04:47:50.595600: step 801, loss 0.422779, acc 0.69\n",
      "2018-01-08T04:47:50.741762: step 802, loss 0.433572, acc 0.68\n",
      "2018-01-08T04:47:50.882333: step 803, loss 0.424823, acc 0.67\n",
      "2018-01-08T04:47:51.031012: step 804, loss 0.427069, acc 0.76\n",
      "2018-01-08T04:47:51.179531: step 805, loss 0.451749, acc 0.74\n",
      "2018-01-08T04:47:51.320713: step 806, loss 0.415405, acc 0.82\n",
      "2018-01-08T04:47:51.457959: step 807, loss 0.404353, acc 0.88\n",
      "2018-01-08T04:47:51.599905: step 808, loss 0.451832, acc 0.72\n",
      "2018-01-08T04:47:51.746381: step 809, loss 0.425582, acc 0.82\n",
      "2018-01-08T04:47:51.896873: step 810, loss 0.423588, acc 0.81\n",
      "2018-01-08T04:47:52.037327: step 811, loss 0.396824, acc 0.77\n",
      "2018-01-08T04:47:52.182569: step 812, loss 0.41278, acc 0.76\n",
      "2018-01-08T04:47:52.331652: step 813, loss 0.459023, acc 0.66\n",
      "2018-01-08T04:47:52.477509: step 814, loss 0.411549, acc 0.8\n",
      "2018-01-08T04:47:52.630747: step 815, loss 0.423222, acc 0.8\n",
      "2018-01-08T04:47:52.776684: step 816, loss 0.417289, acc 0.83\n",
      "2018-01-08T04:47:52.929569: step 817, loss 0.426535, acc 0.78\n",
      "2018-01-08T04:47:53.079255: step 818, loss 0.400821, acc 0.85\n",
      "2018-01-08T04:47:53.229334: step 819, loss 0.410912, acc 0.77\n",
      "2018-01-08T04:47:53.376977: step 820, loss 0.368032, acc 0.84\n",
      "2018-01-08T04:47:53.567214: step 821, loss 0.451537, acc 0.72\n",
      "2018-01-08T04:47:53.798050: step 822, loss 0.457813, acc 0.6\n",
      "2018-01-08T04:47:54.027755: step 823, loss 0.399368, acc 0.84\n",
      "2018-01-08T04:47:54.252317: step 824, loss 0.401534, acc 0.81\n",
      "2018-01-08T04:47:54.474590: step 825, loss 0.381622, acc 0.84\n",
      "2018-01-08T04:47:54.696242: step 826, loss 0.415462, acc 0.87\n",
      "2018-01-08T04:47:54.932335: step 827, loss 0.414057, acc 0.84\n",
      "2018-01-08T04:47:55.150353: step 828, loss 0.395251, acc 0.83\n",
      "2018-01-08T04:47:55.371182: step 829, loss 0.402941, acc 0.83\n",
      "2018-01-08T04:47:55.591727: step 830, loss 0.414127, acc 0.73\n",
      "2018-01-08T04:47:55.815673: step 831, loss 0.457156, acc 0.66\n",
      "2018-01-08T04:47:56.039862: step 832, loss 0.409292, acc 0.8\n",
      "2018-01-08T04:47:56.288013: step 833, loss 0.424193, acc 0.79\n",
      "2018-01-08T04:47:56.519079: step 834, loss 0.4436, acc 0.83\n",
      "2018-01-08T04:47:56.739978: step 835, loss 0.452449, acc 0.75\n",
      "2018-01-08T04:47:56.968839: step 836, loss 0.418505, acc 0.78\n",
      "2018-01-08T04:47:57.189701: step 837, loss 0.439697, acc 0.75\n",
      "2018-01-08T04:47:57.408794: step 838, loss 0.456367, acc 0.65\n",
      "2018-01-08T04:47:57.572843: step 839, loss 0.434138, acc 0.7\n",
      "2018-01-08T04:47:57.731603: step 840, loss 0.415211, acc 0.77\n",
      "2018-01-08T04:47:57.887995: step 841, loss 0.419349, acc 0.78\n",
      "2018-01-08T04:47:58.046225: step 842, loss 0.410851, acc 0.79\n",
      "2018-01-08T04:47:58.205727: step 843, loss 0.416188, acc 0.79\n",
      "2018-01-08T04:47:58.365292: step 844, loss 0.440065, acc 0.72\n",
      "2018-01-08T04:47:58.526347: step 845, loss 0.40413, acc 0.86\n",
      "2018-01-08T04:47:58.687333: step 846, loss 0.409276, acc 0.83\n",
      "2018-01-08T04:47:58.851779: step 847, loss 0.372767, acc 0.89\n",
      "2018-01-08T04:47:59.010779: step 848, loss 0.434211, acc 0.74\n",
      "2018-01-08T04:47:59.169944: step 849, loss 0.43608, acc 0.74\n",
      "2018-01-08T04:47:59.330091: step 850, loss 0.387751, acc 0.8\n",
      "2018-01-08T04:47:59.481753: step 851, loss 0.416572, acc 0.84\n",
      "2018-01-08T04:47:59.643370: step 852, loss 0.415436, acc 0.84\n",
      "2018-01-08T04:47:59.807243: step 853, loss 0.415107, acc 0.81\n",
      "2018-01-08T04:47:59.967734: step 854, loss 0.467464, acc 0.74\n",
      "2018-01-08T04:48:00.125082: step 855, loss 0.426186, acc 0.79\n",
      "2018-01-08T04:48:00.295036: step 856, loss 0.424022, acc 0.75\n",
      "2018-01-08T04:48:00.460897: step 857, loss 0.422194, acc 0.71\n",
      "2018-01-08T04:48:00.623789: step 858, loss 0.433938, acc 0.77\n",
      "2018-01-08T04:48:00.780612: step 859, loss 0.398184, acc 0.8\n",
      "2018-01-08T04:48:00.960334: step 860, loss 0.408112, acc 0.82\n",
      "2018-01-08T04:48:01.124277: step 861, loss 0.382295, acc 0.84\n",
      "2018-01-08T04:48:01.290005: step 862, loss 0.360989, acc 0.83\n",
      "2018-01-08T04:48:01.441122: step 863, loss 0.438184, acc 0.68\n",
      "2018-01-08T04:48:01.587944: step 864, loss 0.397954, acc 0.74\n",
      "2018-01-08T04:48:01.734677: step 865, loss 0.438018, acc 0.68\n",
      "2018-01-08T04:48:01.882659: step 866, loss 0.379643, acc 0.81\n",
      "2018-01-08T04:48:02.036021: step 867, loss 0.408523, acc 0.81\n",
      "2018-01-08T04:48:02.187130: step 868, loss 0.398513, acc 0.88\n",
      "2018-01-08T04:48:02.335292: step 869, loss 0.411213, acc 0.78\n",
      "2018-01-08T04:48:02.485831: step 870, loss 0.432662, acc 0.72\n",
      "2018-01-08T04:48:02.632231: step 871, loss 0.432272, acc 0.71\n",
      "2018-01-08T04:48:02.776105: step 872, loss 0.385066, acc 0.82\n",
      "2018-01-08T04:48:02.929109: step 873, loss 0.392119, acc 0.8\n",
      "2018-01-08T04:48:03.075278: step 874, loss 0.414419, acc 0.78\n",
      "2018-01-08T04:48:03.221067: step 875, loss 0.41151, acc 0.84\n",
      "2018-01-08T04:48:03.364086: step 876, loss 0.416764, acc 0.87\n",
      "2018-01-08T04:48:03.508439: step 877, loss 0.385522, acc 0.86\n",
      "2018-01-08T04:48:03.658545: step 878, loss 0.431369, acc 0.72\n",
      "2018-01-08T04:48:03.801707: step 879, loss 0.406141, acc 0.76\n",
      "2018-01-08T04:48:03.947734: step 880, loss 0.437109, acc 0.73\n",
      "2018-01-08T04:48:04.098655: step 881, loss 0.381927, acc 0.79\n",
      "2018-01-08T04:48:04.243983: step 882, loss 0.385142, acc 0.84\n",
      "2018-01-08T04:48:04.390416: step 883, loss 0.428446, acc 0.8\n",
      "2018-01-08T04:48:04.537581: step 884, loss 0.392161, acc 0.85\n",
      "2018-01-08T04:48:04.626423: step 885, loss 0.494422, acc 0.636364\n",
      "2018-01-08T04:48:04.775183: step 886, loss 0.36192, acc 0.87\n",
      "2018-01-08T04:48:04.923995: step 887, loss 0.422371, acc 0.75\n",
      "2018-01-08T04:48:05.069952: step 888, loss 0.465876, acc 0.61\n",
      "2018-01-08T04:48:05.213308: step 889, loss 0.38991, acc 0.78\n",
      "2018-01-08T04:48:05.357108: step 890, loss 0.414628, acc 0.8\n",
      "2018-01-08T04:48:05.495374: step 891, loss 0.38142, acc 0.83\n",
      "2018-01-08T04:48:05.662867: step 892, loss 0.383314, acc 0.9\n",
      "2018-01-08T04:48:05.821219: step 893, loss 0.456847, acc 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:48:05.983621: step 894, loss 0.415111, acc 0.83\n",
      "2018-01-08T04:48:06.145173: step 895, loss 0.376189, acc 0.82\n",
      "2018-01-08T04:48:06.304728: step 896, loss 0.428839, acc 0.68\n",
      "2018-01-08T04:48:06.463962: step 897, loss 0.408258, acc 0.78\n",
      "2018-01-08T04:48:06.625362: step 898, loss 0.405516, acc 0.71\n",
      "2018-01-08T04:48:06.784570: step 899, loss 0.367115, acc 0.81\n",
      "2018-01-08T04:48:06.943640: step 900, loss 0.4336, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:48:07.233941: step 900, loss 0.428393, acc 0.773633\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-900\n",
      "\n",
      "2018-01-08T04:48:07.457507: step 901, loss 0.377431, acc 0.84\n",
      "2018-01-08T04:48:07.617786: step 902, loss 0.41746, acc 0.8\n",
      "2018-01-08T04:48:07.782047: step 903, loss 0.393909, acc 0.84\n",
      "2018-01-08T04:48:07.940407: step 904, loss 0.38397, acc 0.85\n",
      "2018-01-08T04:48:08.107068: step 905, loss 0.403681, acc 0.76\n",
      "2018-01-08T04:48:08.265469: step 906, loss 0.401279, acc 0.73\n",
      "2018-01-08T04:48:08.428565: step 907, loss 0.405898, acc 0.77\n",
      "2018-01-08T04:48:08.590952: step 908, loss 0.441256, acc 0.7\n",
      "2018-01-08T04:48:08.749673: step 909, loss 0.383973, acc 0.84\n",
      "2018-01-08T04:48:08.912843: step 910, loss 0.41762, acc 0.76\n",
      "2018-01-08T04:48:09.074085: step 911, loss 0.377649, acc 0.84\n",
      "2018-01-08T04:48:09.240681: step 912, loss 0.401895, acc 0.77\n",
      "2018-01-08T04:48:09.401694: step 913, loss 0.400036, acc 0.78\n",
      "2018-01-08T04:48:09.550010: step 914, loss 0.443535, acc 0.73\n",
      "2018-01-08T04:48:09.695180: step 915, loss 0.405286, acc 0.83\n",
      "2018-01-08T04:48:09.839357: step 916, loss 0.366439, acc 0.86\n",
      "2018-01-08T04:48:09.985755: step 917, loss 0.386984, acc 0.78\n",
      "2018-01-08T04:48:10.133791: step 918, loss 0.387143, acc 0.77\n",
      "2018-01-08T04:48:10.283823: step 919, loss 0.413718, acc 0.71\n",
      "2018-01-08T04:48:10.428491: step 920, loss 0.438745, acc 0.69\n",
      "2018-01-08T04:48:10.577717: step 921, loss 0.405963, acc 0.75\n",
      "2018-01-08T04:48:10.724672: step 922, loss 0.382368, acc 0.84\n",
      "2018-01-08T04:48:10.875319: step 923, loss 0.426522, acc 0.78\n",
      "2018-01-08T04:48:11.017028: step 924, loss 0.386074, acc 0.88\n",
      "2018-01-08T04:48:11.166930: step 925, loss 0.41543, acc 0.81\n",
      "2018-01-08T04:48:11.311341: step 926, loss 0.364287, acc 0.85\n",
      "2018-01-08T04:48:11.455797: step 927, loss 0.382895, acc 0.77\n",
      "2018-01-08T04:48:11.604440: step 928, loss 0.45553, acc 0.69\n",
      "2018-01-08T04:48:11.753304: step 929, loss 0.386181, acc 0.8\n",
      "2018-01-08T04:48:11.900360: step 930, loss 0.407578, acc 0.74\n",
      "2018-01-08T04:48:12.039964: step 931, loss 0.408802, acc 0.78\n",
      "2018-01-08T04:48:12.184820: step 932, loss 0.37121, acc 0.88\n",
      "2018-01-08T04:48:12.328336: step 933, loss 0.408694, acc 0.82\n",
      "2018-01-08T04:48:12.474070: step 934, loss 0.404748, acc 0.81\n",
      "2018-01-08T04:48:12.647768: step 935, loss 0.422998, acc 0.77\n",
      "2018-01-08T04:48:12.804512: step 936, loss 0.418374, acc 0.71\n",
      "2018-01-08T04:48:12.949927: step 937, loss 0.361442, acc 0.84\n",
      "2018-01-08T04:48:13.095307: step 938, loss 0.370437, acc 0.78\n",
      "2018-01-08T04:48:13.241647: step 939, loss 0.370689, acc 0.76\n",
      "2018-01-08T04:48:13.383456: step 940, loss 0.381717, acc 0.84\n",
      "2018-01-08T04:48:13.535236: step 941, loss 0.40839, acc 0.83\n",
      "2018-01-08T04:48:13.698695: step 942, loss 0.423737, acc 0.77\n",
      "2018-01-08T04:48:13.864088: step 943, loss 0.419553, acc 0.81\n",
      "2018-01-08T04:48:14.020546: step 944, loss 0.425226, acc 0.8\n",
      "2018-01-08T04:48:14.177615: step 945, loss 0.460739, acc 0.69\n",
      "2018-01-08T04:48:14.338489: step 946, loss 0.407587, acc 0.73\n",
      "2018-01-08T04:48:14.498401: step 947, loss 0.371154, acc 0.87\n",
      "2018-01-08T04:48:14.665128: step 948, loss 0.396869, acc 0.84\n",
      "2018-01-08T04:48:14.831843: step 949, loss 0.370145, acc 0.87\n",
      "2018-01-08T04:48:14.989756: step 950, loss 0.38259, acc 0.87\n",
      "2018-01-08T04:48:15.151215: step 951, loss 0.385215, acc 0.85\n",
      "2018-01-08T04:48:15.311315: step 952, loss 0.391675, acc 0.79\n",
      "2018-01-08T04:48:15.471878: step 953, loss 0.38186, acc 0.83\n",
      "2018-01-08T04:48:15.630609: step 954, loss 0.404305, acc 0.81\n",
      "2018-01-08T04:48:15.789732: step 955, loss 0.387452, acc 0.78\n",
      "2018-01-08T04:48:15.946262: step 956, loss 0.38964, acc 0.79\n",
      "2018-01-08T04:48:16.111891: step 957, loss 0.374412, acc 0.8\n",
      "2018-01-08T04:48:16.269322: step 958, loss 0.42528, acc 0.7\n",
      "2018-01-08T04:48:16.425354: step 959, loss 0.350349, acc 0.84\n",
      "2018-01-08T04:48:16.583363: step 960, loss 0.397675, acc 0.8\n",
      "2018-01-08T04:48:16.743129: step 961, loss 0.349953, acc 0.89\n",
      "2018-01-08T04:48:16.906151: step 962, loss 0.38075, acc 0.85\n",
      "2018-01-08T04:48:17.065361: step 963, loss 0.349476, acc 0.85\n",
      "2018-01-08T04:48:17.225712: step 964, loss 0.370418, acc 0.85\n",
      "2018-01-08T04:48:17.389982: step 965, loss 0.406482, acc 0.78\n",
      "2018-01-08T04:48:17.548683: step 966, loss 0.392461, acc 0.78\n",
      "2018-01-08T04:48:17.692431: step 967, loss 0.362622, acc 0.82\n",
      "2018-01-08T04:48:17.834457: step 968, loss 0.381335, acc 0.81\n",
      "2018-01-08T04:48:17.982669: step 969, loss 0.422283, acc 0.75\n",
      "2018-01-08T04:48:18.126935: step 970, loss 0.382576, acc 0.84\n",
      "2018-01-08T04:48:18.270932: step 971, loss 0.324067, acc 0.94\n",
      "2018-01-08T04:48:18.420148: step 972, loss 0.373401, acc 0.84\n",
      "2018-01-08T04:48:18.561931: step 973, loss 0.362172, acc 0.85\n",
      "2018-01-08T04:48:18.709497: step 974, loss 0.386609, acc 0.81\n",
      "2018-01-08T04:48:18.854399: step 975, loss 0.364921, acc 0.83\n",
      "2018-01-08T04:48:19.003576: step 976, loss 0.402511, acc 0.78\n",
      "2018-01-08T04:48:19.161645: step 977, loss 0.374478, acc 0.87\n",
      "2018-01-08T04:48:19.311062: step 978, loss 0.417231, acc 0.76\n",
      "2018-01-08T04:48:19.458979: step 979, loss 0.347205, acc 0.88\n",
      "2018-01-08T04:48:19.604790: step 980, loss 0.400001, acc 0.83\n",
      "2018-01-08T04:48:19.755973: step 981, loss 0.382727, acc 0.82\n",
      "2018-01-08T04:48:19.903248: step 982, loss 0.353446, acc 0.85\n",
      "2018-01-08T04:48:20.050578: step 983, loss 0.35621, acc 0.82\n",
      "2018-01-08T04:48:20.201229: step 984, loss 0.389595, acc 0.7\n",
      "2018-01-08T04:48:20.353980: step 985, loss 0.412567, acc 0.7\n",
      "2018-01-08T04:48:20.501121: step 986, loss 0.397573, acc 0.74\n",
      "2018-01-08T04:48:20.643810: step 987, loss 0.377739, acc 0.82\n",
      "2018-01-08T04:48:20.802740: step 988, loss 0.367223, acc 0.86\n",
      "2018-01-08T04:48:20.953054: step 989, loss 0.406921, acc 0.82\n",
      "2018-01-08T04:48:21.098061: step 990, loss 0.404871, acc 0.78\n",
      "2018-01-08T04:48:21.258354: step 991, loss 0.374901, acc 0.8\n",
      "2018-01-08T04:48:21.400838: step 992, loss 0.39579, acc 0.73\n",
      "2018-01-08T04:48:21.548561: step 993, loss 0.408987, acc 0.66\n",
      "2018-01-08T04:48:21.716674: step 994, loss 0.404301, acc 0.77\n",
      "2018-01-08T04:48:21.868230: step 995, loss 0.326367, acc 0.88\n",
      "2018-01-08T04:48:22.008796: step 996, loss 0.385939, acc 0.8\n",
      "2018-01-08T04:48:22.156839: step 997, loss 0.366132, acc 0.87\n",
      "2018-01-08T04:48:22.297812: step 998, loss 0.43481, acc 0.75\n",
      "2018-01-08T04:48:22.442330: step 999, loss 0.401112, acc 0.8\n",
      "2018-01-08T04:48:22.577102: step 1000, loss 0.380057, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:48:22.837883: step 1000, loss 0.409741, acc 0.74093\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1000\n",
      "\n",
      "2018-01-08T04:48:23.045793: step 1001, loss 0.338885, acc 0.81\n",
      "2018-01-08T04:48:23.190325: step 1002, loss 0.405232, acc 0.68\n",
      "2018-01-08T04:48:23.337368: step 1003, loss 0.404058, acc 0.7\n",
      "2018-01-08T04:48:23.478582: step 1004, loss 0.391881, acc 0.78\n",
      "2018-01-08T04:48:23.617983: step 1005, loss 0.364381, acc 0.86\n",
      "2018-01-08T04:48:23.761814: step 1006, loss 0.352173, acc 0.82\n",
      "2018-01-08T04:48:23.904191: step 1007, loss 0.396779, acc 0.77\n",
      "2018-01-08T04:48:24.039361: step 1008, loss 0.370832, acc 0.86\n",
      "2018-01-08T04:48:24.179820: step 1009, loss 0.394974, acc 0.78\n",
      "2018-01-08T04:48:24.316240: step 1010, loss 0.402773, acc 0.78\n",
      "2018-01-08T04:48:24.453318: step 1011, loss 0.36663, acc 0.81\n",
      "2018-01-08T04:48:24.594261: step 1012, loss 0.396675, acc 0.74\n",
      "2018-01-08T04:48:24.739471: step 1013, loss 0.393661, acc 0.77\n",
      "2018-01-08T04:48:24.880262: step 1014, loss 0.382346, acc 0.79\n",
      "2018-01-08T04:48:25.020212: step 1015, loss 0.385986, acc 0.79\n",
      "2018-01-08T04:48:25.165869: step 1016, loss 0.35442, acc 0.87\n",
      "2018-01-08T04:48:25.307267: step 1017, loss 0.390588, acc 0.8\n",
      "2018-01-08T04:48:25.446776: step 1018, loss 0.361524, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:48:25.582496: step 1019, loss 0.343881, acc 0.86\n",
      "2018-01-08T04:48:25.718960: step 1020, loss 0.341271, acc 0.82\n",
      "2018-01-08T04:48:25.869630: step 1021, loss 0.354109, acc 0.79\n",
      "2018-01-08T04:48:26.015127: step 1022, loss 0.367796, acc 0.77\n",
      "2018-01-08T04:48:26.160432: step 1023, loss 0.388208, acc 0.8\n",
      "2018-01-08T04:48:26.300235: step 1024, loss 0.389552, acc 0.78\n",
      "2018-01-08T04:48:26.449784: step 1025, loss 0.373529, acc 0.83\n",
      "2018-01-08T04:48:26.590766: step 1026, loss 0.387909, acc 0.82\n",
      "2018-01-08T04:48:26.736238: step 1027, loss 0.368004, acc 0.83\n",
      "2018-01-08T04:48:26.881208: step 1028, loss 0.353866, acc 0.86\n",
      "2018-01-08T04:48:27.021994: step 1029, loss 0.377943, acc 0.79\n",
      "2018-01-08T04:48:27.165966: step 1030, loss 0.423859, acc 0.67\n",
      "2018-01-08T04:48:27.308999: step 1031, loss 0.353095, acc 0.8\n",
      "2018-01-08T04:48:27.447306: step 1032, loss 0.359552, acc 0.88\n",
      "2018-01-08T04:48:27.583323: step 1033, loss 0.368737, acc 0.86\n",
      "2018-01-08T04:48:27.719063: step 1034, loss 0.419792, acc 0.7\n",
      "2018-01-08T04:48:27.857246: step 1035, loss 0.422232, acc 0.8\n",
      "2018-01-08T04:48:27.998393: step 1036, loss 0.351106, acc 0.88\n",
      "2018-01-08T04:48:28.143887: step 1037, loss 0.379932, acc 0.78\n",
      "2018-01-08T04:48:28.285840: step 1038, loss 0.357747, acc 0.8\n",
      "2018-01-08T04:48:28.420225: step 1039, loss 0.432946, acc 0.69\n",
      "2018-01-08T04:48:28.555345: step 1040, loss 0.402505, acc 0.75\n",
      "2018-01-08T04:48:28.693076: step 1041, loss 0.348176, acc 0.84\n",
      "2018-01-08T04:48:28.837796: step 1042, loss 0.377253, acc 0.82\n",
      "2018-01-08T04:48:28.978569: step 1043, loss 0.362452, acc 0.86\n",
      "2018-01-08T04:48:29.117614: step 1044, loss 0.390881, acc 0.81\n",
      "2018-01-08T04:48:29.255004: step 1045, loss 0.375048, acc 0.82\n",
      "2018-01-08T04:48:29.388953: step 1046, loss 0.36007, acc 0.84\n",
      "2018-01-08T04:48:29.529616: step 1047, loss 0.368812, acc 0.77\n",
      "2018-01-08T04:48:29.674360: step 1048, loss 0.412383, acc 0.7\n",
      "2018-01-08T04:48:29.826577: step 1049, loss 0.40377, acc 0.69\n",
      "2018-01-08T04:48:29.975276: step 1050, loss 0.382079, acc 0.82\n",
      "2018-01-08T04:48:30.127515: step 1051, loss 0.363831, acc 0.84\n",
      "2018-01-08T04:48:30.275626: step 1052, loss 0.382124, acc 0.79\n",
      "2018-01-08T04:48:30.422475: step 1053, loss 0.38786, acc 0.81\n",
      "2018-01-08T04:48:30.571002: step 1054, loss 0.327692, acc 0.88\n",
      "2018-01-08T04:48:30.719055: step 1055, loss 0.373982, acc 0.8\n",
      "2018-01-08T04:48:30.870937: step 1056, loss 0.399893, acc 0.71\n",
      "2018-01-08T04:48:31.015979: step 1057, loss 0.369663, acc 0.78\n",
      "2018-01-08T04:48:31.163743: step 1058, loss 0.377678, acc 0.83\n",
      "2018-01-08T04:48:31.306284: step 1059, loss 0.389921, acc 0.79\n",
      "2018-01-08T04:48:31.443731: step 1060, loss 0.387099, acc 0.8\n",
      "2018-01-08T04:48:31.586964: step 1061, loss 0.39239, acc 0.79\n",
      "2018-01-08T04:48:31.679910: step 1062, loss 0.374629, acc 0.772727\n",
      "2018-01-08T04:48:31.835714: step 1063, loss 0.390827, acc 0.82\n",
      "2018-01-08T04:48:31.987658: step 1064, loss 0.356844, acc 0.81\n",
      "2018-01-08T04:48:32.137173: step 1065, loss 0.378033, acc 0.78\n",
      "2018-01-08T04:48:32.287330: step 1066, loss 0.373763, acc 0.8\n",
      "2018-01-08T04:48:32.432538: step 1067, loss 0.332068, acc 0.88\n",
      "2018-01-08T04:48:32.581202: step 1068, loss 0.339462, acc 0.86\n",
      "2018-01-08T04:48:32.730190: step 1069, loss 0.338751, acc 0.87\n",
      "2018-01-08T04:48:32.876219: step 1070, loss 0.358993, acc 0.83\n",
      "2018-01-08T04:48:33.013599: step 1071, loss 0.39058, acc 0.8\n",
      "2018-01-08T04:48:33.155216: step 1072, loss 0.382416, acc 0.87\n",
      "2018-01-08T04:48:33.298226: step 1073, loss 0.3481, acc 0.87\n",
      "2018-01-08T04:48:33.440536: step 1074, loss 0.373357, acc 0.82\n",
      "2018-01-08T04:48:33.578451: step 1075, loss 0.348816, acc 0.78\n",
      "2018-01-08T04:48:33.714295: step 1076, loss 0.371831, acc 0.82\n",
      "2018-01-08T04:48:33.858972: step 1077, loss 0.36124, acc 0.76\n",
      "2018-01-08T04:48:34.002830: step 1078, loss 0.333976, acc 0.9\n",
      "2018-01-08T04:48:34.152083: step 1079, loss 0.346322, acc 0.84\n",
      "2018-01-08T04:48:34.297426: step 1080, loss 0.352435, acc 0.85\n",
      "2018-01-08T04:48:34.432398: step 1081, loss 0.362354, acc 0.86\n",
      "2018-01-08T04:48:34.569998: step 1082, loss 0.393826, acc 0.79\n",
      "2018-01-08T04:48:34.707357: step 1083, loss 0.345206, acc 0.84\n",
      "2018-01-08T04:48:34.849123: step 1084, loss 0.428836, acc 0.72\n",
      "2018-01-08T04:48:34.993393: step 1085, loss 0.392003, acc 0.75\n",
      "2018-01-08T04:48:35.131372: step 1086, loss 0.341889, acc 0.84\n",
      "2018-01-08T04:48:35.271494: step 1087, loss 0.310518, acc 0.89\n",
      "2018-01-08T04:48:35.403877: step 1088, loss 0.442802, acc 0.72\n",
      "2018-01-08T04:48:35.538459: step 1089, loss 0.346509, acc 0.88\n",
      "2018-01-08T04:48:35.680012: step 1090, loss 0.346043, acc 0.87\n",
      "2018-01-08T04:48:35.820909: step 1091, loss 0.386237, acc 0.85\n",
      "2018-01-08T04:48:35.958453: step 1092, loss 0.356358, acc 0.88\n",
      "2018-01-08T04:48:36.097648: step 1093, loss 0.372989, acc 0.84\n",
      "2018-01-08T04:48:36.239943: step 1094, loss 0.33181, acc 0.86\n",
      "2018-01-08T04:48:36.373713: step 1095, loss 0.341232, acc 0.8\n",
      "2018-01-08T04:48:36.517473: step 1096, loss 0.354372, acc 0.78\n",
      "2018-01-08T04:48:36.654446: step 1097, loss 0.321743, acc 0.82\n",
      "2018-01-08T04:48:36.795342: step 1098, loss 0.325088, acc 0.88\n",
      "2018-01-08T04:48:36.935381: step 1099, loss 0.363384, acc 0.86\n",
      "2018-01-08T04:48:37.077717: step 1100, loss 0.364671, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:48:37.331035: step 1100, loss 0.381364, acc 0.806847\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1100\n",
      "\n",
      "2018-01-08T04:48:37.528268: step 1101, loss 0.324854, acc 0.89\n",
      "2018-01-08T04:48:37.666358: step 1102, loss 0.35216, acc 0.88\n",
      "2018-01-08T04:48:37.813347: step 1103, loss 0.38722, acc 0.83\n",
      "2018-01-08T04:48:37.961712: step 1104, loss 0.351849, acc 0.85\n",
      "2018-01-08T04:48:38.109115: step 1105, loss 0.372019, acc 0.77\n",
      "2018-01-08T04:48:38.248297: step 1106, loss 0.389903, acc 0.77\n",
      "2018-01-08T04:48:38.392910: step 1107, loss 0.339815, acc 0.86\n",
      "2018-01-08T04:48:38.535633: step 1108, loss 0.322581, acc 0.89\n",
      "2018-01-08T04:48:38.674118: step 1109, loss 0.344111, acc 0.8\n",
      "2018-01-08T04:48:38.818736: step 1110, loss 0.303129, acc 0.89\n",
      "2018-01-08T04:48:38.957620: step 1111, loss 0.344818, acc 0.86\n",
      "2018-01-08T04:48:39.099102: step 1112, loss 0.303658, acc 0.94\n",
      "2018-01-08T04:48:39.238997: step 1113, loss 0.37477, acc 0.85\n",
      "2018-01-08T04:48:39.375159: step 1114, loss 0.33904, acc 0.86\n",
      "2018-01-08T04:48:39.513589: step 1115, loss 0.327503, acc 0.86\n",
      "2018-01-08T04:48:39.648441: step 1116, loss 0.318607, acc 0.85\n",
      "2018-01-08T04:48:39.787017: step 1117, loss 0.3584, acc 0.78\n",
      "2018-01-08T04:48:39.933740: step 1118, loss 0.400446, acc 0.75\n",
      "2018-01-08T04:48:40.071069: step 1119, loss 0.310887, acc 0.86\n",
      "2018-01-08T04:48:40.222932: step 1120, loss 0.352032, acc 0.82\n",
      "2018-01-08T04:48:40.358675: step 1121, loss 0.372544, acc 0.85\n",
      "2018-01-08T04:48:40.492601: step 1122, loss 0.391556, acc 0.83\n",
      "2018-01-08T04:48:40.631921: step 1123, loss 0.343917, acc 0.81\n",
      "2018-01-08T04:48:40.778683: step 1124, loss 0.346437, acc 0.85\n",
      "2018-01-08T04:48:40.920987: step 1125, loss 0.359417, acc 0.79\n",
      "2018-01-08T04:48:41.072565: step 1126, loss 0.393515, acc 0.73\n",
      "2018-01-08T04:48:41.209925: step 1127, loss 0.360887, acc 0.81\n",
      "2018-01-08T04:48:41.346716: step 1128, loss 0.317561, acc 0.87\n",
      "2018-01-08T04:48:41.486194: step 1129, loss 0.359109, acc 0.88\n",
      "2018-01-08T04:48:41.622345: step 1130, loss 0.351363, acc 0.88\n",
      "2018-01-08T04:48:41.761100: step 1131, loss 0.369426, acc 0.84\n",
      "2018-01-08T04:48:41.903990: step 1132, loss 0.399536, acc 0.75\n",
      "2018-01-08T04:48:42.048133: step 1133, loss 0.325709, acc 0.86\n",
      "2018-01-08T04:48:42.194477: step 1134, loss 0.333173, acc 0.81\n",
      "2018-01-08T04:48:42.335222: step 1135, loss 0.322699, acc 0.86\n",
      "2018-01-08T04:48:42.482945: step 1136, loss 0.302864, acc 0.88\n",
      "2018-01-08T04:48:42.625420: step 1137, loss 0.384338, acc 0.75\n",
      "2018-01-08T04:48:42.767429: step 1138, loss 0.423717, acc 0.71\n",
      "2018-01-08T04:48:42.917082: step 1139, loss 0.323603, acc 0.89\n",
      "2018-01-08T04:48:43.057215: step 1140, loss 0.354237, acc 0.81\n",
      "2018-01-08T04:48:43.197061: step 1141, loss 0.418514, acc 0.71\n",
      "2018-01-08T04:48:43.341790: step 1142, loss 0.328197, acc 0.94\n",
      "2018-01-08T04:48:43.471813: step 1143, loss 0.33654, acc 0.84\n",
      "2018-01-08T04:48:43.606977: step 1144, loss 0.319233, acc 0.84\n",
      "2018-01-08T04:48:43.744874: step 1145, loss 0.361258, acc 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:48:43.887089: step 1146, loss 0.377709, acc 0.78\n",
      "2018-01-08T04:48:44.024149: step 1147, loss 0.345155, acc 0.82\n",
      "2018-01-08T04:48:44.166319: step 1148, loss 0.343554, acc 0.89\n",
      "2018-01-08T04:48:44.307483: step 1149, loss 0.373688, acc 0.82\n",
      "2018-01-08T04:48:44.450497: step 1150, loss 0.352239, acc 0.84\n",
      "2018-01-08T04:48:44.587991: step 1151, loss 0.297426, acc 0.92\n",
      "2018-01-08T04:48:44.729108: step 1152, loss 0.33957, acc 0.81\n",
      "2018-01-08T04:48:44.874648: step 1153, loss 0.316458, acc 0.87\n",
      "2018-01-08T04:48:45.020340: step 1154, loss 0.3451, acc 0.82\n",
      "2018-01-08T04:48:45.158803: step 1155, loss 0.319669, acc 0.81\n",
      "2018-01-08T04:48:45.297046: step 1156, loss 0.337637, acc 0.84\n",
      "2018-01-08T04:48:45.438857: step 1157, loss 0.372804, acc 0.8\n",
      "2018-01-08T04:48:45.585340: step 1158, loss 0.331544, acc 0.86\n",
      "2018-01-08T04:48:45.727740: step 1159, loss 0.318847, acc 0.88\n",
      "2018-01-08T04:48:45.876378: step 1160, loss 0.352914, acc 0.84\n",
      "2018-01-08T04:48:46.019641: step 1161, loss 0.331011, acc 0.84\n",
      "2018-01-08T04:48:46.159317: step 1162, loss 0.347281, acc 0.82\n",
      "2018-01-08T04:48:46.305172: step 1163, loss 0.343924, acc 0.9\n",
      "2018-01-08T04:48:46.449139: step 1164, loss 0.352711, acc 0.82\n",
      "2018-01-08T04:48:46.595475: step 1165, loss 0.333739, acc 0.83\n",
      "2018-01-08T04:48:46.745658: step 1166, loss 0.356469, acc 0.81\n",
      "2018-01-08T04:48:46.895068: step 1167, loss 0.339448, acc 0.83\n",
      "2018-01-08T04:48:47.043910: step 1168, loss 0.351232, acc 0.86\n",
      "2018-01-08T04:48:47.194133: step 1169, loss 0.35541, acc 0.85\n",
      "2018-01-08T04:48:47.345337: step 1170, loss 0.347136, acc 0.81\n",
      "2018-01-08T04:48:47.492377: step 1171, loss 0.331021, acc 0.85\n",
      "2018-01-08T04:48:47.641211: step 1172, loss 0.327422, acc 0.85\n",
      "2018-01-08T04:48:47.780936: step 1173, loss 0.385369, acc 0.76\n",
      "2018-01-08T04:48:47.929821: step 1174, loss 0.348772, acc 0.81\n",
      "2018-01-08T04:48:48.074953: step 1175, loss 0.347827, acc 0.8\n",
      "2018-01-08T04:48:48.218382: step 1176, loss 0.330175, acc 0.81\n",
      "2018-01-08T04:48:48.362857: step 1177, loss 0.327465, acc 0.89\n",
      "2018-01-08T04:48:48.508863: step 1178, loss 0.363734, acc 0.82\n",
      "2018-01-08T04:48:48.656109: step 1179, loss 0.340403, acc 0.81\n",
      "2018-01-08T04:48:48.807411: step 1180, loss 0.344372, acc 0.87\n",
      "2018-01-08T04:48:48.956525: step 1181, loss 0.344092, acc 0.85\n",
      "2018-01-08T04:48:49.099823: step 1182, loss 0.308025, acc 0.9\n",
      "2018-01-08T04:48:49.239752: step 1183, loss 0.327724, acc 0.81\n",
      "2018-01-08T04:48:49.387461: step 1184, loss 0.319513, acc 0.82\n",
      "2018-01-08T04:48:49.531781: step 1185, loss 0.331079, acc 0.83\n",
      "2018-01-08T04:48:49.675451: step 1186, loss 0.35676, acc 0.78\n",
      "2018-01-08T04:48:49.814575: step 1187, loss 0.354359, acc 0.83\n",
      "2018-01-08T04:48:49.960770: step 1188, loss 0.356448, acc 0.81\n",
      "2018-01-08T04:48:50.107616: step 1189, loss 0.338773, acc 0.83\n",
      "2018-01-08T04:48:50.246042: step 1190, loss 0.356699, acc 0.85\n",
      "2018-01-08T04:48:50.387774: step 1191, loss 0.345196, acc 0.84\n",
      "2018-01-08T04:48:50.522925: step 1192, loss 0.3159, acc 0.85\n",
      "2018-01-08T04:48:50.666030: step 1193, loss 0.303486, acc 0.91\n",
      "2018-01-08T04:48:50.807744: step 1194, loss 0.351296, acc 0.83\n",
      "2018-01-08T04:48:50.949821: step 1195, loss 0.315373, acc 0.84\n",
      "2018-01-08T04:48:51.086961: step 1196, loss 0.336898, acc 0.79\n",
      "2018-01-08T04:48:51.224414: step 1197, loss 0.345453, acc 0.84\n",
      "2018-01-08T04:48:51.360289: step 1198, loss 0.353573, acc 0.81\n",
      "2018-01-08T04:48:51.501065: step 1199, loss 0.321419, acc 0.9\n",
      "2018-01-08T04:48:51.645390: step 1200, loss 0.389108, acc 0.79\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:48:51.909532: step 1200, loss 0.366363, acc 0.8186\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1200\n",
      "\n",
      "2018-01-08T04:48:52.119499: step 1201, loss 0.296475, acc 0.86\n",
      "2018-01-08T04:48:52.260983: step 1202, loss 0.335176, acc 0.81\n",
      "2018-01-08T04:48:52.400841: step 1203, loss 0.39685, acc 0.72\n",
      "2018-01-08T04:48:52.542907: step 1204, loss 0.338093, acc 0.82\n",
      "2018-01-08T04:48:52.682529: step 1205, loss 0.310518, acc 0.83\n",
      "2018-01-08T04:48:52.824756: step 1206, loss 0.306901, acc 0.85\n",
      "2018-01-08T04:48:52.967033: step 1207, loss 0.315219, acc 0.9\n",
      "2018-01-08T04:48:53.104876: step 1208, loss 0.322047, acc 0.87\n",
      "2018-01-08T04:48:53.248312: step 1209, loss 0.320039, acc 0.89\n",
      "2018-01-08T04:48:53.386726: step 1210, loss 0.354959, acc 0.83\n",
      "2018-01-08T04:48:53.527676: step 1211, loss 0.310193, acc 0.86\n",
      "2018-01-08T04:48:53.678021: step 1212, loss 0.317236, acc 0.83\n",
      "2018-01-08T04:48:53.829296: step 1213, loss 0.374863, acc 0.78\n",
      "2018-01-08T04:48:53.976642: step 1214, loss 0.359927, acc 0.79\n",
      "2018-01-08T04:48:54.128777: step 1215, loss 0.325154, acc 0.84\n",
      "2018-01-08T04:48:54.277091: step 1216, loss 0.307539, acc 0.89\n",
      "2018-01-08T04:48:54.428619: step 1217, loss 0.323583, acc 0.88\n",
      "2018-01-08T04:48:54.575952: step 1218, loss 0.330066, acc 0.86\n",
      "2018-01-08T04:48:54.725932: step 1219, loss 0.319457, acc 0.87\n",
      "2018-01-08T04:48:54.873227: step 1220, loss 0.333017, acc 0.82\n",
      "2018-01-08T04:48:55.021934: step 1221, loss 0.318906, acc 0.87\n",
      "2018-01-08T04:48:55.169697: step 1222, loss 0.31103, acc 0.86\n",
      "2018-01-08T04:48:55.314963: step 1223, loss 0.354191, acc 0.8\n",
      "2018-01-08T04:48:55.456772: step 1224, loss 0.350138, acc 0.78\n",
      "2018-01-08T04:48:55.603686: step 1225, loss 0.364198, acc 0.79\n",
      "2018-01-08T04:48:55.746008: step 1226, loss 0.332668, acc 0.84\n",
      "2018-01-08T04:48:55.893103: step 1227, loss 0.325347, acc 0.88\n",
      "2018-01-08T04:48:56.042244: step 1228, loss 0.309952, acc 0.89\n",
      "2018-01-08T04:48:56.192288: step 1229, loss 0.387339, acc 0.8\n",
      "2018-01-08T04:48:56.341179: step 1230, loss 0.327421, acc 0.88\n",
      "2018-01-08T04:48:56.484431: step 1231, loss 0.329682, acc 0.89\n",
      "2018-01-08T04:48:56.636718: step 1232, loss 0.340866, acc 0.88\n",
      "2018-01-08T04:48:56.785460: step 1233, loss 0.307603, acc 0.92\n",
      "2018-01-08T04:48:56.936958: step 1234, loss 0.33403, acc 0.87\n",
      "2018-01-08T04:48:57.085754: step 1235, loss 0.345491, acc 0.87\n",
      "2018-01-08T04:48:57.230293: step 1236, loss 0.335293, acc 0.89\n",
      "2018-01-08T04:48:57.378108: step 1237, loss 0.319703, acc 0.82\n",
      "2018-01-08T04:48:57.527213: step 1238, loss 0.315429, acc 0.88\n",
      "2018-01-08T04:48:57.686999: step 1239, loss 0.302442, acc 0.863636\n",
      "2018-01-08T04:48:57.916446: step 1240, loss 0.332138, acc 0.81\n",
      "2018-01-08T04:48:58.139149: step 1241, loss 0.329716, acc 0.84\n",
      "2018-01-08T04:48:58.361508: step 1242, loss 0.352858, acc 0.82\n",
      "2018-01-08T04:48:58.580798: step 1243, loss 0.334307, acc 0.85\n",
      "2018-01-08T04:48:58.808436: step 1244, loss 0.295224, acc 0.87\n",
      "2018-01-08T04:48:59.047708: step 1245, loss 0.306582, acc 0.92\n",
      "2018-01-08T04:48:59.281216: step 1246, loss 0.306611, acc 0.92\n",
      "2018-01-08T04:48:59.497720: step 1247, loss 0.326196, acc 0.85\n",
      "2018-01-08T04:48:59.721947: step 1248, loss 0.315461, acc 0.87\n",
      "2018-01-08T04:48:59.946386: step 1249, loss 0.363917, acc 0.76\n",
      "2018-01-08T04:49:00.179055: step 1250, loss 0.304528, acc 0.82\n",
      "2018-01-08T04:49:00.403019: step 1251, loss 0.273433, acc 0.93\n",
      "2018-01-08T04:49:00.630525: step 1252, loss 0.32272, acc 0.87\n",
      "2018-01-08T04:49:00.856055: step 1253, loss 0.329676, acc 0.85\n",
      "2018-01-08T04:49:01.117298: step 1254, loss 0.362359, acc 0.84\n",
      "2018-01-08T04:49:01.334375: step 1255, loss 0.33126, acc 0.84\n",
      "2018-01-08T04:49:01.540508: step 1256, loss 0.288729, acc 0.9\n",
      "2018-01-08T04:49:01.698842: step 1257, loss 0.320649, acc 0.88\n",
      "2018-01-08T04:49:01.859363: step 1258, loss 0.334101, acc 0.82\n",
      "2018-01-08T04:49:02.017343: step 1259, loss 0.313753, acc 0.86\n",
      "2018-01-08T04:49:02.188918: step 1260, loss 0.321769, acc 0.81\n",
      "2018-01-08T04:49:02.348050: step 1261, loss 0.316958, acc 0.82\n",
      "2018-01-08T04:49:02.506400: step 1262, loss 0.304566, acc 0.89\n",
      "2018-01-08T04:49:02.665776: step 1263, loss 0.31199, acc 0.86\n",
      "2018-01-08T04:49:02.830774: step 1264, loss 0.319764, acc 0.86\n",
      "2018-01-08T04:49:02.989873: step 1265, loss 0.355681, acc 0.83\n",
      "2018-01-08T04:49:03.155917: step 1266, loss 0.349709, acc 0.78\n",
      "2018-01-08T04:49:03.310675: step 1267, loss 0.345728, acc 0.82\n",
      "2018-01-08T04:49:03.466219: step 1268, loss 0.30152, acc 0.85\n",
      "2018-01-08T04:49:03.628564: step 1269, loss 0.305985, acc 0.86\n",
      "2018-01-08T04:49:03.789171: step 1270, loss 0.347467, acc 0.8\n",
      "2018-01-08T04:49:03.949815: step 1271, loss 0.331063, acc 0.84\n",
      "2018-01-08T04:49:04.111405: step 1272, loss 0.309174, acc 0.87\n",
      "2018-01-08T04:49:04.267721: step 1273, loss 0.298362, acc 0.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:49:04.425181: step 1274, loss 0.330504, acc 0.84\n",
      "2018-01-08T04:49:04.584108: step 1275, loss 0.334865, acc 0.84\n",
      "2018-01-08T04:49:04.748085: step 1276, loss 0.250886, acc 0.94\n",
      "2018-01-08T04:49:04.915050: step 1277, loss 0.292905, acc 0.9\n",
      "2018-01-08T04:49:05.076128: step 1278, loss 0.309251, acc 0.83\n",
      "2018-01-08T04:49:05.236097: step 1279, loss 0.312921, acc 0.82\n",
      "2018-01-08T04:49:05.400711: step 1280, loss 0.2883, acc 0.89\n",
      "2018-01-08T04:49:05.553229: step 1281, loss 0.335768, acc 0.83\n",
      "2018-01-08T04:49:05.697862: step 1282, loss 0.318925, acc 0.89\n",
      "2018-01-08T04:49:05.844415: step 1283, loss 0.307397, acc 0.84\n",
      "2018-01-08T04:49:05.996572: step 1284, loss 0.30406, acc 0.84\n",
      "2018-01-08T04:49:06.146037: step 1285, loss 0.308859, acc 0.8\n",
      "2018-01-08T04:49:06.293950: step 1286, loss 0.33815, acc 0.8\n",
      "2018-01-08T04:49:06.442232: step 1287, loss 0.280897, acc 0.92\n",
      "2018-01-08T04:49:06.584670: step 1288, loss 0.281676, acc 0.92\n",
      "2018-01-08T04:49:06.728608: step 1289, loss 0.298178, acc 0.84\n",
      "2018-01-08T04:49:06.879800: step 1290, loss 0.296405, acc 0.92\n",
      "2018-01-08T04:49:07.025466: step 1291, loss 0.320732, acc 0.88\n",
      "2018-01-08T04:49:07.178289: step 1292, loss 0.280907, acc 0.86\n",
      "2018-01-08T04:49:07.320415: step 1293, loss 0.308348, acc 0.83\n",
      "2018-01-08T04:49:07.464932: step 1294, loss 0.348885, acc 0.78\n",
      "2018-01-08T04:49:07.603217: step 1295, loss 0.360384, acc 0.78\n",
      "2018-01-08T04:49:07.745336: step 1296, loss 0.268838, acc 0.91\n",
      "2018-01-08T04:49:07.892122: step 1297, loss 0.283597, acc 0.88\n",
      "2018-01-08T04:49:08.042158: step 1298, loss 0.315183, acc 0.88\n",
      "2018-01-08T04:49:08.186152: step 1299, loss 0.307623, acc 0.93\n",
      "2018-01-08T04:49:08.335379: step 1300, loss 0.300425, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:49:08.599842: step 1300, loss 0.350494, acc 0.820644\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1300\n",
      "\n",
      "2018-01-08T04:49:08.805624: step 1301, loss 0.311037, acc 0.85\n",
      "2018-01-08T04:49:08.949546: step 1302, loss 0.308819, acc 0.9\n",
      "2018-01-08T04:49:09.096487: step 1303, loss 0.316255, acc 0.88\n",
      "2018-01-08T04:49:09.241930: step 1304, loss 0.297128, acc 0.88\n",
      "2018-01-08T04:49:09.383749: step 1305, loss 0.314325, acc 0.86\n",
      "2018-01-08T04:49:09.526654: step 1306, loss 0.288892, acc 0.88\n",
      "2018-01-08T04:49:09.671844: step 1307, loss 0.313051, acc 0.85\n",
      "2018-01-08T04:49:09.811439: step 1308, loss 0.352543, acc 0.84\n",
      "2018-01-08T04:49:09.949932: step 1309, loss 0.285922, acc 0.88\n",
      "2018-01-08T04:49:10.090325: step 1310, loss 0.319647, acc 0.83\n",
      "2018-01-08T04:49:10.225759: step 1311, loss 0.297579, acc 0.82\n",
      "2018-01-08T04:49:10.366075: step 1312, loss 0.343495, acc 0.83\n",
      "2018-01-08T04:49:10.503001: step 1313, loss 0.253142, acc 0.9\n",
      "2018-01-08T04:49:10.645861: step 1314, loss 0.311158, acc 0.86\n",
      "2018-01-08T04:49:10.791398: step 1315, loss 0.307558, acc 0.89\n",
      "2018-01-08T04:49:10.930133: step 1316, loss 0.329327, acc 0.84\n",
      "2018-01-08T04:49:11.067784: step 1317, loss 0.283181, acc 0.9\n",
      "2018-01-08T04:49:11.205532: step 1318, loss 0.304902, acc 0.87\n",
      "2018-01-08T04:49:11.343281: step 1319, loss 0.330768, acc 0.85\n",
      "2018-01-08T04:49:11.477586: step 1320, loss 0.339189, acc 0.82\n",
      "2018-01-08T04:49:11.621242: step 1321, loss 0.338523, acc 0.83\n",
      "2018-01-08T04:49:11.763489: step 1322, loss 0.29451, acc 0.88\n",
      "2018-01-08T04:49:11.902471: step 1323, loss 0.312544, acc 0.86\n",
      "2018-01-08T04:49:12.039237: step 1324, loss 0.288816, acc 0.88\n",
      "2018-01-08T04:49:12.182652: step 1325, loss 0.352058, acc 0.8\n",
      "2018-01-08T04:49:12.321475: step 1326, loss 0.306307, acc 0.89\n",
      "2018-01-08T04:49:12.464602: step 1327, loss 0.291142, acc 0.86\n",
      "2018-01-08T04:49:12.609854: step 1328, loss 0.362287, acc 0.82\n",
      "2018-01-08T04:49:12.749277: step 1329, loss 0.341724, acc 0.86\n",
      "2018-01-08T04:49:12.899548: step 1330, loss 0.310526, acc 0.87\n",
      "2018-01-08T04:49:13.042493: step 1331, loss 0.320203, acc 0.84\n",
      "2018-01-08T04:49:13.182052: step 1332, loss 0.307971, acc 0.84\n",
      "2018-01-08T04:49:13.321093: step 1333, loss 0.30493, acc 0.83\n",
      "2018-01-08T04:49:13.457204: step 1334, loss 0.296503, acc 0.86\n",
      "2018-01-08T04:49:13.599440: step 1335, loss 0.329571, acc 0.83\n",
      "2018-01-08T04:49:13.744627: step 1336, loss 0.305046, acc 0.84\n",
      "2018-01-08T04:49:13.889344: step 1337, loss 0.323147, acc 0.86\n",
      "2018-01-08T04:49:14.042539: step 1338, loss 0.3153, acc 0.88\n",
      "2018-01-08T04:49:14.200788: step 1339, loss 0.295764, acc 0.88\n",
      "2018-01-08T04:49:14.346700: step 1340, loss 0.310541, acc 0.89\n",
      "2018-01-08T04:49:14.495474: step 1341, loss 0.323287, acc 0.8\n",
      "2018-01-08T04:49:14.647837: step 1342, loss 0.318338, acc 0.86\n",
      "2018-01-08T04:49:14.794886: step 1343, loss 0.301649, acc 0.85\n",
      "2018-01-08T04:49:14.943082: step 1344, loss 0.277069, acc 0.91\n",
      "2018-01-08T04:49:15.089580: step 1345, loss 0.267584, acc 0.94\n",
      "2018-01-08T04:49:15.233747: step 1346, loss 0.292257, acc 0.88\n",
      "2018-01-08T04:49:15.378277: step 1347, loss 0.299358, acc 0.82\n",
      "2018-01-08T04:49:15.527162: step 1348, loss 0.305738, acc 0.85\n",
      "2018-01-08T04:49:15.674879: step 1349, loss 0.313185, acc 0.83\n",
      "2018-01-08T04:49:15.822353: step 1350, loss 0.300527, acc 0.84\n",
      "2018-01-08T04:49:15.970153: step 1351, loss 0.326492, acc 0.81\n",
      "2018-01-08T04:49:16.120830: step 1352, loss 0.283208, acc 0.89\n",
      "2018-01-08T04:49:16.270074: step 1353, loss 0.318031, acc 0.84\n",
      "2018-01-08T04:49:16.414906: step 1354, loss 0.305784, acc 0.89\n",
      "2018-01-08T04:49:16.559477: step 1355, loss 0.252934, acc 0.89\n",
      "2018-01-08T04:49:16.706522: step 1356, loss 0.270148, acc 0.93\n",
      "2018-01-08T04:49:16.856644: step 1357, loss 0.335428, acc 0.86\n",
      "2018-01-08T04:49:17.001014: step 1358, loss 0.266303, acc 0.87\n",
      "2018-01-08T04:49:17.144367: step 1359, loss 0.303193, acc 0.83\n",
      "2018-01-08T04:49:17.288548: step 1360, loss 0.319203, acc 0.85\n",
      "2018-01-08T04:49:17.436526: step 1361, loss 0.32477, acc 0.8\n",
      "2018-01-08T04:49:17.583157: step 1362, loss 0.330735, acc 0.86\n",
      "2018-01-08T04:49:17.722926: step 1363, loss 0.357117, acc 0.78\n",
      "2018-01-08T04:49:17.866286: step 1364, loss 0.2791, acc 0.91\n",
      "2018-01-08T04:49:18.011679: step 1365, loss 0.288124, acc 0.94\n",
      "2018-01-08T04:49:18.150643: step 1366, loss 0.337769, acc 0.8\n",
      "2018-01-08T04:49:18.294002: step 1367, loss 0.369122, acc 0.72\n",
      "2018-01-08T04:49:18.431803: step 1368, loss 0.302653, acc 0.85\n",
      "2018-01-08T04:49:18.578208: step 1369, loss 0.314024, acc 0.87\n",
      "2018-01-08T04:49:18.720715: step 1370, loss 0.295196, acc 0.88\n",
      "2018-01-08T04:49:18.867621: step 1371, loss 0.319007, acc 0.84\n",
      "2018-01-08T04:49:19.009760: step 1372, loss 0.263363, acc 0.94\n",
      "2018-01-08T04:49:19.153093: step 1373, loss 0.291983, acc 0.88\n",
      "2018-01-08T04:49:19.290331: step 1374, loss 0.328791, acc 0.89\n",
      "2018-01-08T04:49:19.427428: step 1375, loss 0.319841, acc 0.87\n",
      "2018-01-08T04:49:19.561827: step 1376, loss 0.327787, acc 0.85\n",
      "2018-01-08T04:49:19.704912: step 1377, loss 0.309571, acc 0.85\n",
      "2018-01-08T04:49:19.847192: step 1378, loss 0.315668, acc 0.83\n",
      "2018-01-08T04:49:19.982315: step 1379, loss 0.300536, acc 0.9\n",
      "2018-01-08T04:49:20.121883: step 1380, loss 0.304584, acc 0.86\n",
      "2018-01-08T04:49:20.259482: step 1381, loss 0.315559, acc 0.86\n",
      "2018-01-08T04:49:20.397982: step 1382, loss 0.321061, acc 0.84\n",
      "2018-01-08T04:49:20.548233: step 1383, loss 0.275038, acc 0.91\n",
      "2018-01-08T04:49:20.696646: step 1384, loss 0.30974, acc 0.85\n",
      "2018-01-08T04:49:20.844418: step 1385, loss 0.295158, acc 0.83\n",
      "2018-01-08T04:49:20.985560: step 1386, loss 0.305932, acc 0.84\n",
      "2018-01-08T04:49:21.131476: step 1387, loss 0.303221, acc 0.85\n",
      "2018-01-08T04:49:21.275689: step 1388, loss 0.276504, acc 0.91\n",
      "2018-01-08T04:49:21.412905: step 1389, loss 0.299163, acc 0.9\n",
      "2018-01-08T04:49:21.552057: step 1390, loss 0.285638, acc 0.89\n",
      "2018-01-08T04:49:21.700789: step 1391, loss 0.292611, acc 0.87\n",
      "2018-01-08T04:49:21.854003: step 1392, loss 0.271072, acc 0.91\n",
      "2018-01-08T04:49:22.011243: step 1393, loss 0.268321, acc 0.88\n",
      "2018-01-08T04:49:22.156191: step 1394, loss 0.300387, acc 0.86\n",
      "2018-01-08T04:49:22.298887: step 1395, loss 0.266692, acc 0.84\n",
      "2018-01-08T04:49:22.443067: step 1396, loss 0.321319, acc 0.79\n",
      "2018-01-08T04:49:22.586368: step 1397, loss 0.283813, acc 0.85\n",
      "2018-01-08T04:49:22.734081: step 1398, loss 0.279649, acc 0.9\n",
      "2018-01-08T04:49:22.885258: step 1399, loss 0.337775, acc 0.85\n",
      "2018-01-08T04:49:23.032767: step 1400, loss 0.283056, acc 0.88\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:49:23.313111: step 1400, loss 0.339815, acc 0.831375\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1400\n",
      "\n",
      "2018-01-08T04:49:23.520342: step 1401, loss 0.301147, acc 0.89\n",
      "2018-01-08T04:49:23.671164: step 1402, loss 0.294878, acc 0.87\n",
      "2018-01-08T04:49:23.814442: step 1403, loss 0.319049, acc 0.84\n",
      "2018-01-08T04:49:23.960670: step 1404, loss 0.279705, acc 0.92\n",
      "2018-01-08T04:49:24.103398: step 1405, loss 0.31157, acc 0.83\n",
      "2018-01-08T04:49:24.246035: step 1406, loss 0.297791, acc 0.88\n",
      "2018-01-08T04:49:24.394811: step 1407, loss 0.30088, acc 0.88\n",
      "2018-01-08T04:49:24.539272: step 1408, loss 0.271461, acc 0.94\n",
      "2018-01-08T04:49:24.678009: step 1409, loss 0.340176, acc 0.82\n",
      "2018-01-08T04:49:24.828319: step 1410, loss 0.30374, acc 0.87\n",
      "2018-01-08T04:49:24.975310: step 1411, loss 0.3274, acc 0.86\n",
      "2018-01-08T04:49:25.124157: step 1412, loss 0.285313, acc 0.89\n",
      "2018-01-08T04:49:25.268766: step 1413, loss 0.338877, acc 0.81\n",
      "2018-01-08T04:49:25.407524: step 1414, loss 0.323278, acc 0.81\n",
      "2018-01-08T04:49:25.548695: step 1415, loss 0.328666, acc 0.83\n",
      "2018-01-08T04:49:25.634863: step 1416, loss 0.250606, acc 0.909091\n",
      "2018-01-08T04:49:25.778909: step 1417, loss 0.280513, acc 0.87\n",
      "2018-01-08T04:49:25.923094: step 1418, loss 0.274164, acc 0.89\n",
      "2018-01-08T04:49:26.071276: step 1419, loss 0.29576, acc 0.88\n",
      "2018-01-08T04:49:26.217089: step 1420, loss 0.270888, acc 0.91\n",
      "2018-01-08T04:49:26.355780: step 1421, loss 0.298286, acc 0.86\n",
      "2018-01-08T04:49:26.502375: step 1422, loss 0.247765, acc 0.91\n",
      "2018-01-08T04:49:26.649099: step 1423, loss 0.304197, acc 0.89\n",
      "2018-01-08T04:49:26.793013: step 1424, loss 0.28318, acc 0.9\n",
      "2018-01-08T04:49:26.935834: step 1425, loss 0.325315, acc 0.84\n",
      "2018-01-08T04:49:27.075191: step 1426, loss 0.290809, acc 0.89\n",
      "2018-01-08T04:49:27.220076: step 1427, loss 0.281725, acc 0.88\n",
      "2018-01-08T04:49:27.355755: step 1428, loss 0.288969, acc 0.88\n",
      "2018-01-08T04:49:27.496603: step 1429, loss 0.282715, acc 0.9\n",
      "2018-01-08T04:49:27.635277: step 1430, loss 0.294039, acc 0.9\n",
      "2018-01-08T04:49:27.778599: step 1431, loss 0.296913, acc 0.88\n",
      "2018-01-08T04:49:27.923356: step 1432, loss 0.275156, acc 0.87\n",
      "2018-01-08T04:49:28.067564: step 1433, loss 0.310867, acc 0.84\n",
      "2018-01-08T04:49:28.214416: step 1434, loss 0.275142, acc 0.89\n",
      "2018-01-08T04:49:28.359984: step 1435, loss 0.293244, acc 0.89\n",
      "2018-01-08T04:49:28.497631: step 1436, loss 0.274116, acc 0.91\n",
      "2018-01-08T04:49:28.635624: step 1437, loss 0.271903, acc 0.92\n",
      "2018-01-08T04:49:28.775951: step 1438, loss 0.311024, acc 0.81\n",
      "2018-01-08T04:49:28.917507: step 1439, loss 0.286955, acc 0.86\n",
      "2018-01-08T04:49:29.066660: step 1440, loss 0.269089, acc 0.9\n",
      "2018-01-08T04:49:29.204458: step 1441, loss 0.278932, acc 0.9\n",
      "2018-01-08T04:49:29.344562: step 1442, loss 0.294552, acc 0.83\n",
      "2018-01-08T04:49:29.476912: step 1443, loss 0.332807, acc 0.78\n",
      "2018-01-08T04:49:29.617832: step 1444, loss 0.301916, acc 0.86\n",
      "2018-01-08T04:49:29.767408: step 1445, loss 0.245832, acc 0.92\n",
      "2018-01-08T04:49:29.913591: step 1446, loss 0.340532, acc 0.83\n",
      "2018-01-08T04:49:30.057035: step 1447, loss 0.319398, acc 0.82\n",
      "2018-01-08T04:49:30.198116: step 1448, loss 0.276711, acc 0.87\n",
      "2018-01-08T04:49:30.343796: step 1449, loss 0.273155, acc 0.88\n",
      "2018-01-08T04:49:30.494526: step 1450, loss 0.253148, acc 0.92\n",
      "2018-01-08T04:49:30.648673: step 1451, loss 0.295975, acc 0.86\n",
      "2018-01-08T04:49:30.795635: step 1452, loss 0.324482, acc 0.79\n",
      "2018-01-08T04:49:30.944066: step 1453, loss 0.29023, acc 0.87\n",
      "2018-01-08T04:49:31.082967: step 1454, loss 0.297393, acc 0.88\n",
      "2018-01-08T04:49:31.224880: step 1455, loss 0.28677, acc 0.92\n",
      "2018-01-08T04:49:31.360192: step 1456, loss 0.309135, acc 0.84\n",
      "2018-01-08T04:49:31.504325: step 1457, loss 0.328819, acc 0.86\n",
      "2018-01-08T04:49:31.647371: step 1458, loss 0.277007, acc 0.9\n",
      "2018-01-08T04:49:31.801704: step 1459, loss 0.312597, acc 0.85\n",
      "2018-01-08T04:49:31.951049: step 1460, loss 0.302943, acc 0.89\n",
      "2018-01-08T04:49:32.097363: step 1461, loss 0.284222, acc 0.83\n",
      "2018-01-08T04:49:32.246258: step 1462, loss 0.294841, acc 0.87\n",
      "2018-01-08T04:49:32.396020: step 1463, loss 0.274875, acc 0.9\n",
      "2018-01-08T04:49:32.544237: step 1464, loss 0.280533, acc 0.92\n",
      "2018-01-08T04:49:32.686128: step 1465, loss 0.279112, acc 0.89\n",
      "2018-01-08T04:49:32.830742: step 1466, loss 0.269784, acc 0.88\n",
      "2018-01-08T04:49:32.976819: step 1467, loss 0.290757, acc 0.86\n",
      "2018-01-08T04:49:33.121975: step 1468, loss 0.292767, acc 0.83\n",
      "2018-01-08T04:49:33.271900: step 1469, loss 0.284598, acc 0.88\n",
      "2018-01-08T04:49:33.411886: step 1470, loss 0.373358, acc 0.76\n",
      "2018-01-08T04:49:33.559462: step 1471, loss 0.274191, acc 0.89\n",
      "2018-01-08T04:49:33.698571: step 1472, loss 0.275154, acc 0.85\n",
      "2018-01-08T04:49:33.842504: step 1473, loss 0.29065, acc 0.85\n",
      "2018-01-08T04:49:33.977836: step 1474, loss 0.259715, acc 0.88\n",
      "2018-01-08T04:49:34.120811: step 1475, loss 0.285022, acc 0.86\n",
      "2018-01-08T04:49:34.259837: step 1476, loss 0.270625, acc 0.87\n",
      "2018-01-08T04:49:34.400243: step 1477, loss 0.274639, acc 0.89\n",
      "2018-01-08T04:49:34.542152: step 1478, loss 0.254794, acc 0.89\n",
      "2018-01-08T04:49:34.681278: step 1479, loss 0.266175, acc 0.93\n",
      "2018-01-08T04:49:34.827521: step 1480, loss 0.224125, acc 0.94\n",
      "2018-01-08T04:49:34.969194: step 1481, loss 0.270253, acc 0.89\n",
      "2018-01-08T04:49:35.110612: step 1482, loss 0.302534, acc 0.81\n",
      "2018-01-08T04:49:35.248406: step 1483, loss 0.301807, acc 0.83\n",
      "2018-01-08T04:49:35.384668: step 1484, loss 0.272779, acc 0.9\n",
      "2018-01-08T04:49:35.517129: step 1485, loss 0.246133, acc 0.93\n",
      "2018-01-08T04:49:35.658156: step 1486, loss 0.279211, acc 0.87\n",
      "2018-01-08T04:49:35.798433: step 1487, loss 0.25364, acc 0.92\n",
      "2018-01-08T04:49:35.933935: step 1488, loss 0.28794, acc 0.89\n",
      "2018-01-08T04:49:36.073152: step 1489, loss 0.300225, acc 0.87\n",
      "2018-01-08T04:49:36.210072: step 1490, loss 0.257736, acc 0.89\n",
      "2018-01-08T04:49:36.357474: step 1491, loss 0.295268, acc 0.87\n",
      "2018-01-08T04:49:36.493925: step 1492, loss 0.305209, acc 0.82\n",
      "2018-01-08T04:49:36.639909: step 1493, loss 0.247878, acc 0.92\n",
      "2018-01-08T04:49:36.782945: step 1494, loss 0.316143, acc 0.85\n",
      "2018-01-08T04:49:36.926810: step 1495, loss 0.286643, acc 0.86\n",
      "2018-01-08T04:49:37.073115: step 1496, loss 0.283573, acc 0.82\n",
      "2018-01-08T04:49:37.219543: step 1497, loss 0.284186, acc 0.84\n",
      "2018-01-08T04:49:37.356613: step 1498, loss 0.28331, acc 0.82\n",
      "2018-01-08T04:49:37.494780: step 1499, loss 0.247451, acc 0.89\n",
      "2018-01-08T04:49:37.635475: step 1500, loss 0.26827, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:49:37.912544: step 1500, loss 0.325102, acc 0.833418\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1500\n",
      "\n",
      "2018-01-08T04:49:38.122034: step 1501, loss 0.318302, acc 0.84\n",
      "2018-01-08T04:49:38.272680: step 1502, loss 0.262027, acc 0.91\n",
      "2018-01-08T04:49:38.415096: step 1503, loss 0.276417, acc 0.92\n",
      "2018-01-08T04:49:38.560159: step 1504, loss 0.290977, acc 0.89\n",
      "2018-01-08T04:49:38.704069: step 1505, loss 0.269329, acc 0.87\n",
      "2018-01-08T04:49:38.848802: step 1506, loss 0.273524, acc 0.86\n",
      "2018-01-08T04:49:38.991583: step 1507, loss 0.260056, acc 0.85\n",
      "2018-01-08T04:49:39.139630: step 1508, loss 0.341553, acc 0.8\n",
      "2018-01-08T04:49:39.279157: step 1509, loss 0.257543, acc 0.88\n",
      "2018-01-08T04:49:39.425490: step 1510, loss 0.279128, acc 0.91\n",
      "2018-01-08T04:49:39.566977: step 1511, loss 0.296698, acc 0.88\n",
      "2018-01-08T04:49:39.711877: step 1512, loss 0.254592, acc 0.94\n",
      "2018-01-08T04:49:39.853436: step 1513, loss 0.268467, acc 0.87\n",
      "2018-01-08T04:49:39.995288: step 1514, loss 0.247847, acc 0.94\n",
      "2018-01-08T04:49:40.140234: step 1515, loss 0.241022, acc 0.91\n",
      "2018-01-08T04:49:40.291540: step 1516, loss 0.252578, acc 0.89\n",
      "2018-01-08T04:49:40.440808: step 1517, loss 0.274398, acc 0.89\n",
      "2018-01-08T04:49:40.590711: step 1518, loss 0.264787, acc 0.91\n",
      "2018-01-08T04:49:40.734026: step 1519, loss 0.293321, acc 0.82\n",
      "2018-01-08T04:49:40.883714: step 1520, loss 0.312716, acc 0.82\n",
      "2018-01-08T04:49:41.032519: step 1521, loss 0.247423, acc 0.9\n",
      "2018-01-08T04:49:41.175324: step 1522, loss 0.26177, acc 0.91\n",
      "2018-01-08T04:49:41.320516: step 1523, loss 0.269125, acc 0.87\n",
      "2018-01-08T04:49:41.459929: step 1524, loss 0.275777, acc 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:49:41.651398: step 1525, loss 0.289698, acc 0.84\n",
      "2018-01-08T04:49:41.876532: step 1526, loss 0.275153, acc 0.9\n",
      "2018-01-08T04:49:42.097149: step 1527, loss 0.233469, acc 0.92\n",
      "2018-01-08T04:49:42.328164: step 1528, loss 0.305133, acc 0.82\n",
      "2018-01-08T04:49:42.550329: step 1529, loss 0.276517, acc 0.83\n",
      "2018-01-08T04:49:42.778464: step 1530, loss 0.247328, acc 0.93\n",
      "2018-01-08T04:49:43.013344: step 1531, loss 0.311713, acc 0.84\n",
      "2018-01-08T04:49:43.244180: step 1532, loss 0.287759, acc 0.88\n",
      "2018-01-08T04:49:43.463732: step 1533, loss 0.278428, acc 0.88\n",
      "2018-01-08T04:49:43.691775: step 1534, loss 0.278507, acc 0.85\n",
      "2018-01-08T04:49:43.916172: step 1535, loss 0.259054, acc 0.91\n",
      "2018-01-08T04:49:44.143657: step 1536, loss 0.301103, acc 0.89\n",
      "2018-01-08T04:49:44.365100: step 1537, loss 0.254063, acc 0.87\n",
      "2018-01-08T04:49:44.592335: step 1538, loss 0.251818, acc 0.91\n",
      "2018-01-08T04:49:44.826910: step 1539, loss 0.253731, acc 0.88\n",
      "2018-01-08T04:49:45.054245: step 1540, loss 0.263062, acc 0.87\n",
      "2018-01-08T04:49:45.276260: step 1541, loss 0.280543, acc 0.88\n",
      "2018-01-08T04:49:45.496561: step 1542, loss 0.319374, acc 0.84\n",
      "2018-01-08T04:49:45.681354: step 1543, loss 0.251387, acc 0.87\n",
      "2018-01-08T04:49:45.846984: step 1544, loss 0.285161, acc 0.87\n",
      "2018-01-08T04:49:46.006715: step 1545, loss 0.285223, acc 0.87\n",
      "2018-01-08T04:49:46.170530: step 1546, loss 0.291642, acc 0.83\n",
      "2018-01-08T04:49:46.331417: step 1547, loss 0.258458, acc 0.94\n",
      "2018-01-08T04:49:46.488524: step 1548, loss 0.274422, acc 0.87\n",
      "2018-01-08T04:49:46.655512: step 1549, loss 0.245417, acc 0.92\n",
      "2018-01-08T04:49:46.818697: step 1550, loss 0.24618, acc 0.92\n",
      "2018-01-08T04:49:46.977542: step 1551, loss 0.317312, acc 0.84\n",
      "2018-01-08T04:49:47.131917: step 1552, loss 0.247921, acc 0.87\n",
      "2018-01-08T04:49:47.286715: step 1553, loss 0.289249, acc 0.83\n",
      "2018-01-08T04:49:47.441390: step 1554, loss 0.280556, acc 0.91\n",
      "2018-01-08T04:49:47.597386: step 1555, loss 0.28402, acc 0.84\n",
      "2018-01-08T04:49:47.760365: step 1556, loss 0.230191, acc 0.95\n",
      "2018-01-08T04:49:47.919021: step 1557, loss 0.268504, acc 0.91\n",
      "2018-01-08T04:49:48.073108: step 1558, loss 0.289491, acc 0.83\n",
      "2018-01-08T04:49:48.232784: step 1559, loss 0.235968, acc 0.89\n",
      "2018-01-08T04:49:48.392904: step 1560, loss 0.261449, acc 0.83\n",
      "2018-01-08T04:49:48.577387: step 1561, loss 0.276537, acc 0.88\n",
      "2018-01-08T04:49:48.736944: step 1562, loss 0.299785, acc 0.82\n",
      "2018-01-08T04:49:48.897166: step 1563, loss 0.279091, acc 0.86\n",
      "2018-01-08T04:49:49.062477: step 1564, loss 0.281113, acc 0.88\n",
      "2018-01-08T04:49:49.225441: step 1565, loss 0.291896, acc 0.87\n",
      "2018-01-08T04:49:49.381375: step 1566, loss 0.275518, acc 0.88\n",
      "2018-01-08T04:49:49.537786: step 1567, loss 0.286105, acc 0.86\n",
      "2018-01-08T04:49:49.687366: step 1568, loss 0.275877, acc 0.86\n",
      "2018-01-08T04:49:49.838306: step 1569, loss 0.266405, acc 0.86\n",
      "2018-01-08T04:49:49.985389: step 1570, loss 0.278804, acc 0.87\n",
      "2018-01-08T04:49:50.134456: step 1571, loss 0.292239, acc 0.81\n",
      "2018-01-08T04:49:50.285519: step 1572, loss 0.29978, acc 0.81\n",
      "2018-01-08T04:49:50.429937: step 1573, loss 0.229744, acc 0.94\n",
      "2018-01-08T04:49:50.576955: step 1574, loss 0.321026, acc 0.82\n",
      "2018-01-08T04:49:50.720001: step 1575, loss 0.273417, acc 0.92\n",
      "2018-01-08T04:49:50.859718: step 1576, loss 0.300279, acc 0.85\n",
      "2018-01-08T04:49:51.001336: step 1577, loss 0.293087, acc 0.86\n",
      "2018-01-08T04:49:51.148314: step 1578, loss 0.239156, acc 0.95\n",
      "2018-01-08T04:49:51.296252: step 1579, loss 0.255281, acc 0.88\n",
      "2018-01-08T04:49:51.442601: step 1580, loss 0.300771, acc 0.82\n",
      "2018-01-08T04:49:51.586813: step 1581, loss 0.272105, acc 0.88\n",
      "2018-01-08T04:49:51.731407: step 1582, loss 0.317947, acc 0.82\n",
      "2018-01-08T04:49:51.876107: step 1583, loss 0.227744, acc 0.94\n",
      "2018-01-08T04:49:52.021447: step 1584, loss 0.275865, acc 0.85\n",
      "2018-01-08T04:49:52.172138: step 1585, loss 0.260164, acc 0.85\n",
      "2018-01-08T04:49:52.312348: step 1586, loss 0.24887, acc 0.89\n",
      "2018-01-08T04:49:52.452967: step 1587, loss 0.267366, acc 0.91\n",
      "2018-01-08T04:49:52.602933: step 1588, loss 0.297018, acc 0.84\n",
      "2018-01-08T04:49:52.748567: step 1589, loss 0.250853, acc 0.91\n",
      "2018-01-08T04:49:52.902340: step 1590, loss 0.248776, acc 0.9\n",
      "2018-01-08T04:49:53.049927: step 1591, loss 0.249043, acc 0.91\n",
      "2018-01-08T04:49:53.199410: step 1592, loss 0.276639, acc 0.84\n",
      "2018-01-08T04:49:53.286915: step 1593, loss 0.231714, acc 0.954545\n",
      "2018-01-08T04:49:53.433360: step 1594, loss 0.247547, acc 0.9\n",
      "2018-01-08T04:49:53.613257: step 1595, loss 0.301772, acc 0.88\n",
      "2018-01-08T04:49:53.757373: step 1596, loss 0.288077, acc 0.86\n",
      "2018-01-08T04:49:53.899480: step 1597, loss 0.260906, acc 0.89\n",
      "2018-01-08T04:49:54.035621: step 1598, loss 0.229567, acc 0.92\n",
      "2018-01-08T04:49:54.179651: step 1599, loss 0.247401, acc 0.89\n",
      "2018-01-08T04:49:54.319881: step 1600, loss 0.278659, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:49:54.593900: step 1600, loss 0.315529, acc 0.822177\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1600\n",
      "\n",
      "2018-01-08T04:49:54.801447: step 1601, loss 0.2592, acc 0.9\n",
      "2018-01-08T04:49:54.938485: step 1602, loss 0.254856, acc 0.88\n",
      "2018-01-08T04:49:55.075510: step 1603, loss 0.290902, acc 0.87\n",
      "2018-01-08T04:49:55.211342: step 1604, loss 0.254045, acc 0.95\n",
      "2018-01-08T04:49:55.350725: step 1605, loss 0.253907, acc 0.92\n",
      "2018-01-08T04:49:55.484507: step 1606, loss 0.260916, acc 0.92\n",
      "2018-01-08T04:49:55.624366: step 1607, loss 0.233192, acc 0.91\n",
      "2018-01-08T04:49:55.765869: step 1608, loss 0.252542, acc 0.91\n",
      "2018-01-08T04:49:55.913202: step 1609, loss 0.26503, acc 0.88\n",
      "2018-01-08T04:49:56.053964: step 1610, loss 0.264168, acc 0.86\n",
      "2018-01-08T04:49:56.193343: step 1611, loss 0.273027, acc 0.84\n",
      "2018-01-08T04:49:56.335431: step 1612, loss 0.282693, acc 0.86\n",
      "2018-01-08T04:49:56.478754: step 1613, loss 0.278002, acc 0.87\n",
      "2018-01-08T04:49:56.617653: step 1614, loss 0.297645, acc 0.86\n",
      "2018-01-08T04:49:56.763497: step 1615, loss 0.269491, acc 0.83\n",
      "2018-01-08T04:49:56.899820: step 1616, loss 0.264031, acc 0.91\n",
      "2018-01-08T04:49:57.042222: step 1617, loss 0.23488, acc 0.92\n",
      "2018-01-08T04:49:57.179654: step 1618, loss 0.258812, acc 0.89\n",
      "2018-01-08T04:49:57.316788: step 1619, loss 0.26138, acc 0.9\n",
      "2018-01-08T04:49:57.453619: step 1620, loss 0.254098, acc 0.9\n",
      "2018-01-08T04:49:57.593167: step 1621, loss 0.245313, acc 0.9\n",
      "2018-01-08T04:49:57.741653: step 1622, loss 0.21252, acc 0.95\n",
      "2018-01-08T04:49:57.893468: step 1623, loss 0.26072, acc 0.89\n",
      "2018-01-08T04:49:58.036943: step 1624, loss 0.265925, acc 0.85\n",
      "2018-01-08T04:49:58.187240: step 1625, loss 0.26483, acc 0.86\n",
      "2018-01-08T04:49:58.331462: step 1626, loss 0.269246, acc 0.91\n",
      "2018-01-08T04:49:58.488346: step 1627, loss 0.283807, acc 0.89\n",
      "2018-01-08T04:49:58.634377: step 1628, loss 0.249524, acc 0.89\n",
      "2018-01-08T04:49:58.779239: step 1629, loss 0.262531, acc 0.87\n",
      "2018-01-08T04:49:58.927468: step 1630, loss 0.271968, acc 0.86\n",
      "2018-01-08T04:49:59.081032: step 1631, loss 0.220203, acc 0.94\n",
      "2018-01-08T04:49:59.227118: step 1632, loss 0.234056, acc 0.9\n",
      "2018-01-08T04:49:59.372022: step 1633, loss 0.255398, acc 0.92\n",
      "2018-01-08T04:49:59.513096: step 1634, loss 0.270326, acc 0.85\n",
      "2018-01-08T04:49:59.660037: step 1635, loss 0.214144, acc 0.94\n",
      "2018-01-08T04:49:59.811148: step 1636, loss 0.274382, acc 0.88\n",
      "2018-01-08T04:49:59.955818: step 1637, loss 0.243249, acc 0.9\n",
      "2018-01-08T04:50:00.103618: step 1638, loss 0.246057, acc 0.89\n",
      "2018-01-08T04:50:00.251211: step 1639, loss 0.255217, acc 0.85\n",
      "2018-01-08T04:50:00.398043: step 1640, loss 0.269196, acc 0.88\n",
      "2018-01-08T04:50:00.550540: step 1641, loss 0.207605, acc 0.94\n",
      "2018-01-08T04:50:00.700028: step 1642, loss 0.240434, acc 0.89\n",
      "2018-01-08T04:50:00.847480: step 1643, loss 0.231698, acc 0.94\n",
      "2018-01-08T04:50:00.994729: step 1644, loss 0.273149, acc 0.88\n",
      "2018-01-08T04:50:01.143286: step 1645, loss 0.256508, acc 0.87\n",
      "2018-01-08T04:50:01.287839: step 1646, loss 0.257882, acc 0.94\n",
      "2018-01-08T04:50:01.436869: step 1647, loss 0.2282, acc 0.95\n",
      "2018-01-08T04:50:01.581796: step 1648, loss 0.266002, acc 0.88\n",
      "2018-01-08T04:50:01.721321: step 1649, loss 0.260439, acc 0.93\n",
      "2018-01-08T04:50:01.864807: step 1650, loss 0.241987, acc 0.9\n",
      "2018-01-08T04:50:02.006938: step 1651, loss 0.217886, acc 0.9\n",
      "2018-01-08T04:50:02.144413: step 1652, loss 0.256625, acc 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:50:02.291036: step 1653, loss 0.22144, acc 0.95\n",
      "2018-01-08T04:50:02.430577: step 1654, loss 0.2711, acc 0.87\n",
      "2018-01-08T04:50:02.571077: step 1655, loss 0.227319, acc 0.92\n",
      "2018-01-08T04:50:02.705561: step 1656, loss 0.251217, acc 0.9\n",
      "2018-01-08T04:50:02.845541: step 1657, loss 0.253455, acc 0.88\n",
      "2018-01-08T04:50:02.986009: step 1658, loss 0.239393, acc 0.89\n",
      "2018-01-08T04:50:03.132616: step 1659, loss 0.227863, acc 0.93\n",
      "2018-01-08T04:50:03.275749: step 1660, loss 0.238531, acc 0.9\n",
      "2018-01-08T04:50:03.409717: step 1661, loss 0.238573, acc 0.92\n",
      "2018-01-08T04:50:03.544961: step 1662, loss 0.230273, acc 0.92\n",
      "2018-01-08T04:50:03.689280: step 1663, loss 0.283832, acc 0.87\n",
      "2018-01-08T04:50:03.832657: step 1664, loss 0.221429, acc 0.91\n",
      "2018-01-08T04:50:03.974670: step 1665, loss 0.254028, acc 0.84\n",
      "2018-01-08T04:50:04.114593: step 1666, loss 0.226369, acc 0.91\n",
      "2018-01-08T04:50:04.252312: step 1667, loss 0.254152, acc 0.87\n",
      "2018-01-08T04:50:04.392311: step 1668, loss 0.236956, acc 0.94\n",
      "2018-01-08T04:50:04.535889: step 1669, loss 0.24538, acc 0.92\n",
      "2018-01-08T04:50:04.683121: step 1670, loss 0.27098, acc 0.87\n",
      "2018-01-08T04:50:04.835065: step 1671, loss 0.270004, acc 0.9\n",
      "2018-01-08T04:50:04.973574: step 1672, loss 0.233147, acc 0.92\n",
      "2018-01-08T04:50:05.109478: step 1673, loss 0.272822, acc 0.87\n",
      "2018-01-08T04:50:05.248684: step 1674, loss 0.263952, acc 0.85\n",
      "2018-01-08T04:50:05.384603: step 1675, loss 0.239402, acc 0.87\n",
      "2018-01-08T04:50:05.521335: step 1676, loss 0.235436, acc 0.89\n",
      "2018-01-08T04:50:05.666266: step 1677, loss 0.263375, acc 0.88\n",
      "2018-01-08T04:50:05.817506: step 1678, loss 0.227694, acc 0.95\n",
      "2018-01-08T04:50:05.964620: step 1679, loss 0.265681, acc 0.85\n",
      "2018-01-08T04:50:06.112218: step 1680, loss 0.268509, acc 0.89\n",
      "2018-01-08T04:50:06.252107: step 1681, loss 0.234618, acc 0.9\n",
      "2018-01-08T04:50:06.396416: step 1682, loss 0.238384, acc 0.93\n",
      "2018-01-08T04:50:06.540313: step 1683, loss 0.243616, acc 0.9\n",
      "2018-01-08T04:50:06.689334: step 1684, loss 0.231593, acc 0.93\n",
      "2018-01-08T04:50:06.848807: step 1685, loss 0.281974, acc 0.86\n",
      "2018-01-08T04:50:06.988563: step 1686, loss 0.279581, acc 0.8\n",
      "2018-01-08T04:50:07.131438: step 1687, loss 0.269785, acc 0.86\n",
      "2018-01-08T04:50:07.274656: step 1688, loss 0.215058, acc 0.93\n",
      "2018-01-08T04:50:07.411471: step 1689, loss 0.237762, acc 0.89\n",
      "2018-01-08T04:50:07.558632: step 1690, loss 0.241721, acc 0.89\n",
      "2018-01-08T04:50:07.701700: step 1691, loss 0.25656, acc 0.9\n",
      "2018-01-08T04:50:07.855103: step 1692, loss 0.269386, acc 0.89\n",
      "2018-01-08T04:50:08.001257: step 1693, loss 0.255799, acc 0.93\n",
      "2018-01-08T04:50:08.148328: step 1694, loss 0.274626, acc 0.89\n",
      "2018-01-08T04:50:08.297713: step 1695, loss 0.298419, acc 0.88\n",
      "2018-01-08T04:50:08.446732: step 1696, loss 0.250038, acc 0.91\n",
      "2018-01-08T04:50:08.602216: step 1697, loss 0.268679, acc 0.9\n",
      "2018-01-08T04:50:08.754388: step 1698, loss 0.23721, acc 0.89\n",
      "2018-01-08T04:50:08.901787: step 1699, loss 0.224582, acc 0.9\n",
      "2018-01-08T04:50:09.047563: step 1700, loss 0.217174, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:50:09.313771: step 1700, loss 0.302097, acc 0.833929\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1700\n",
      "\n",
      "2018-01-08T04:50:09.523086: step 1701, loss 0.235321, acc 0.92\n",
      "2018-01-08T04:50:09.662550: step 1702, loss 0.252965, acc 0.9\n",
      "2018-01-08T04:50:09.803546: step 1703, loss 0.228882, acc 0.93\n",
      "2018-01-08T04:50:09.945488: step 1704, loss 0.214589, acc 0.9\n",
      "2018-01-08T04:50:10.080679: step 1705, loss 0.22984, acc 0.92\n",
      "2018-01-08T04:50:10.222163: step 1706, loss 0.320424, acc 0.82\n",
      "2018-01-08T04:50:10.360238: step 1707, loss 0.257721, acc 0.88\n",
      "2018-01-08T04:50:10.498342: step 1708, loss 0.282053, acc 0.89\n",
      "2018-01-08T04:50:10.639687: step 1709, loss 0.245026, acc 0.87\n",
      "2018-01-08T04:50:10.780571: step 1710, loss 0.249191, acc 0.9\n",
      "2018-01-08T04:50:10.922361: step 1711, loss 0.251077, acc 0.9\n",
      "2018-01-08T04:50:11.066948: step 1712, loss 0.212371, acc 0.93\n",
      "2018-01-08T04:50:11.215214: step 1713, loss 0.273459, acc 0.89\n",
      "2018-01-08T04:50:11.349245: step 1714, loss 0.216828, acc 0.9\n",
      "2018-01-08T04:50:11.489948: step 1715, loss 0.277283, acc 0.85\n",
      "2018-01-08T04:50:11.628820: step 1716, loss 0.280994, acc 0.85\n",
      "2018-01-08T04:50:11.765355: step 1717, loss 0.225152, acc 0.93\n",
      "2018-01-08T04:50:11.906587: step 1718, loss 0.214783, acc 0.93\n",
      "2018-01-08T04:50:12.050540: step 1719, loss 0.252281, acc 0.91\n",
      "2018-01-08T04:50:12.194460: step 1720, loss 0.279043, acc 0.87\n",
      "2018-01-08T04:50:12.325841: step 1721, loss 0.260343, acc 0.87\n",
      "2018-01-08T04:50:12.468874: step 1722, loss 0.265575, acc 0.86\n",
      "2018-01-08T04:50:12.614225: step 1723, loss 0.210398, acc 0.95\n",
      "2018-01-08T04:50:12.757395: step 1724, loss 0.271514, acc 0.84\n",
      "2018-01-08T04:50:12.903787: step 1725, loss 0.223727, acc 0.86\n",
      "2018-01-08T04:50:13.048322: step 1726, loss 0.268175, acc 0.84\n",
      "2018-01-08T04:50:13.197163: step 1727, loss 0.269558, acc 0.89\n",
      "2018-01-08T04:50:13.329460: step 1728, loss 0.241054, acc 0.88\n",
      "2018-01-08T04:50:13.468085: step 1729, loss 0.229378, acc 0.93\n",
      "2018-01-08T04:50:13.608967: step 1730, loss 0.269342, acc 0.89\n",
      "2018-01-08T04:50:13.758652: step 1731, loss 0.22161, acc 0.94\n",
      "2018-01-08T04:50:13.910655: step 1732, loss 0.220398, acc 0.93\n",
      "2018-01-08T04:50:14.065372: step 1733, loss 0.26752, acc 0.91\n",
      "2018-01-08T04:50:14.213576: step 1734, loss 0.248507, acc 0.88\n",
      "2018-01-08T04:50:14.356367: step 1735, loss 0.283779, acc 0.83\n",
      "2018-01-08T04:50:14.499975: step 1736, loss 0.247019, acc 0.9\n",
      "2018-01-08T04:50:14.648195: step 1737, loss 0.251828, acc 0.88\n",
      "2018-01-08T04:50:14.797224: step 1738, loss 0.230899, acc 0.95\n",
      "2018-01-08T04:50:14.948277: step 1739, loss 0.270243, acc 0.82\n",
      "2018-01-08T04:50:15.093598: step 1740, loss 0.204452, acc 0.95\n",
      "2018-01-08T04:50:15.240903: step 1741, loss 0.274039, acc 0.85\n",
      "2018-01-08T04:50:15.383546: step 1742, loss 0.221551, acc 0.91\n",
      "2018-01-08T04:50:15.531888: step 1743, loss 0.278851, acc 0.84\n",
      "2018-01-08T04:50:15.681710: step 1744, loss 0.288899, acc 0.8\n",
      "2018-01-08T04:50:15.834087: step 1745, loss 0.279511, acc 0.83\n",
      "2018-01-08T04:50:15.978028: step 1746, loss 0.275019, acc 0.87\n",
      "2018-01-08T04:50:16.131844: step 1747, loss 0.251578, acc 0.89\n",
      "2018-01-08T04:50:16.283164: step 1748, loss 0.247884, acc 0.9\n",
      "2018-01-08T04:50:16.432685: step 1749, loss 0.277515, acc 0.85\n",
      "2018-01-08T04:50:16.586619: step 1750, loss 0.252258, acc 0.88\n",
      "2018-01-08T04:50:16.732135: step 1751, loss 0.251139, acc 0.89\n",
      "2018-01-08T04:50:16.881950: step 1752, loss 0.251273, acc 0.9\n",
      "2018-01-08T04:50:17.030610: step 1753, loss 0.247489, acc 0.91\n",
      "2018-01-08T04:50:17.181031: step 1754, loss 0.200318, acc 0.94\n",
      "2018-01-08T04:50:17.324727: step 1755, loss 0.212178, acc 0.95\n",
      "2018-01-08T04:50:17.467908: step 1756, loss 0.234023, acc 0.88\n",
      "2018-01-08T04:50:17.626288: step 1757, loss 0.234875, acc 0.91\n",
      "2018-01-08T04:50:17.859100: step 1758, loss 0.20654, acc 0.95\n",
      "2018-01-08T04:50:18.079176: step 1759, loss 0.232472, acc 0.89\n",
      "2018-01-08T04:50:18.302186: step 1760, loss 0.260392, acc 0.84\n",
      "2018-01-08T04:50:18.526567: step 1761, loss 0.256978, acc 0.85\n",
      "2018-01-08T04:50:18.749154: step 1762, loss 0.231517, acc 0.89\n",
      "2018-01-08T04:50:18.996758: step 1763, loss 0.229603, acc 0.89\n",
      "2018-01-08T04:50:19.228463: step 1764, loss 0.24086, acc 0.94\n",
      "2018-01-08T04:50:19.443371: step 1765, loss 0.271055, acc 0.86\n",
      "2018-01-08T04:50:19.672737: step 1766, loss 0.241524, acc 0.92\n",
      "2018-01-08T04:50:19.894712: step 1767, loss 0.260583, acc 0.88\n",
      "2018-01-08T04:50:20.115697: step 1768, loss 0.207234, acc 0.92\n",
      "2018-01-08T04:50:20.340279: step 1769, loss 0.236275, acc 0.9\n",
      "2018-01-08T04:50:20.480054: step 1770, loss 0.289084, acc 0.909091\n",
      "2018-01-08T04:50:20.721816: step 1771, loss 0.226528, acc 0.89\n",
      "2018-01-08T04:50:20.946575: step 1772, loss 0.303587, acc 0.86\n",
      "2018-01-08T04:50:21.177773: step 1773, loss 0.254428, acc 0.9\n",
      "2018-01-08T04:50:21.406854: step 1774, loss 0.225925, acc 0.95\n",
      "2018-01-08T04:50:21.613958: step 1775, loss 0.216539, acc 0.91\n",
      "2018-01-08T04:50:21.774680: step 1776, loss 0.217823, acc 0.89\n",
      "2018-01-08T04:50:21.930826: step 1777, loss 0.289515, acc 0.82\n",
      "2018-01-08T04:50:22.089961: step 1778, loss 0.203076, acc 0.9\n",
      "2018-01-08T04:50:22.251212: step 1779, loss 0.194823, acc 0.94\n",
      "2018-01-08T04:50:22.406986: step 1780, loss 0.234955, acc 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:50:22.569921: step 1781, loss 0.20302, acc 0.95\n",
      "2018-01-08T04:50:22.726279: step 1782, loss 0.264849, acc 0.89\n",
      "2018-01-08T04:50:22.886649: step 1783, loss 0.263842, acc 0.89\n",
      "2018-01-08T04:50:23.050053: step 1784, loss 0.210093, acc 0.93\n",
      "2018-01-08T04:50:23.211755: step 1785, loss 0.265543, acc 0.9\n",
      "2018-01-08T04:50:23.366749: step 1786, loss 0.243213, acc 0.9\n",
      "2018-01-08T04:50:23.519716: step 1787, loss 0.297265, acc 0.82\n",
      "2018-01-08T04:50:23.677792: step 1788, loss 0.227997, acc 0.92\n",
      "2018-01-08T04:50:23.837812: step 1789, loss 0.193905, acc 0.96\n",
      "2018-01-08T04:50:23.999939: step 1790, loss 0.240782, acc 0.87\n",
      "2018-01-08T04:50:24.159208: step 1791, loss 0.223139, acc 0.93\n",
      "2018-01-08T04:50:24.318226: step 1792, loss 0.232378, acc 0.92\n",
      "2018-01-08T04:50:24.484226: step 1793, loss 0.245359, acc 0.89\n",
      "2018-01-08T04:50:24.644099: step 1794, loss 0.267457, acc 0.85\n",
      "2018-01-08T04:50:24.822192: step 1795, loss 0.245133, acc 0.85\n",
      "2018-01-08T04:50:24.991553: step 1796, loss 0.255308, acc 0.86\n",
      "2018-01-08T04:50:25.163825: step 1797, loss 0.220946, acc 0.91\n",
      "2018-01-08T04:50:25.338406: step 1798, loss 0.252126, acc 0.89\n",
      "2018-01-08T04:50:25.497278: step 1799, loss 0.249499, acc 0.9\n",
      "2018-01-08T04:50:25.652437: step 1800, loss 0.245021, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:50:25.935076: step 1800, loss 0.300134, acc 0.838528\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1800\n",
      "\n",
      "2018-01-08T04:50:26.145181: step 1801, loss 0.222201, acc 0.9\n",
      "2018-01-08T04:50:26.287123: step 1802, loss 0.213473, acc 0.91\n",
      "2018-01-08T04:50:26.429868: step 1803, loss 0.202335, acc 0.9\n",
      "2018-01-08T04:50:26.573245: step 1804, loss 0.241284, acc 0.88\n",
      "2018-01-08T04:50:26.726449: step 1805, loss 0.204803, acc 0.93\n",
      "2018-01-08T04:50:26.871371: step 1806, loss 0.238835, acc 0.9\n",
      "2018-01-08T04:50:27.015086: step 1807, loss 0.217109, acc 0.92\n",
      "2018-01-08T04:50:27.162292: step 1808, loss 0.241414, acc 0.92\n",
      "2018-01-08T04:50:27.313220: step 1809, loss 0.206834, acc 0.91\n",
      "2018-01-08T04:50:27.455724: step 1810, loss 0.19546, acc 0.92\n",
      "2018-01-08T04:50:27.597132: step 1811, loss 0.283975, acc 0.86\n",
      "2018-01-08T04:50:27.743974: step 1812, loss 0.244376, acc 0.93\n",
      "2018-01-08T04:50:27.894281: step 1813, loss 0.202701, acc 0.93\n",
      "2018-01-08T04:50:28.037186: step 1814, loss 0.234505, acc 0.87\n",
      "2018-01-08T04:50:28.184315: step 1815, loss 0.239205, acc 0.9\n",
      "2018-01-08T04:50:28.329346: step 1816, loss 0.252107, acc 0.92\n",
      "2018-01-08T04:50:28.471732: step 1817, loss 0.200072, acc 0.96\n",
      "2018-01-08T04:50:28.617320: step 1818, loss 0.214994, acc 0.89\n",
      "2018-01-08T04:50:28.762634: step 1819, loss 0.210999, acc 0.92\n",
      "2018-01-08T04:50:28.916953: step 1820, loss 0.242698, acc 0.89\n",
      "2018-01-08T04:50:29.069101: step 1821, loss 0.233851, acc 0.92\n",
      "2018-01-08T04:50:29.217024: step 1822, loss 0.206714, acc 0.93\n",
      "2018-01-08T04:50:29.366660: step 1823, loss 0.233532, acc 0.88\n",
      "2018-01-08T04:50:29.515016: step 1824, loss 0.246394, acc 0.88\n",
      "2018-01-08T04:50:29.675736: step 1825, loss 0.234269, acc 0.92\n",
      "2018-01-08T04:50:29.836038: step 1826, loss 0.202584, acc 0.94\n",
      "2018-01-08T04:50:30.002265: step 1827, loss 0.281852, acc 0.83\n",
      "2018-01-08T04:50:30.159481: step 1828, loss 0.229606, acc 0.87\n",
      "2018-01-08T04:50:30.314548: step 1829, loss 0.214253, acc 0.9\n",
      "2018-01-08T04:50:30.475566: step 1830, loss 0.25059, acc 0.87\n",
      "2018-01-08T04:50:30.641959: step 1831, loss 0.223993, acc 0.93\n",
      "2018-01-08T04:50:30.801384: step 1832, loss 0.219768, acc 0.94\n",
      "2018-01-08T04:50:30.963315: step 1833, loss 0.243615, acc 0.94\n",
      "2018-01-08T04:50:31.127104: step 1834, loss 0.211468, acc 0.91\n",
      "2018-01-08T04:50:31.285093: step 1835, loss 0.228562, acc 0.9\n",
      "2018-01-08T04:50:31.441264: step 1836, loss 0.234539, acc 0.92\n",
      "2018-01-08T04:50:31.599594: step 1837, loss 0.259575, acc 0.9\n",
      "2018-01-08T04:50:31.757305: step 1838, loss 0.23339, acc 0.92\n",
      "2018-01-08T04:50:31.924760: step 1839, loss 0.198378, acc 0.9\n",
      "2018-01-08T04:50:32.084285: step 1840, loss 0.241951, acc 0.89\n",
      "2018-01-08T04:50:32.242578: step 1841, loss 0.225499, acc 0.88\n",
      "2018-01-08T04:50:32.397184: step 1842, loss 0.193079, acc 0.94\n",
      "2018-01-08T04:50:32.558096: step 1843, loss 0.21111, acc 0.92\n",
      "2018-01-08T04:50:32.717552: step 1844, loss 0.198644, acc 0.94\n",
      "2018-01-08T04:50:32.875099: step 1845, loss 0.236845, acc 0.9\n",
      "2018-01-08T04:50:33.033939: step 1846, loss 0.207063, acc 0.95\n",
      "2018-01-08T04:50:33.193536: step 1847, loss 0.252195, acc 0.86\n",
      "2018-01-08T04:50:33.349083: step 1848, loss 0.190018, acc 0.96\n",
      "2018-01-08T04:50:33.506947: step 1849, loss 0.227739, acc 0.91\n",
      "2018-01-08T04:50:33.659066: step 1850, loss 0.198528, acc 0.93\n",
      "2018-01-08T04:50:33.806511: step 1851, loss 0.184402, acc 0.93\n",
      "2018-01-08T04:50:33.946305: step 1852, loss 0.221865, acc 0.93\n",
      "2018-01-08T04:50:34.101291: step 1853, loss 0.215953, acc 0.93\n",
      "2018-01-08T04:50:34.243363: step 1854, loss 0.236961, acc 0.92\n",
      "2018-01-08T04:50:34.424300: step 1855, loss 0.194571, acc 0.98\n",
      "2018-01-08T04:50:34.572875: step 1856, loss 0.220523, acc 0.92\n",
      "2018-01-08T04:50:34.720904: step 1857, loss 0.202191, acc 0.95\n",
      "2018-01-08T04:50:34.866328: step 1858, loss 0.223762, acc 0.91\n",
      "2018-01-08T04:50:35.021819: step 1859, loss 0.183333, acc 0.93\n",
      "2018-01-08T04:50:35.168465: step 1860, loss 0.237858, acc 0.85\n",
      "2018-01-08T04:50:35.321076: step 1861, loss 0.226353, acc 0.88\n",
      "2018-01-08T04:50:35.460619: step 1862, loss 0.242187, acc 0.85\n",
      "2018-01-08T04:50:35.609381: step 1863, loss 0.213561, acc 0.94\n",
      "2018-01-08T04:50:35.753825: step 1864, loss 0.226789, acc 0.9\n",
      "2018-01-08T04:50:35.911575: step 1865, loss 0.22164, acc 0.95\n",
      "2018-01-08T04:50:36.057086: step 1866, loss 0.225471, acc 0.88\n",
      "2018-01-08T04:50:36.199955: step 1867, loss 0.234547, acc 0.89\n",
      "2018-01-08T04:50:36.343214: step 1868, loss 0.241101, acc 0.88\n",
      "2018-01-08T04:50:36.485490: step 1869, loss 0.231805, acc 0.9\n",
      "2018-01-08T04:50:36.638199: step 1870, loss 0.210862, acc 0.91\n",
      "2018-01-08T04:50:36.782100: step 1871, loss 0.228468, acc 0.89\n",
      "2018-01-08T04:50:36.935143: step 1872, loss 0.239082, acc 0.91\n",
      "2018-01-08T04:50:37.088391: step 1873, loss 0.255132, acc 0.85\n",
      "2018-01-08T04:50:37.234478: step 1874, loss 0.199135, acc 0.91\n",
      "2018-01-08T04:50:37.383032: step 1875, loss 0.196174, acc 0.93\n",
      "2018-01-08T04:50:37.525258: step 1876, loss 0.226061, acc 0.9\n",
      "2018-01-08T04:50:37.680685: step 1877, loss 0.215399, acc 0.9\n",
      "2018-01-08T04:50:37.849795: step 1878, loss 0.246981, acc 0.89\n",
      "2018-01-08T04:50:38.007437: step 1879, loss 0.299036, acc 0.87\n",
      "2018-01-08T04:50:38.175102: step 1880, loss 0.189776, acc 0.96\n",
      "2018-01-08T04:50:38.331296: step 1881, loss 0.233107, acc 0.89\n",
      "2018-01-08T04:50:38.490100: step 1882, loss 0.222186, acc 0.9\n",
      "2018-01-08T04:50:38.656807: step 1883, loss 0.220853, acc 0.95\n",
      "2018-01-08T04:50:38.820004: step 1884, loss 0.206455, acc 0.95\n",
      "2018-01-08T04:50:38.989473: step 1885, loss 0.233723, acc 0.92\n",
      "2018-01-08T04:50:39.149092: step 1886, loss 0.217978, acc 0.9\n",
      "2018-01-08T04:50:39.308478: step 1887, loss 0.234803, acc 0.9\n",
      "2018-01-08T04:50:39.470971: step 1888, loss 0.22882, acc 0.88\n",
      "2018-01-08T04:50:39.633450: step 1889, loss 0.188084, acc 0.93\n",
      "2018-01-08T04:50:39.797529: step 1890, loss 0.251047, acc 0.91\n",
      "2018-01-08T04:50:39.958562: step 1891, loss 0.23687, acc 0.88\n",
      "2018-01-08T04:50:40.119053: step 1892, loss 0.245904, acc 0.86\n",
      "2018-01-08T04:50:40.278538: step 1893, loss 0.233278, acc 0.9\n",
      "2018-01-08T04:50:40.441183: step 1894, loss 0.21602, acc 0.92\n",
      "2018-01-08T04:50:40.604388: step 1895, loss 0.250713, acc 0.86\n",
      "2018-01-08T04:50:40.765498: step 1896, loss 0.185851, acc 0.93\n",
      "2018-01-08T04:50:40.937503: step 1897, loss 0.222018, acc 0.91\n",
      "2018-01-08T04:50:41.099522: step 1898, loss 0.233669, acc 0.88\n",
      "2018-01-08T04:50:41.258919: step 1899, loss 0.277935, acc 0.86\n",
      "2018-01-08T04:50:41.410961: step 1900, loss 0.208978, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:50:41.702949: step 1900, loss 0.290984, acc 0.844149\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-1900\n",
      "\n",
      "2018-01-08T04:50:41.917973: step 1901, loss 0.261116, acc 0.87\n",
      "2018-01-08T04:50:42.062074: step 1902, loss 0.204208, acc 0.93\n",
      "2018-01-08T04:50:42.212996: step 1903, loss 0.241119, acc 0.89\n",
      "2018-01-08T04:50:42.359333: step 1904, loss 0.237898, acc 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:50:42.508138: step 1905, loss 0.214444, acc 0.91\n",
      "2018-01-08T04:50:42.656406: step 1906, loss 0.207981, acc 0.97\n",
      "2018-01-08T04:50:42.809944: step 1907, loss 0.200632, acc 0.92\n",
      "2018-01-08T04:50:42.970457: step 1908, loss 0.182488, acc 0.96\n",
      "2018-01-08T04:50:43.119033: step 1909, loss 0.248357, acc 0.88\n",
      "2018-01-08T04:50:43.270634: step 1910, loss 0.206617, acc 0.91\n",
      "2018-01-08T04:50:43.413629: step 1911, loss 0.231682, acc 0.89\n",
      "2018-01-08T04:50:43.554730: step 1912, loss 0.241563, acc 0.91\n",
      "2018-01-08T04:50:43.706171: step 1913, loss 0.212174, acc 0.94\n",
      "2018-01-08T04:50:43.850656: step 1914, loss 0.269602, acc 0.9\n",
      "2018-01-08T04:50:43.995206: step 1915, loss 0.176529, acc 0.97\n",
      "2018-01-08T04:50:44.137314: step 1916, loss 0.240712, acc 0.89\n",
      "2018-01-08T04:50:44.277892: step 1917, loss 0.261451, acc 0.86\n",
      "2018-01-08T04:50:44.425379: step 1918, loss 0.260399, acc 0.88\n",
      "2018-01-08T04:50:44.566589: step 1919, loss 0.238973, acc 0.91\n",
      "2018-01-08T04:50:44.727482: step 1920, loss 0.194337, acc 0.94\n",
      "2018-01-08T04:50:44.880140: step 1921, loss 0.222488, acc 0.91\n",
      "2018-01-08T04:50:45.025751: step 1922, loss 0.234034, acc 0.91\n",
      "2018-01-08T04:50:45.170806: step 1923, loss 0.203266, acc 0.91\n",
      "2018-01-08T04:50:45.314695: step 1924, loss 0.202179, acc 0.89\n",
      "2018-01-08T04:50:45.459729: step 1925, loss 0.270359, acc 0.86\n",
      "2018-01-08T04:50:45.602939: step 1926, loss 0.218999, acc 0.93\n",
      "2018-01-08T04:50:45.737386: step 1927, loss 0.242414, acc 0.88\n",
      "2018-01-08T04:50:45.878401: step 1928, loss 0.222365, acc 0.89\n",
      "2018-01-08T04:50:46.019455: step 1929, loss 0.212305, acc 0.85\n",
      "2018-01-08T04:50:46.158699: step 1930, loss 0.224986, acc 0.91\n",
      "2018-01-08T04:50:46.299744: step 1931, loss 0.195871, acc 0.95\n",
      "2018-01-08T04:50:46.436800: step 1932, loss 0.193865, acc 0.9\n",
      "2018-01-08T04:50:46.578364: step 1933, loss 0.204616, acc 0.91\n",
      "2018-01-08T04:50:46.717107: step 1934, loss 0.226491, acc 0.9\n",
      "2018-01-08T04:50:46.863091: step 1935, loss 0.192903, acc 0.94\n",
      "2018-01-08T04:50:46.999270: step 1936, loss 0.2142, acc 0.92\n",
      "2018-01-08T04:50:47.145332: step 1937, loss 0.233051, acc 0.89\n",
      "2018-01-08T04:50:47.286342: step 1938, loss 0.21718, acc 0.9\n",
      "2018-01-08T04:50:47.422403: step 1939, loss 0.210391, acc 0.89\n",
      "2018-01-08T04:50:47.557644: step 1940, loss 0.249691, acc 0.9\n",
      "2018-01-08T04:50:47.702697: step 1941, loss 0.215677, acc 0.91\n",
      "2018-01-08T04:50:47.848785: step 1942, loss 0.226048, acc 0.88\n",
      "2018-01-08T04:50:47.994752: step 1943, loss 0.235327, acc 0.85\n",
      "2018-01-08T04:50:48.132764: step 1944, loss 0.247342, acc 0.87\n",
      "2018-01-08T04:50:48.276036: step 1945, loss 0.245902, acc 0.89\n",
      "2018-01-08T04:50:48.417681: step 1946, loss 0.253083, acc 0.89\n",
      "2018-01-08T04:50:48.501873: step 1947, loss 0.319495, acc 0.772727\n",
      "2018-01-08T04:50:48.649840: step 1948, loss 0.220084, acc 0.9\n",
      "2018-01-08T04:50:48.797448: step 1949, loss 0.206988, acc 0.91\n",
      "2018-01-08T04:50:48.936569: step 1950, loss 0.227636, acc 0.91\n",
      "2018-01-08T04:50:49.080896: step 1951, loss 0.191903, acc 0.95\n",
      "2018-01-08T04:50:49.226158: step 1952, loss 0.209269, acc 0.9\n",
      "2018-01-08T04:50:49.361407: step 1953, loss 0.180389, acc 0.95\n",
      "2018-01-08T04:50:49.495888: step 1954, loss 0.204155, acc 0.93\n",
      "2018-01-08T04:50:49.641286: step 1955, loss 0.232684, acc 0.89\n",
      "2018-01-08T04:50:49.789030: step 1956, loss 0.199453, acc 0.93\n",
      "2018-01-08T04:50:49.935618: step 1957, loss 0.189052, acc 0.91\n",
      "2018-01-08T04:50:50.081418: step 1958, loss 0.177781, acc 0.95\n",
      "2018-01-08T04:50:50.226974: step 1959, loss 0.206985, acc 0.92\n",
      "2018-01-08T04:50:50.368678: step 1960, loss 0.196324, acc 0.94\n",
      "2018-01-08T04:50:50.513962: step 1961, loss 0.195627, acc 0.94\n",
      "2018-01-08T04:50:50.661321: step 1962, loss 0.215361, acc 0.9\n",
      "2018-01-08T04:50:50.813608: step 1963, loss 0.216729, acc 0.89\n",
      "2018-01-08T04:50:50.956746: step 1964, loss 0.22245, acc 0.9\n",
      "2018-01-08T04:50:51.115613: step 1965, loss 0.227489, acc 0.89\n",
      "2018-01-08T04:50:51.264364: step 1966, loss 0.237251, acc 0.9\n",
      "2018-01-08T04:50:51.403977: step 1967, loss 0.234392, acc 0.86\n",
      "2018-01-08T04:50:51.548114: step 1968, loss 0.181214, acc 0.94\n",
      "2018-01-08T04:50:51.701292: step 1969, loss 0.206496, acc 0.96\n",
      "2018-01-08T04:50:51.844916: step 1970, loss 0.22801, acc 0.91\n",
      "2018-01-08T04:50:51.988185: step 1971, loss 0.177923, acc 0.98\n",
      "2018-01-08T04:50:52.137058: step 1972, loss 0.183724, acc 0.94\n",
      "2018-01-08T04:50:52.285663: step 1973, loss 0.216665, acc 0.88\n",
      "2018-01-08T04:50:52.428116: step 1974, loss 0.213851, acc 0.9\n",
      "2018-01-08T04:50:52.584878: step 1975, loss 0.204959, acc 0.93\n",
      "2018-01-08T04:50:52.728095: step 1976, loss 0.199351, acc 0.93\n",
      "2018-01-08T04:50:52.878087: step 1977, loss 0.201595, acc 0.95\n",
      "2018-01-08T04:50:53.027886: step 1978, loss 0.216671, acc 0.9\n",
      "2018-01-08T04:50:53.176247: step 1979, loss 0.210215, acc 0.89\n",
      "2018-01-08T04:50:53.317136: step 1980, loss 0.243509, acc 0.88\n",
      "2018-01-08T04:50:53.462614: step 1981, loss 0.21033, acc 0.91\n",
      "2018-01-08T04:50:53.605953: step 1982, loss 0.219431, acc 0.91\n",
      "2018-01-08T04:50:53.744789: step 1983, loss 0.234928, acc 0.89\n",
      "2018-01-08T04:50:53.888968: step 1984, loss 0.234927, acc 0.83\n",
      "2018-01-08T04:50:54.026214: step 1985, loss 0.201595, acc 0.9\n",
      "2018-01-08T04:50:54.167008: step 1986, loss 0.212542, acc 0.92\n",
      "2018-01-08T04:50:54.304476: step 1987, loss 0.176413, acc 0.93\n",
      "2018-01-08T04:50:54.448586: step 1988, loss 0.198784, acc 0.91\n",
      "2018-01-08T04:50:54.594675: step 1989, loss 0.246723, acc 0.91\n",
      "2018-01-08T04:50:54.735231: step 1990, loss 0.214035, acc 0.93\n",
      "2018-01-08T04:50:54.890609: step 1991, loss 0.220711, acc 0.88\n",
      "2018-01-08T04:50:55.028376: step 1992, loss 0.205308, acc 0.93\n",
      "2018-01-08T04:50:55.170827: step 1993, loss 0.224253, acc 0.9\n",
      "2018-01-08T04:50:55.307101: step 1994, loss 0.213972, acc 0.92\n",
      "2018-01-08T04:50:55.446817: step 1995, loss 0.202816, acc 0.91\n",
      "2018-01-08T04:50:55.584855: step 1996, loss 0.17728, acc 0.96\n",
      "2018-01-08T04:50:55.722052: step 1997, loss 0.201976, acc 0.91\n",
      "2018-01-08T04:50:55.865577: step 1998, loss 0.204148, acc 0.94\n",
      "2018-01-08T04:50:56.007450: step 1999, loss 0.253725, acc 0.85\n",
      "2018-01-08T04:50:56.145079: step 2000, loss 0.217798, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:50:56.403848: step 2000, loss 0.279046, acc 0.846704\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2000\n",
      "\n",
      "2018-01-08T04:50:56.605180: step 2001, loss 0.220432, acc 0.9\n",
      "2018-01-08T04:50:56.745079: step 2002, loss 0.181236, acc 0.94\n",
      "2018-01-08T04:50:56.887821: step 2003, loss 0.233726, acc 0.91\n",
      "2018-01-08T04:50:57.028398: step 2004, loss 0.204933, acc 0.9\n",
      "2018-01-08T04:50:57.173403: step 2005, loss 0.225009, acc 0.93\n",
      "2018-01-08T04:50:57.317166: step 2006, loss 0.193243, acc 0.94\n",
      "2018-01-08T04:50:57.450786: step 2007, loss 0.255124, acc 0.85\n",
      "2018-01-08T04:50:57.592366: step 2008, loss 0.212269, acc 0.91\n",
      "2018-01-08T04:50:57.736716: step 2009, loss 0.185013, acc 0.94\n",
      "2018-01-08T04:50:57.881010: step 2010, loss 0.199893, acc 0.94\n",
      "2018-01-08T04:50:58.022820: step 2011, loss 0.230921, acc 0.9\n",
      "2018-01-08T04:50:58.166185: step 2012, loss 0.168686, acc 0.95\n",
      "2018-01-08T04:50:58.305897: step 2013, loss 0.204165, acc 0.91\n",
      "2018-01-08T04:50:58.444358: step 2014, loss 0.243335, acc 0.88\n",
      "2018-01-08T04:50:58.585910: step 2015, loss 0.222519, acc 0.87\n",
      "2018-01-08T04:50:58.726167: step 2016, loss 0.204998, acc 0.93\n",
      "2018-01-08T04:50:58.891252: step 2017, loss 0.202188, acc 0.92\n",
      "2018-01-08T04:50:59.040442: step 2018, loss 0.199991, acc 0.93\n",
      "2018-01-08T04:50:59.182999: step 2019, loss 0.219738, acc 0.9\n",
      "2018-01-08T04:50:59.324679: step 2020, loss 0.170926, acc 0.95\n",
      "2018-01-08T04:50:59.465557: step 2021, loss 0.21097, acc 0.93\n",
      "2018-01-08T04:50:59.600853: step 2022, loss 0.224226, acc 0.9\n",
      "2018-01-08T04:50:59.748948: step 2023, loss 0.201363, acc 0.92\n",
      "2018-01-08T04:50:59.888814: step 2024, loss 0.184315, acc 0.91\n",
      "2018-01-08T04:51:00.029238: step 2025, loss 0.179268, acc 0.93\n",
      "2018-01-08T04:51:00.172829: step 2026, loss 0.199932, acc 0.92\n",
      "2018-01-08T04:51:00.311346: step 2027, loss 0.209872, acc 0.9\n",
      "2018-01-08T04:51:00.448610: step 2028, loss 0.221332, acc 0.89\n",
      "2018-01-08T04:51:00.588668: step 2029, loss 0.213466, acc 0.9\n",
      "2018-01-08T04:51:00.727074: step 2030, loss 0.188597, acc 0.94\n",
      "2018-01-08T04:51:00.873479: step 2031, loss 0.215963, acc 0.85\n",
      "2018-01-08T04:51:01.017675: step 2032, loss 0.228079, acc 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:51:01.154895: step 2033, loss 0.238907, acc 0.93\n",
      "2018-01-08T04:51:01.295017: step 2034, loss 0.204653, acc 0.91\n",
      "2018-01-08T04:51:01.432248: step 2035, loss 0.189901, acc 0.94\n",
      "2018-01-08T04:51:01.570745: step 2036, loss 0.165691, acc 0.94\n",
      "2018-01-08T04:51:01.719946: step 2037, loss 0.207134, acc 0.91\n",
      "2018-01-08T04:51:01.864180: step 2038, loss 0.2011, acc 0.91\n",
      "2018-01-08T04:51:02.010672: step 2039, loss 0.254067, acc 0.81\n",
      "2018-01-08T04:51:02.162359: step 2040, loss 0.176429, acc 0.94\n",
      "2018-01-08T04:51:02.305121: step 2041, loss 0.185013, acc 0.91\n",
      "2018-01-08T04:51:02.452072: step 2042, loss 0.200036, acc 0.92\n",
      "2018-01-08T04:51:02.606792: step 2043, loss 0.238813, acc 0.91\n",
      "2018-01-08T04:51:02.755749: step 2044, loss 0.184272, acc 0.94\n",
      "2018-01-08T04:51:02.904349: step 2045, loss 0.195736, acc 0.94\n",
      "2018-01-08T04:51:03.052896: step 2046, loss 0.213269, acc 0.91\n",
      "2018-01-08T04:51:03.200303: step 2047, loss 0.203952, acc 0.9\n",
      "2018-01-08T04:51:03.341362: step 2048, loss 0.209923, acc 0.87\n",
      "2018-01-08T04:51:03.481200: step 2049, loss 0.215679, acc 0.88\n",
      "2018-01-08T04:51:03.633025: step 2050, loss 0.197389, acc 0.9\n",
      "2018-01-08T04:51:03.777466: step 2051, loss 0.194502, acc 0.96\n",
      "2018-01-08T04:51:03.926569: step 2052, loss 0.209591, acc 0.92\n",
      "2018-01-08T04:51:04.066712: step 2053, loss 0.248117, acc 0.85\n",
      "2018-01-08T04:51:04.210984: step 2054, loss 0.168126, acc 0.98\n",
      "2018-01-08T04:51:04.360823: step 2055, loss 0.220379, acc 0.88\n",
      "2018-01-08T04:51:04.505651: step 2056, loss 0.191951, acc 0.96\n",
      "2018-01-08T04:51:04.654802: step 2057, loss 0.224432, acc 0.91\n",
      "2018-01-08T04:51:04.806507: step 2058, loss 0.209282, acc 0.91\n",
      "2018-01-08T04:51:04.956636: step 2059, loss 0.189977, acc 0.87\n",
      "2018-01-08T04:51:05.110701: step 2060, loss 0.238709, acc 0.88\n",
      "2018-01-08T04:51:05.253164: step 2061, loss 0.195755, acc 0.94\n",
      "2018-01-08T04:51:05.388919: step 2062, loss 0.198128, acc 0.93\n",
      "2018-01-08T04:51:05.534691: step 2063, loss 0.240297, acc 0.88\n",
      "2018-01-08T04:51:05.729295: step 2064, loss 0.226532, acc 0.91\n",
      "2018-01-08T04:51:05.960235: step 2065, loss 0.21395, acc 0.91\n",
      "2018-01-08T04:51:06.187275: step 2066, loss 0.248939, acc 0.85\n",
      "2018-01-08T04:51:06.407827: step 2067, loss 0.19968, acc 0.91\n",
      "2018-01-08T04:51:06.628719: step 2068, loss 0.209571, acc 0.92\n",
      "2018-01-08T04:51:06.863328: step 2069, loss 0.17802, acc 0.94\n",
      "2018-01-08T04:51:07.086218: step 2070, loss 0.191571, acc 0.97\n",
      "2018-01-08T04:51:07.308604: step 2071, loss 0.166155, acc 0.98\n",
      "2018-01-08T04:51:07.531547: step 2072, loss 0.215587, acc 0.9\n",
      "2018-01-08T04:51:07.762761: step 2073, loss 0.162855, acc 0.97\n",
      "2018-01-08T04:51:07.989031: step 2074, loss 0.152664, acc 0.96\n",
      "2018-01-08T04:51:08.216529: step 2075, loss 0.216543, acc 0.91\n",
      "2018-01-08T04:51:08.449215: step 2076, loss 0.186501, acc 0.92\n",
      "2018-01-08T04:51:08.673701: step 2077, loss 0.242623, acc 0.92\n",
      "2018-01-08T04:51:08.900896: step 2078, loss 0.184405, acc 0.89\n",
      "2018-01-08T04:51:09.122873: step 2079, loss 0.212382, acc 0.92\n",
      "2018-01-08T04:51:09.344751: step 2080, loss 0.22532, acc 0.91\n",
      "2018-01-08T04:51:09.575774: step 2081, loss 0.196474, acc 0.94\n",
      "2018-01-08T04:51:09.756267: step 2082, loss 0.20921, acc 0.91\n",
      "2018-01-08T04:51:09.921088: step 2083, loss 0.207962, acc 0.94\n",
      "2018-01-08T04:51:10.084131: step 2084, loss 0.204596, acc 0.9\n",
      "2018-01-08T04:51:10.254544: step 2085, loss 0.175462, acc 0.96\n",
      "2018-01-08T04:51:10.413348: step 2086, loss 0.182728, acc 0.92\n",
      "2018-01-08T04:51:10.569310: step 2087, loss 0.205611, acc 0.9\n",
      "2018-01-08T04:51:10.735285: step 2088, loss 0.251566, acc 0.86\n",
      "2018-01-08T04:51:10.900793: step 2089, loss 0.148665, acc 0.97\n",
      "2018-01-08T04:51:11.051815: step 2090, loss 0.171846, acc 0.93\n",
      "2018-01-08T04:51:11.218976: step 2091, loss 0.213706, acc 0.88\n",
      "2018-01-08T04:51:11.371186: step 2092, loss 0.209031, acc 0.93\n",
      "2018-01-08T04:51:11.535634: step 2093, loss 0.209448, acc 0.89\n",
      "2018-01-08T04:51:11.692638: step 2094, loss 0.206276, acc 0.89\n",
      "2018-01-08T04:51:11.849538: step 2095, loss 0.245065, acc 0.89\n",
      "2018-01-08T04:51:12.006454: step 2096, loss 0.20618, acc 0.89\n",
      "2018-01-08T04:51:12.169935: step 2097, loss 0.190693, acc 0.92\n",
      "2018-01-08T04:51:12.328473: step 2098, loss 0.205566, acc 0.93\n",
      "2018-01-08T04:51:12.490437: step 2099, loss 0.196824, acc 0.91\n",
      "2018-01-08T04:51:12.647396: step 2100, loss 0.215372, acc 0.9\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:51:12.940984: step 2100, loss 0.282422, acc 0.838528\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2100\n",
      "\n",
      "2018-01-08T04:51:13.165831: step 2101, loss 0.198816, acc 0.9\n",
      "2018-01-08T04:51:13.326464: step 2102, loss 0.217765, acc 0.91\n",
      "2018-01-08T04:51:13.499774: step 2103, loss 0.142141, acc 0.97\n",
      "2018-01-08T04:51:13.664211: step 2104, loss 0.196059, acc 0.93\n",
      "2018-01-08T04:51:13.824521: step 2105, loss 0.221776, acc 0.89\n",
      "2018-01-08T04:51:13.968168: step 2106, loss 0.222242, acc 0.86\n",
      "2018-01-08T04:51:14.116335: step 2107, loss 0.205191, acc 0.94\n",
      "2018-01-08T04:51:14.265230: step 2108, loss 0.183656, acc 0.93\n",
      "2018-01-08T04:51:14.404684: step 2109, loss 0.1739, acc 0.95\n",
      "2018-01-08T04:51:14.545545: step 2110, loss 0.2388, acc 0.88\n",
      "2018-01-08T04:51:14.688987: step 2111, loss 0.20228, acc 0.92\n",
      "2018-01-08T04:51:14.834888: step 2112, loss 0.209595, acc 0.91\n",
      "2018-01-08T04:51:14.984116: step 2113, loss 0.189435, acc 0.94\n",
      "2018-01-08T04:51:15.126978: step 2114, loss 0.182959, acc 0.93\n",
      "2018-01-08T04:51:15.273854: step 2115, loss 0.173204, acc 0.94\n",
      "2018-01-08T04:51:15.417354: step 2116, loss 0.18975, acc 0.93\n",
      "2018-01-08T04:51:15.566981: step 2117, loss 0.204576, acc 0.87\n",
      "2018-01-08T04:51:15.710946: step 2118, loss 0.194917, acc 0.91\n",
      "2018-01-08T04:51:15.863478: step 2119, loss 0.197716, acc 0.93\n",
      "2018-01-08T04:51:16.011274: step 2120, loss 0.173484, acc 0.93\n",
      "2018-01-08T04:51:16.163782: step 2121, loss 0.225823, acc 0.92\n",
      "2018-01-08T04:51:16.311030: step 2122, loss 0.200823, acc 0.94\n",
      "2018-01-08T04:51:16.457020: step 2123, loss 0.173459, acc 0.95\n",
      "2018-01-08T04:51:16.546011: step 2124, loss 0.203573, acc 0.909091\n",
      "2018-01-08T04:51:16.699075: step 2125, loss 0.217702, acc 0.9\n",
      "2018-01-08T04:51:16.844862: step 2126, loss 0.230827, acc 0.9\n",
      "2018-01-08T04:51:16.991962: step 2127, loss 0.201965, acc 0.9\n",
      "2018-01-08T04:51:17.142886: step 2128, loss 0.225706, acc 0.85\n",
      "2018-01-08T04:51:17.295324: step 2129, loss 0.174304, acc 0.92\n",
      "2018-01-08T04:51:17.439407: step 2130, loss 0.212974, acc 0.87\n",
      "2018-01-08T04:51:17.578706: step 2131, loss 0.214365, acc 0.9\n",
      "2018-01-08T04:51:17.734970: step 2132, loss 0.207825, acc 0.92\n",
      "2018-01-08T04:51:17.899905: step 2133, loss 0.17025, acc 0.96\n",
      "2018-01-08T04:51:18.058324: step 2134, loss 0.151005, acc 0.98\n",
      "2018-01-08T04:51:18.224373: step 2135, loss 0.209297, acc 0.9\n",
      "2018-01-08T04:51:18.384518: step 2136, loss 0.217642, acc 0.91\n",
      "2018-01-08T04:51:18.542970: step 2137, loss 0.207945, acc 0.9\n",
      "2018-01-08T04:51:18.699736: step 2138, loss 0.207197, acc 0.93\n",
      "2018-01-08T04:51:18.871046: step 2139, loss 0.212195, acc 0.91\n",
      "2018-01-08T04:51:19.035851: step 2140, loss 0.207219, acc 0.91\n",
      "2018-01-08T04:51:19.200739: step 2141, loss 0.236766, acc 0.87\n",
      "2018-01-08T04:51:19.356161: step 2142, loss 0.198169, acc 0.93\n",
      "2018-01-08T04:51:19.513162: step 2143, loss 0.191839, acc 0.94\n",
      "2018-01-08T04:51:19.670625: step 2144, loss 0.196767, acc 0.94\n",
      "2018-01-08T04:51:19.840863: step 2145, loss 0.166155, acc 0.96\n",
      "2018-01-08T04:51:19.999419: step 2146, loss 0.19513, acc 0.93\n",
      "2018-01-08T04:51:20.163617: step 2147, loss 0.187307, acc 0.91\n",
      "2018-01-08T04:51:20.326209: step 2148, loss 0.208661, acc 0.91\n",
      "2018-01-08T04:51:20.491962: step 2149, loss 0.183624, acc 0.93\n",
      "2018-01-08T04:51:20.649922: step 2150, loss 0.171292, acc 0.93\n",
      "2018-01-08T04:51:20.814400: step 2151, loss 0.19361, acc 0.91\n",
      "2018-01-08T04:51:20.976160: step 2152, loss 0.166918, acc 0.94\n",
      "2018-01-08T04:51:21.142554: step 2153, loss 0.155159, acc 0.97\n",
      "2018-01-08T04:51:21.302561: step 2154, loss 0.215561, acc 0.91\n",
      "2018-01-08T04:51:21.463265: step 2155, loss 0.180751, acc 0.94\n",
      "2018-01-08T04:51:21.625238: step 2156, loss 0.215044, acc 0.89\n",
      "2018-01-08T04:51:21.773311: step 2157, loss 0.185022, acc 0.93\n",
      "2018-01-08T04:51:21.923236: step 2158, loss 0.204842, acc 0.93\n",
      "2018-01-08T04:51:22.071456: step 2159, loss 0.204196, acc 0.92\n",
      "2018-01-08T04:51:22.222443: step 2160, loss 0.210172, acc 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:51:22.368193: step 2161, loss 0.202956, acc 0.92\n",
      "2018-01-08T04:51:22.515719: step 2162, loss 0.219617, acc 0.92\n",
      "2018-01-08T04:51:22.660373: step 2163, loss 0.155477, acc 0.97\n",
      "2018-01-08T04:51:22.807608: step 2164, loss 0.188657, acc 0.92\n",
      "2018-01-08T04:51:22.951489: step 2165, loss 0.224164, acc 0.85\n",
      "2018-01-08T04:51:23.098197: step 2166, loss 0.222415, acc 0.86\n",
      "2018-01-08T04:51:23.249129: step 2167, loss 0.25151, acc 0.83\n",
      "2018-01-08T04:51:23.393173: step 2168, loss 0.244123, acc 0.91\n",
      "2018-01-08T04:51:23.540038: step 2169, loss 0.183486, acc 0.93\n",
      "2018-01-08T04:51:23.689590: step 2170, loss 0.206063, acc 0.89\n",
      "2018-01-08T04:51:23.839466: step 2171, loss 0.196885, acc 0.9\n",
      "2018-01-08T04:51:23.981754: step 2172, loss 0.166399, acc 0.95\n",
      "2018-01-08T04:51:24.126791: step 2173, loss 0.195502, acc 0.9\n",
      "2018-01-08T04:51:24.267121: step 2174, loss 0.192167, acc 0.94\n",
      "2018-01-08T04:51:24.418445: step 2175, loss 0.190059, acc 0.88\n",
      "2018-01-08T04:51:24.563807: step 2176, loss 0.195681, acc 0.94\n",
      "2018-01-08T04:51:24.703729: step 2177, loss 0.184069, acc 0.94\n",
      "2018-01-08T04:51:24.856713: step 2178, loss 0.213259, acc 0.92\n",
      "2018-01-08T04:51:25.001085: step 2179, loss 0.212563, acc 0.91\n",
      "2018-01-08T04:51:25.147325: step 2180, loss 0.212205, acc 0.9\n",
      "2018-01-08T04:51:25.289064: step 2181, loss 0.181823, acc 0.93\n",
      "2018-01-08T04:51:25.425636: step 2182, loss 0.214877, acc 0.91\n",
      "2018-01-08T04:51:25.571990: step 2183, loss 0.180356, acc 0.92\n",
      "2018-01-08T04:51:25.711334: step 2184, loss 0.205253, acc 0.94\n",
      "2018-01-08T04:51:25.854465: step 2185, loss 0.243077, acc 0.89\n",
      "2018-01-08T04:51:25.997513: step 2186, loss 0.229441, acc 0.89\n",
      "2018-01-08T04:51:26.136088: step 2187, loss 0.207031, acc 0.92\n",
      "2018-01-08T04:51:26.275622: step 2188, loss 0.195004, acc 0.92\n",
      "2018-01-08T04:51:26.409123: step 2189, loss 0.178692, acc 0.93\n",
      "2018-01-08T04:51:26.549318: step 2190, loss 0.230198, acc 0.89\n",
      "2018-01-08T04:51:26.688450: step 2191, loss 0.175931, acc 0.95\n",
      "2018-01-08T04:51:26.834551: step 2192, loss 0.154513, acc 0.95\n",
      "2018-01-08T04:51:26.976045: step 2193, loss 0.171469, acc 0.95\n",
      "2018-01-08T04:51:27.121112: step 2194, loss 0.205759, acc 0.91\n",
      "2018-01-08T04:51:27.257297: step 2195, loss 0.185542, acc 0.94\n",
      "2018-01-08T04:51:27.387020: step 2196, loss 0.174567, acc 0.96\n",
      "2018-01-08T04:51:27.519165: step 2197, loss 0.165726, acc 0.97\n",
      "2018-01-08T04:51:27.660746: step 2198, loss 0.172852, acc 0.91\n",
      "2018-01-08T04:51:27.803462: step 2199, loss 0.182336, acc 0.94\n",
      "2018-01-08T04:51:27.945795: step 2200, loss 0.163253, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:51:28.218059: step 2200, loss 0.279815, acc 0.832397\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2200\n",
      "\n",
      "2018-01-08T04:51:28.415558: step 2201, loss 0.177061, acc 0.92\n",
      "2018-01-08T04:51:28.561495: step 2202, loss 0.180363, acc 0.93\n",
      "2018-01-08T04:51:28.699100: step 2203, loss 0.201208, acc 0.9\n",
      "2018-01-08T04:51:28.847968: step 2204, loss 0.182921, acc 0.92\n",
      "2018-01-08T04:51:28.989055: step 2205, loss 0.176479, acc 0.94\n",
      "2018-01-08T04:51:29.129938: step 2206, loss 0.16996, acc 0.95\n",
      "2018-01-08T04:51:29.267045: step 2207, loss 0.212519, acc 0.91\n",
      "2018-01-08T04:51:29.403955: step 2208, loss 0.177294, acc 0.93\n",
      "2018-01-08T04:51:29.545100: step 2209, loss 0.225891, acc 0.91\n",
      "2018-01-08T04:51:29.686891: step 2210, loss 0.169226, acc 0.93\n",
      "2018-01-08T04:51:29.833217: step 2211, loss 0.189001, acc 0.92\n",
      "2018-01-08T04:51:29.981568: step 2212, loss 0.229443, acc 0.91\n",
      "2018-01-08T04:51:30.123542: step 2213, loss 0.188765, acc 0.91\n",
      "2018-01-08T04:51:30.271693: step 2214, loss 0.188795, acc 0.94\n",
      "2018-01-08T04:51:30.414120: step 2215, loss 0.182561, acc 0.95\n",
      "2018-01-08T04:51:30.577198: step 2216, loss 0.202736, acc 0.91\n",
      "2018-01-08T04:51:30.725379: step 2217, loss 0.203811, acc 0.88\n",
      "2018-01-08T04:51:30.880670: step 2218, loss 0.219389, acc 0.9\n",
      "2018-01-08T04:51:31.020693: step 2219, loss 0.185881, acc 0.9\n",
      "2018-01-08T04:51:31.172700: step 2220, loss 0.214485, acc 0.9\n",
      "2018-01-08T04:51:31.316446: step 2221, loss 0.206565, acc 0.91\n",
      "2018-01-08T04:51:31.463803: step 2222, loss 0.238139, acc 0.89\n",
      "2018-01-08T04:51:31.610935: step 2223, loss 0.199089, acc 0.91\n",
      "2018-01-08T04:51:31.753756: step 2224, loss 0.164597, acc 0.95\n",
      "2018-01-08T04:51:31.898451: step 2225, loss 0.208676, acc 0.93\n",
      "2018-01-08T04:51:32.037373: step 2226, loss 0.192334, acc 0.92\n",
      "2018-01-08T04:51:32.190814: step 2227, loss 0.191742, acc 0.92\n",
      "2018-01-08T04:51:32.341436: step 2228, loss 0.20173, acc 0.88\n",
      "2018-01-08T04:51:32.489750: step 2229, loss 0.191509, acc 0.88\n",
      "2018-01-08T04:51:32.642845: step 2230, loss 0.152114, acc 0.92\n",
      "2018-01-08T04:51:32.791023: step 2231, loss 0.155689, acc 0.97\n",
      "2018-01-08T04:51:32.943604: step 2232, loss 0.206708, acc 0.89\n",
      "2018-01-08T04:51:33.094628: step 2233, loss 0.184329, acc 0.94\n",
      "2018-01-08T04:51:33.237116: step 2234, loss 0.174124, acc 0.9\n",
      "2018-01-08T04:51:33.383034: step 2235, loss 0.153493, acc 0.97\n",
      "2018-01-08T04:51:33.527793: step 2236, loss 0.182034, acc 0.95\n",
      "2018-01-08T04:51:33.687733: step 2237, loss 0.139639, acc 0.98\n",
      "2018-01-08T04:51:33.917914: step 2238, loss 0.180617, acc 0.93\n",
      "2018-01-08T04:51:34.138217: step 2239, loss 0.15104, acc 0.98\n",
      "2018-01-08T04:51:34.363179: step 2240, loss 0.162981, acc 0.97\n",
      "2018-01-08T04:51:34.582036: step 2241, loss 0.198473, acc 0.93\n",
      "2018-01-08T04:51:34.809891: step 2242, loss 0.170744, acc 0.94\n",
      "2018-01-08T04:51:35.032108: step 2243, loss 0.184278, acc 0.91\n",
      "2018-01-08T04:51:35.256441: step 2244, loss 0.164466, acc 0.95\n",
      "2018-01-08T04:51:35.492266: step 2245, loss 0.168831, acc 0.95\n",
      "2018-01-08T04:51:35.723301: step 2246, loss 0.162007, acc 0.94\n",
      "2018-01-08T04:51:35.963681: step 2247, loss 0.188704, acc 0.92\n",
      "2018-01-08T04:51:36.183587: step 2248, loss 0.207324, acc 0.89\n",
      "2018-01-08T04:51:36.403317: step 2249, loss 0.212463, acc 0.91\n",
      "2018-01-08T04:51:36.619509: step 2250, loss 0.187951, acc 0.91\n",
      "2018-01-08T04:51:36.848162: step 2251, loss 0.161434, acc 0.94\n",
      "2018-01-08T04:51:37.074163: step 2252, loss 0.227627, acc 0.93\n",
      "2018-01-08T04:51:37.293703: step 2253, loss 0.211335, acc 0.89\n",
      "2018-01-08T04:51:37.513542: step 2254, loss 0.17489, acc 0.91\n",
      "2018-01-08T04:51:37.704588: step 2255, loss 0.190978, acc 0.93\n",
      "2018-01-08T04:51:37.871295: step 2256, loss 0.196957, acc 0.91\n",
      "2018-01-08T04:51:38.031764: step 2257, loss 0.150025, acc 0.99\n",
      "2018-01-08T04:51:38.200298: step 2258, loss 0.146057, acc 0.96\n",
      "2018-01-08T04:51:38.361661: step 2259, loss 0.182869, acc 0.92\n",
      "2018-01-08T04:51:38.523386: step 2260, loss 0.16414, acc 0.93\n",
      "2018-01-08T04:51:38.685053: step 2261, loss 0.218463, acc 0.91\n",
      "2018-01-08T04:51:38.854312: step 2262, loss 0.162243, acc 0.95\n",
      "2018-01-08T04:51:39.011750: step 2263, loss 0.174372, acc 0.93\n",
      "2018-01-08T04:51:39.178802: step 2264, loss 0.19272, acc 0.95\n",
      "2018-01-08T04:51:39.338411: step 2265, loss 0.157442, acc 0.94\n",
      "2018-01-08T04:51:39.492396: step 2266, loss 0.16697, acc 0.91\n",
      "2018-01-08T04:51:39.650664: step 2267, loss 0.15419, acc 0.95\n",
      "2018-01-08T04:51:39.813016: step 2268, loss 0.194306, acc 0.93\n",
      "2018-01-08T04:51:39.976630: step 2269, loss 0.213387, acc 0.92\n",
      "2018-01-08T04:51:40.143879: step 2270, loss 0.192981, acc 0.93\n",
      "2018-01-08T04:51:40.298147: step 2271, loss 0.17386, acc 0.95\n",
      "2018-01-08T04:51:40.456039: step 2272, loss 0.153166, acc 0.95\n",
      "2018-01-08T04:51:40.612653: step 2273, loss 0.20751, acc 0.91\n",
      "2018-01-08T04:51:40.775070: step 2274, loss 0.167745, acc 0.93\n",
      "2018-01-08T04:51:40.937446: step 2275, loss 0.149757, acc 0.96\n",
      "2018-01-08T04:51:41.101179: step 2276, loss 0.152303, acc 0.96\n",
      "2018-01-08T04:51:41.257932: step 2277, loss 0.214842, acc 0.92\n",
      "2018-01-08T04:51:41.409858: step 2278, loss 0.192383, acc 0.89\n",
      "2018-01-08T04:51:41.566841: step 2279, loss 0.140342, acc 0.97\n",
      "2018-01-08T04:51:41.715643: step 2280, loss 0.202632, acc 0.91\n",
      "2018-01-08T04:51:41.864469: step 2281, loss 0.168123, acc 0.91\n",
      "2018-01-08T04:51:42.016290: step 2282, loss 0.19267, acc 0.94\n",
      "2018-01-08T04:51:42.164918: step 2283, loss 0.181575, acc 0.94\n",
      "2018-01-08T04:51:42.315038: step 2284, loss 0.148365, acc 0.94\n",
      "2018-01-08T04:51:42.461359: step 2285, loss 0.209459, acc 0.91\n",
      "2018-01-08T04:51:42.605575: step 2286, loss 0.232179, acc 0.9\n",
      "2018-01-08T04:51:42.751254: step 2287, loss 0.16007, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:51:42.898715: step 2288, loss 0.202025, acc 0.9\n",
      "2018-01-08T04:51:43.048218: step 2289, loss 0.163684, acc 0.95\n",
      "2018-01-08T04:51:43.196890: step 2290, loss 0.174068, acc 0.94\n",
      "2018-01-08T04:51:43.343883: step 2291, loss 0.172161, acc 0.93\n",
      "2018-01-08T04:51:43.482712: step 2292, loss 0.157624, acc 0.96\n",
      "2018-01-08T04:51:43.625272: step 2293, loss 0.175822, acc 0.94\n",
      "2018-01-08T04:51:43.770591: step 2294, loss 0.164832, acc 0.97\n",
      "2018-01-08T04:51:43.922201: step 2295, loss 0.162288, acc 0.96\n",
      "2018-01-08T04:51:44.073692: step 2296, loss 0.187303, acc 0.92\n",
      "2018-01-08T04:51:44.211872: step 2297, loss 0.195915, acc 0.9\n",
      "2018-01-08T04:51:44.358189: step 2298, loss 0.1379, acc 0.97\n",
      "2018-01-08T04:51:44.499833: step 2299, loss 0.226699, acc 0.89\n",
      "2018-01-08T04:51:44.649268: step 2300, loss 0.169185, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:51:44.927823: step 2300, loss 0.267234, acc 0.851303\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2300\n",
      "\n",
      "2018-01-08T04:51:45.079557: step 2301, loss 0.182055, acc 0.909091\n",
      "2018-01-08T04:51:45.234531: step 2302, loss 0.146078, acc 0.93\n",
      "2018-01-08T04:51:45.375649: step 2303, loss 0.158387, acc 0.97\n",
      "2018-01-08T04:51:45.521028: step 2304, loss 0.15774, acc 0.96\n",
      "2018-01-08T04:51:45.668389: step 2305, loss 0.211569, acc 0.92\n",
      "2018-01-08T04:51:45.834518: step 2306, loss 0.244997, acc 0.9\n",
      "2018-01-08T04:51:46.000672: step 2307, loss 0.1384, acc 0.99\n",
      "2018-01-08T04:51:46.163325: step 2308, loss 0.187892, acc 0.93\n",
      "2018-01-08T04:51:46.323488: step 2309, loss 0.203798, acc 0.88\n",
      "2018-01-08T04:51:46.477686: step 2310, loss 0.165806, acc 0.96\n",
      "2018-01-08T04:51:46.640484: step 2311, loss 0.173677, acc 0.96\n",
      "2018-01-08T04:51:46.799719: step 2312, loss 0.126789, acc 0.96\n",
      "2018-01-08T04:51:46.956982: step 2313, loss 0.170303, acc 0.99\n",
      "2018-01-08T04:51:47.115661: step 2314, loss 0.182491, acc 0.93\n",
      "2018-01-08T04:51:47.274763: step 2315, loss 0.153247, acc 0.95\n",
      "2018-01-08T04:51:47.427386: step 2316, loss 0.189923, acc 0.88\n",
      "2018-01-08T04:51:47.586069: step 2317, loss 0.191861, acc 0.94\n",
      "2018-01-08T04:51:47.746841: step 2318, loss 0.162849, acc 0.94\n",
      "2018-01-08T04:51:47.906984: step 2319, loss 0.118637, acc 0.97\n",
      "2018-01-08T04:51:48.073367: step 2320, loss 0.183013, acc 0.93\n",
      "2018-01-08T04:51:48.230758: step 2321, loss 0.145389, acc 0.98\n",
      "2018-01-08T04:51:48.395990: step 2322, loss 0.168084, acc 0.93\n",
      "2018-01-08T04:51:48.551319: step 2323, loss 0.18492, acc 0.91\n",
      "2018-01-08T04:51:48.711741: step 2324, loss 0.173842, acc 0.94\n",
      "2018-01-08T04:51:48.871120: step 2325, loss 0.169217, acc 0.95\n",
      "2018-01-08T04:51:49.029574: step 2326, loss 0.195183, acc 0.91\n",
      "2018-01-08T04:51:49.186821: step 2327, loss 0.179498, acc 0.93\n",
      "2018-01-08T04:51:49.345771: step 2328, loss 0.160772, acc 0.96\n",
      "2018-01-08T04:51:49.499649: step 2329, loss 0.17327, acc 0.96\n",
      "2018-01-08T04:51:49.665910: step 2330, loss 0.162008, acc 0.94\n",
      "2018-01-08T04:51:49.814042: step 2331, loss 0.144144, acc 0.96\n",
      "2018-01-08T04:51:49.959338: step 2332, loss 0.166326, acc 0.95\n",
      "2018-01-08T04:51:50.105894: step 2333, loss 0.171439, acc 0.95\n",
      "2018-01-08T04:51:50.251430: step 2334, loss 0.150513, acc 0.94\n",
      "2018-01-08T04:51:50.400927: step 2335, loss 0.180672, acc 0.95\n",
      "2018-01-08T04:51:50.540866: step 2336, loss 0.158092, acc 0.95\n",
      "2018-01-08T04:51:50.684629: step 2337, loss 0.15621, acc 0.95\n",
      "2018-01-08T04:51:50.836126: step 2338, loss 0.176593, acc 0.92\n",
      "2018-01-08T04:51:50.986494: step 2339, loss 0.154575, acc 0.95\n",
      "2018-01-08T04:51:51.138762: step 2340, loss 0.215111, acc 0.88\n",
      "2018-01-08T04:51:51.283058: step 2341, loss 0.188141, acc 0.89\n",
      "2018-01-08T04:51:51.429756: step 2342, loss 0.177613, acc 0.95\n",
      "2018-01-08T04:51:51.575806: step 2343, loss 0.175594, acc 0.93\n",
      "2018-01-08T04:51:51.726218: step 2344, loss 0.114194, acc 0.98\n",
      "2018-01-08T04:51:51.872532: step 2345, loss 0.183485, acc 0.92\n",
      "2018-01-08T04:51:52.022012: step 2346, loss 0.209785, acc 0.9\n",
      "2018-01-08T04:51:52.165545: step 2347, loss 0.161597, acc 0.95\n",
      "2018-01-08T04:51:52.314047: step 2348, loss 0.181324, acc 0.96\n",
      "2018-01-08T04:51:52.460903: step 2349, loss 0.182496, acc 0.95\n",
      "2018-01-08T04:51:52.605403: step 2350, loss 0.144895, acc 0.97\n",
      "2018-01-08T04:51:52.755875: step 2351, loss 0.181491, acc 0.9\n",
      "2018-01-08T04:51:52.907909: step 2352, loss 0.158607, acc 0.96\n",
      "2018-01-08T04:51:53.056546: step 2353, loss 0.183058, acc 0.9\n",
      "2018-01-08T04:51:53.199870: step 2354, loss 0.184294, acc 0.9\n",
      "2018-01-08T04:51:53.342843: step 2355, loss 0.209688, acc 0.9\n",
      "2018-01-08T04:51:53.488954: step 2356, loss 0.166772, acc 0.93\n",
      "2018-01-08T04:51:53.638515: step 2357, loss 0.149939, acc 0.98\n",
      "2018-01-08T04:51:53.777357: step 2358, loss 0.191937, acc 0.94\n",
      "2018-01-08T04:51:53.918312: step 2359, loss 0.178727, acc 0.93\n",
      "2018-01-08T04:51:54.061117: step 2360, loss 0.148751, acc 0.95\n",
      "2018-01-08T04:51:54.203276: step 2361, loss 0.197123, acc 0.9\n",
      "2018-01-08T04:51:54.344337: step 2362, loss 0.179428, acc 0.96\n",
      "2018-01-08T04:51:54.491267: step 2363, loss 0.158369, acc 0.96\n",
      "2018-01-08T04:51:54.627449: step 2364, loss 0.197767, acc 0.87\n",
      "2018-01-08T04:51:54.767377: step 2365, loss 0.166568, acc 0.97\n",
      "2018-01-08T04:51:54.915345: step 2366, loss 0.164269, acc 0.94\n",
      "2018-01-08T04:51:55.060118: step 2367, loss 0.165154, acc 0.95\n",
      "2018-01-08T04:51:55.201784: step 2368, loss 0.177343, acc 0.93\n",
      "2018-01-08T04:51:55.342841: step 2369, loss 0.159703, acc 0.95\n",
      "2018-01-08T04:51:55.475949: step 2370, loss 0.13907, acc 0.95\n",
      "2018-01-08T04:51:55.612182: step 2371, loss 0.183507, acc 0.9\n",
      "2018-01-08T04:51:55.750256: step 2372, loss 0.168946, acc 0.94\n",
      "2018-01-08T04:51:55.892746: step 2373, loss 0.121371, acc 0.99\n",
      "2018-01-08T04:51:56.029544: step 2374, loss 0.165212, acc 0.93\n",
      "2018-01-08T04:51:56.175837: step 2375, loss 0.211784, acc 0.9\n",
      "2018-01-08T04:51:56.321968: step 2376, loss 0.178984, acc 0.91\n",
      "2018-01-08T04:51:56.466248: step 2377, loss 0.195069, acc 0.91\n",
      "2018-01-08T04:51:56.607568: step 2378, loss 0.152493, acc 0.94\n",
      "2018-01-08T04:51:56.743955: step 2379, loss 0.152047, acc 0.95\n",
      "2018-01-08T04:51:56.888422: step 2380, loss 0.175881, acc 0.95\n",
      "2018-01-08T04:51:57.033956: step 2381, loss 0.158064, acc 0.9\n",
      "2018-01-08T04:51:57.179411: step 2382, loss 0.149428, acc 0.95\n",
      "2018-01-08T04:51:57.315703: step 2383, loss 0.178747, acc 0.94\n",
      "2018-01-08T04:51:57.448514: step 2384, loss 0.140021, acc 0.96\n",
      "2018-01-08T04:51:57.587866: step 2385, loss 0.165655, acc 0.94\n",
      "2018-01-08T04:51:57.734957: step 2386, loss 0.172021, acc 0.92\n",
      "2018-01-08T04:51:57.883865: step 2387, loss 0.1502, acc 0.92\n",
      "2018-01-08T04:51:58.031159: step 2388, loss 0.171654, acc 0.92\n",
      "2018-01-08T04:51:58.183631: step 2389, loss 0.180062, acc 0.93\n",
      "2018-01-08T04:51:58.326547: step 2390, loss 0.194805, acc 0.9\n",
      "2018-01-08T04:51:58.475028: step 2391, loss 0.166052, acc 0.93\n",
      "2018-01-08T04:51:58.620610: step 2392, loss 0.174894, acc 0.94\n",
      "2018-01-08T04:51:58.763421: step 2393, loss 0.15738, acc 0.94\n",
      "2018-01-08T04:51:58.917516: step 2394, loss 0.188419, acc 0.91\n",
      "2018-01-08T04:51:59.060837: step 2395, loss 0.15859, acc 0.94\n",
      "2018-01-08T04:51:59.209495: step 2396, loss 0.169099, acc 0.95\n",
      "2018-01-08T04:51:59.353019: step 2397, loss 0.228099, acc 0.83\n",
      "2018-01-08T04:51:59.493130: step 2398, loss 0.181001, acc 0.94\n",
      "2018-01-08T04:51:59.637938: step 2399, loss 0.150207, acc 0.94\n",
      "2018-01-08T04:51:59.788190: step 2400, loss 0.177744, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:52:00.069172: step 2400, loss 0.265606, acc 0.848748\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2400\n",
      "\n",
      "2018-01-08T04:52:00.280472: step 2401, loss 0.18379, acc 0.9\n",
      "2018-01-08T04:52:00.427356: step 2402, loss 0.171401, acc 0.95\n",
      "2018-01-08T04:52:00.579168: step 2403, loss 0.214839, acc 0.88\n",
      "2018-01-08T04:52:00.721321: step 2404, loss 0.139471, acc 0.97\n",
      "2018-01-08T04:52:00.869358: step 2405, loss 0.202019, acc 0.93\n",
      "2018-01-08T04:52:01.019312: step 2406, loss 0.189701, acc 0.93\n",
      "2018-01-08T04:52:01.164224: step 2407, loss 0.188213, acc 0.93\n",
      "2018-01-08T04:52:01.306389: step 2408, loss 0.168163, acc 0.93\n",
      "2018-01-08T04:52:01.445437: step 2409, loss 0.162812, acc 0.91\n",
      "2018-01-08T04:52:01.587158: step 2410, loss 0.183451, acc 0.92\n",
      "2018-01-08T04:52:01.777770: step 2411, loss 0.203371, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:52:02.005791: step 2412, loss 0.16787, acc 0.93\n",
      "2018-01-08T04:52:02.231284: step 2413, loss 0.189264, acc 0.91\n",
      "2018-01-08T04:52:02.461374: step 2414, loss 0.147619, acc 0.95\n",
      "2018-01-08T04:52:02.688312: step 2415, loss 0.152317, acc 0.94\n",
      "2018-01-08T04:52:02.917778: step 2416, loss 0.162131, acc 0.94\n",
      "2018-01-08T04:52:03.137353: step 2417, loss 0.135814, acc 0.97\n",
      "2018-01-08T04:52:03.358445: step 2418, loss 0.157201, acc 0.95\n",
      "2018-01-08T04:52:03.591000: step 2419, loss 0.181293, acc 0.9\n",
      "2018-01-08T04:52:03.816284: step 2420, loss 0.174328, acc 0.96\n",
      "2018-01-08T04:52:04.034280: step 2421, loss 0.193047, acc 0.91\n",
      "2018-01-08T04:52:04.262457: step 2422, loss 0.198706, acc 0.9\n",
      "2018-01-08T04:52:04.485402: step 2423, loss 0.16133, acc 0.93\n",
      "2018-01-08T04:52:04.717986: step 2424, loss 0.160427, acc 0.93\n",
      "2018-01-08T04:52:04.953810: step 2425, loss 0.184241, acc 0.95\n",
      "2018-01-08T04:52:05.186022: step 2426, loss 0.195244, acc 0.92\n",
      "2018-01-08T04:52:05.415369: step 2427, loss 0.151136, acc 0.95\n",
      "2018-01-08T04:52:05.632247: step 2428, loss 0.194778, acc 0.93\n",
      "2018-01-08T04:52:05.817527: step 2429, loss 0.127989, acc 0.97\n",
      "2018-01-08T04:52:05.978245: step 2430, loss 0.136377, acc 0.94\n",
      "2018-01-08T04:52:06.138955: step 2431, loss 0.198058, acc 0.89\n",
      "2018-01-08T04:52:06.319496: step 2432, loss 0.183304, acc 0.92\n",
      "2018-01-08T04:52:06.477482: step 2433, loss 0.164923, acc 0.95\n",
      "2018-01-08T04:52:06.643067: step 2434, loss 0.148281, acc 0.96\n",
      "2018-01-08T04:52:06.803303: step 2435, loss 0.130601, acc 0.96\n",
      "2018-01-08T04:52:06.971455: step 2436, loss 0.164752, acc 0.96\n",
      "2018-01-08T04:52:07.125713: step 2437, loss 0.16063, acc 0.95\n",
      "2018-01-08T04:52:07.283301: step 2438, loss 0.187752, acc 0.9\n",
      "2018-01-08T04:52:07.437994: step 2439, loss 0.175659, acc 0.92\n",
      "2018-01-08T04:52:07.590833: step 2440, loss 0.173822, acc 0.95\n",
      "2018-01-08T04:52:07.749346: step 2441, loss 0.179512, acc 0.94\n",
      "2018-01-08T04:52:07.913641: step 2442, loss 0.180334, acc 0.93\n",
      "2018-01-08T04:52:08.078896: step 2443, loss 0.188914, acc 0.9\n",
      "2018-01-08T04:52:08.239690: step 2444, loss 0.152365, acc 0.95\n",
      "2018-01-08T04:52:08.403778: step 2445, loss 0.189968, acc 0.9\n",
      "2018-01-08T04:52:08.570655: step 2446, loss 0.167327, acc 0.93\n",
      "2018-01-08T04:52:08.734237: step 2447, loss 0.159378, acc 0.96\n",
      "2018-01-08T04:52:08.903296: step 2448, loss 0.180398, acc 0.94\n",
      "2018-01-08T04:52:09.063391: step 2449, loss 0.163433, acc 0.92\n",
      "2018-01-08T04:52:09.219697: step 2450, loss 0.117435, acc 0.98\n",
      "2018-01-08T04:52:09.376312: step 2451, loss 0.201411, acc 0.93\n",
      "2018-01-08T04:52:09.536532: step 2452, loss 0.200108, acc 0.9\n",
      "2018-01-08T04:52:09.690959: step 2453, loss 0.186205, acc 0.9\n",
      "2018-01-08T04:52:09.844255: step 2454, loss 0.16402, acc 0.93\n",
      "2018-01-08T04:52:09.990830: step 2455, loss 0.207402, acc 0.9\n",
      "2018-01-08T04:52:10.134119: step 2456, loss 0.153149, acc 0.94\n",
      "2018-01-08T04:52:10.281218: step 2457, loss 0.169552, acc 0.91\n",
      "2018-01-08T04:52:10.427444: step 2458, loss 0.147807, acc 0.97\n",
      "2018-01-08T04:52:10.577050: step 2459, loss 0.163801, acc 0.96\n",
      "2018-01-08T04:52:10.723594: step 2460, loss 0.202054, acc 0.9\n",
      "2018-01-08T04:52:10.872467: step 2461, loss 0.125046, acc 0.98\n",
      "2018-01-08T04:52:11.017733: step 2462, loss 0.177643, acc 0.94\n",
      "2018-01-08T04:52:11.166865: step 2463, loss 0.153966, acc 0.95\n",
      "2018-01-08T04:52:11.313522: step 2464, loss 0.158008, acc 0.91\n",
      "2018-01-08T04:52:11.456095: step 2465, loss 0.161641, acc 0.94\n",
      "2018-01-08T04:52:11.596149: step 2466, loss 0.161109, acc 0.95\n",
      "2018-01-08T04:52:11.741065: step 2467, loss 0.140638, acc 0.93\n",
      "2018-01-08T04:52:11.885552: step 2468, loss 0.138832, acc 0.98\n",
      "2018-01-08T04:52:12.027487: step 2469, loss 0.152863, acc 0.96\n",
      "2018-01-08T04:52:12.166845: step 2470, loss 0.158562, acc 0.94\n",
      "2018-01-08T04:52:12.308539: step 2471, loss 0.218877, acc 0.86\n",
      "2018-01-08T04:52:12.453102: step 2472, loss 0.187444, acc 0.92\n",
      "2018-01-08T04:52:12.600421: step 2473, loss 0.167206, acc 0.91\n",
      "2018-01-08T04:52:12.752194: step 2474, loss 0.190825, acc 0.9\n",
      "2018-01-08T04:52:12.897739: step 2475, loss 0.15359, acc 0.94\n",
      "2018-01-08T04:52:13.044431: step 2476, loss 0.183964, acc 0.92\n",
      "2018-01-08T04:52:13.200603: step 2477, loss 0.173334, acc 0.93\n",
      "2018-01-08T04:52:13.288114: step 2478, loss 0.220432, acc 0.909091\n",
      "2018-01-08T04:52:13.438008: step 2479, loss 0.145103, acc 0.94\n",
      "2018-01-08T04:52:13.582852: step 2480, loss 0.186281, acc 0.9\n",
      "2018-01-08T04:52:13.730870: step 2481, loss 0.185966, acc 0.91\n",
      "2018-01-08T04:52:13.891949: step 2482, loss 0.137896, acc 0.95\n",
      "2018-01-08T04:52:14.054737: step 2483, loss 0.194755, acc 0.91\n",
      "2018-01-08T04:52:14.216599: step 2484, loss 0.165929, acc 0.94\n",
      "2018-01-08T04:52:14.377172: step 2485, loss 0.126051, acc 0.99\n",
      "2018-01-08T04:52:14.535270: step 2486, loss 0.126488, acc 0.96\n",
      "2018-01-08T04:52:14.697651: step 2487, loss 0.189103, acc 0.93\n",
      "2018-01-08T04:52:14.858903: step 2488, loss 0.162718, acc 0.97\n",
      "2018-01-08T04:52:15.022386: step 2489, loss 0.11791, acc 0.96\n",
      "2018-01-08T04:52:15.191159: step 2490, loss 0.166119, acc 0.92\n",
      "2018-01-08T04:52:15.345162: step 2491, loss 0.159288, acc 0.97\n",
      "2018-01-08T04:52:15.504165: step 2492, loss 0.162509, acc 0.94\n",
      "2018-01-08T04:52:15.659996: step 2493, loss 0.140964, acc 0.95\n",
      "2018-01-08T04:52:15.830160: step 2494, loss 0.14256, acc 0.96\n",
      "2018-01-08T04:52:15.987693: step 2495, loss 0.137835, acc 0.97\n",
      "2018-01-08T04:52:16.153086: step 2496, loss 0.162835, acc 0.93\n",
      "2018-01-08T04:52:16.308933: step 2497, loss 0.145005, acc 0.98\n",
      "2018-01-08T04:52:16.500317: step 2498, loss 0.171027, acc 0.94\n",
      "2018-01-08T04:52:16.660044: step 2499, loss 0.151313, acc 0.95\n",
      "2018-01-08T04:52:16.820863: step 2500, loss 0.16646, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:52:17.104590: step 2500, loss 0.258681, acc 0.853858\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2500\n",
      "\n",
      "2018-01-08T04:52:17.328433: step 2501, loss 0.165836, acc 0.91\n",
      "2018-01-08T04:52:17.491367: step 2502, loss 0.165756, acc 0.94\n",
      "2018-01-08T04:52:17.651765: step 2503, loss 0.161181, acc 0.94\n",
      "2018-01-08T04:52:17.807734: step 2504, loss 0.160396, acc 0.93\n",
      "2018-01-08T04:52:17.953102: step 2505, loss 0.137781, acc 0.93\n",
      "2018-01-08T04:52:18.094315: step 2506, loss 0.124886, acc 0.97\n",
      "2018-01-08T04:52:18.236968: step 2507, loss 0.162546, acc 0.93\n",
      "2018-01-08T04:52:18.382427: step 2508, loss 0.162221, acc 0.93\n",
      "2018-01-08T04:52:18.527023: step 2509, loss 0.209662, acc 0.86\n",
      "2018-01-08T04:52:18.671528: step 2510, loss 0.170255, acc 0.94\n",
      "2018-01-08T04:52:18.819700: step 2511, loss 0.1472, acc 0.93\n",
      "2018-01-08T04:52:18.971505: step 2512, loss 0.140981, acc 0.95\n",
      "2018-01-08T04:52:19.119249: step 2513, loss 0.137692, acc 0.95\n",
      "2018-01-08T04:52:19.257097: step 2514, loss 0.179269, acc 0.91\n",
      "2018-01-08T04:52:19.405878: step 2515, loss 0.166821, acc 0.94\n",
      "2018-01-08T04:52:19.546478: step 2516, loss 0.140858, acc 0.96\n",
      "2018-01-08T04:52:19.686893: step 2517, loss 0.152571, acc 0.95\n",
      "2018-01-08T04:52:19.835902: step 2518, loss 0.174245, acc 0.91\n",
      "2018-01-08T04:52:19.983727: step 2519, loss 0.169078, acc 0.91\n",
      "2018-01-08T04:52:20.132344: step 2520, loss 0.155839, acc 0.95\n",
      "2018-01-08T04:52:20.281219: step 2521, loss 0.136322, acc 0.99\n",
      "2018-01-08T04:52:20.426947: step 2522, loss 0.155784, acc 0.94\n",
      "2018-01-08T04:52:20.571298: step 2523, loss 0.1292, acc 0.97\n",
      "2018-01-08T04:52:20.714514: step 2524, loss 0.137915, acc 0.95\n",
      "2018-01-08T04:52:20.861897: step 2525, loss 0.215319, acc 0.89\n",
      "2018-01-08T04:52:21.001282: step 2526, loss 0.145563, acc 0.93\n",
      "2018-01-08T04:52:21.145606: step 2527, loss 0.16987, acc 0.93\n",
      "2018-01-08T04:52:21.289514: step 2528, loss 0.155905, acc 0.97\n",
      "2018-01-08T04:52:21.430044: step 2529, loss 0.136375, acc 0.97\n",
      "2018-01-08T04:52:21.572440: step 2530, loss 0.184509, acc 0.9\n",
      "2018-01-08T04:52:21.725229: step 2531, loss 0.159444, acc 0.93\n",
      "2018-01-08T04:52:21.887917: step 2532, loss 0.172665, acc 0.94\n",
      "2018-01-08T04:52:22.053734: step 2533, loss 0.144309, acc 0.94\n",
      "2018-01-08T04:52:22.210957: step 2534, loss 0.13486, acc 0.95\n",
      "2018-01-08T04:52:22.364129: step 2535, loss 0.164124, acc 0.94\n",
      "2018-01-08T04:52:22.527835: step 2536, loss 0.140962, acc 0.94\n",
      "2018-01-08T04:52:22.688348: step 2537, loss 0.18788, acc 0.93\n",
      "2018-01-08T04:52:22.848760: step 2538, loss 0.174284, acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:52:23.009801: step 2539, loss 0.138202, acc 0.96\n",
      "2018-01-08T04:52:23.172313: step 2540, loss 0.148515, acc 0.94\n",
      "2018-01-08T04:52:23.339790: step 2541, loss 0.16232, acc 0.92\n",
      "2018-01-08T04:52:23.500059: step 2542, loss 0.134994, acc 0.94\n",
      "2018-01-08T04:52:23.668850: step 2543, loss 0.138454, acc 0.95\n",
      "2018-01-08T04:52:23.829647: step 2544, loss 0.172715, acc 0.92\n",
      "2018-01-08T04:52:23.984428: step 2545, loss 0.152769, acc 0.95\n",
      "2018-01-08T04:52:24.148607: step 2546, loss 0.165343, acc 0.92\n",
      "2018-01-08T04:52:24.311161: step 2547, loss 0.177819, acc 0.94\n",
      "2018-01-08T04:52:24.465983: step 2548, loss 0.164939, acc 0.94\n",
      "2018-01-08T04:52:24.623860: step 2549, loss 0.136514, acc 0.99\n",
      "2018-01-08T04:52:24.785572: step 2550, loss 0.142064, acc 0.96\n",
      "2018-01-08T04:52:24.948926: step 2551, loss 0.191336, acc 0.91\n",
      "2018-01-08T04:52:25.115372: step 2552, loss 0.152641, acc 0.97\n",
      "2018-01-08T04:52:25.277562: step 2553, loss 0.129284, acc 0.95\n",
      "2018-01-08T04:52:25.432549: step 2554, loss 0.151963, acc 0.94\n",
      "2018-01-08T04:52:25.593261: step 2555, loss 0.172378, acc 0.93\n",
      "2018-01-08T04:52:25.747279: step 2556, loss 0.147697, acc 0.93\n",
      "2018-01-08T04:52:25.901475: step 2557, loss 0.159764, acc 0.93\n",
      "2018-01-08T04:52:26.043244: step 2558, loss 0.208007, acc 0.93\n",
      "2018-01-08T04:52:26.188893: step 2559, loss 0.139474, acc 0.96\n",
      "2018-01-08T04:52:26.335771: step 2560, loss 0.155584, acc 0.94\n",
      "2018-01-08T04:52:26.478018: step 2561, loss 0.157693, acc 0.94\n",
      "2018-01-08T04:52:26.654487: step 2562, loss 0.151692, acc 0.95\n",
      "2018-01-08T04:52:26.804101: step 2563, loss 0.201377, acc 0.9\n",
      "2018-01-08T04:52:26.945648: step 2564, loss 0.122488, acc 0.98\n",
      "2018-01-08T04:52:27.091257: step 2565, loss 0.152376, acc 0.92\n",
      "2018-01-08T04:52:27.240103: step 2566, loss 0.133493, acc 0.95\n",
      "2018-01-08T04:52:27.388766: step 2567, loss 0.210828, acc 0.9\n",
      "2018-01-08T04:52:27.526034: step 2568, loss 0.14256, acc 0.94\n",
      "2018-01-08T04:52:27.668227: step 2569, loss 0.128502, acc 0.95\n",
      "2018-01-08T04:52:27.819688: step 2570, loss 0.145448, acc 0.95\n",
      "2018-01-08T04:52:27.970160: step 2571, loss 0.200699, acc 0.9\n",
      "2018-01-08T04:52:28.120123: step 2572, loss 0.121742, acc 0.97\n",
      "2018-01-08T04:52:28.266855: step 2573, loss 0.170321, acc 0.94\n",
      "2018-01-08T04:52:28.412794: step 2574, loss 0.150823, acc 0.94\n",
      "2018-01-08T04:52:28.564924: step 2575, loss 0.13119, acc 0.94\n",
      "2018-01-08T04:52:28.707990: step 2576, loss 0.217976, acc 0.9\n",
      "2018-01-08T04:52:28.857259: step 2577, loss 0.137765, acc 0.94\n",
      "2018-01-08T04:52:29.003883: step 2578, loss 0.148453, acc 0.96\n",
      "2018-01-08T04:52:29.149453: step 2579, loss 0.163204, acc 0.95\n",
      "2018-01-08T04:52:29.297355: step 2580, loss 0.133432, acc 0.96\n",
      "2018-01-08T04:52:29.440386: step 2581, loss 0.180685, acc 0.93\n",
      "2018-01-08T04:52:29.583534: step 2582, loss 0.13111, acc 0.94\n",
      "2018-01-08T04:52:29.728702: step 2583, loss 0.16676, acc 0.96\n",
      "2018-01-08T04:52:29.869603: step 2584, loss 0.160335, acc 0.93\n",
      "2018-01-08T04:52:30.008230: step 2585, loss 0.140905, acc 0.95\n",
      "2018-01-08T04:52:30.155185: step 2586, loss 0.147665, acc 0.95\n",
      "2018-01-08T04:52:30.292987: step 2587, loss 0.167267, acc 0.92\n",
      "2018-01-08T04:52:30.436632: step 2588, loss 0.15499, acc 0.94\n",
      "2018-01-08T04:52:30.584195: step 2589, loss 0.153161, acc 0.94\n",
      "2018-01-08T04:52:30.718580: step 2590, loss 0.155494, acc 0.95\n",
      "2018-01-08T04:52:30.861190: step 2591, loss 0.124817, acc 0.98\n",
      "2018-01-08T04:52:30.997359: step 2592, loss 0.168387, acc 0.9\n",
      "2018-01-08T04:52:31.140635: step 2593, loss 0.15492, acc 0.97\n",
      "2018-01-08T04:52:31.280135: step 2594, loss 0.175752, acc 0.91\n",
      "2018-01-08T04:52:31.414215: step 2595, loss 0.148504, acc 0.93\n",
      "2018-01-08T04:52:31.551047: step 2596, loss 0.192938, acc 0.92\n",
      "2018-01-08T04:52:31.734075: step 2597, loss 0.156834, acc 0.9\n",
      "2018-01-08T04:52:31.869648: step 2598, loss 0.145522, acc 0.96\n",
      "2018-01-08T04:52:32.010808: step 2599, loss 0.133191, acc 0.95\n",
      "2018-01-08T04:52:32.147844: step 2600, loss 0.151799, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:52:32.413871: step 2600, loss 0.254176, acc 0.859479\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2600\n",
      "\n",
      "2018-01-08T04:52:32.618165: step 2601, loss 0.17748, acc 0.92\n",
      "2018-01-08T04:52:32.758542: step 2602, loss 0.14085, acc 0.94\n",
      "2018-01-08T04:52:32.898806: step 2603, loss 0.159007, acc 0.93\n",
      "2018-01-08T04:52:33.044128: step 2604, loss 0.137318, acc 0.94\n",
      "2018-01-08T04:52:33.181932: step 2605, loss 0.172745, acc 0.91\n",
      "2018-01-08T04:52:33.316274: step 2606, loss 0.151501, acc 0.97\n",
      "2018-01-08T04:52:33.452606: step 2607, loss 0.156255, acc 0.95\n",
      "2018-01-08T04:52:33.590305: step 2608, loss 0.148307, acc 0.94\n",
      "2018-01-08T04:52:33.732124: step 2609, loss 0.192733, acc 0.89\n",
      "2018-01-08T04:52:33.880642: step 2610, loss 0.172067, acc 0.94\n",
      "2018-01-08T04:52:34.026134: step 2611, loss 0.150729, acc 0.95\n",
      "2018-01-08T04:52:34.169644: step 2612, loss 0.123988, acc 0.97\n",
      "2018-01-08T04:52:34.308604: step 2613, loss 0.162628, acc 0.91\n",
      "2018-01-08T04:52:34.450369: step 2614, loss 0.171264, acc 0.91\n",
      "2018-01-08T04:52:34.595919: step 2615, loss 0.155299, acc 0.93\n",
      "2018-01-08T04:52:34.739259: step 2616, loss 0.140702, acc 0.94\n",
      "2018-01-08T04:52:34.891625: step 2617, loss 0.172969, acc 0.93\n",
      "2018-01-08T04:52:35.042176: step 2618, loss 0.172592, acc 0.93\n",
      "2018-01-08T04:52:35.189117: step 2619, loss 0.161822, acc 0.95\n",
      "2018-01-08T04:52:35.332740: step 2620, loss 0.194437, acc 0.92\n",
      "2018-01-08T04:52:35.480174: step 2621, loss 0.178511, acc 0.93\n",
      "2018-01-08T04:52:35.620966: step 2622, loss 0.173981, acc 0.92\n",
      "2018-01-08T04:52:35.766930: step 2623, loss 0.166268, acc 0.92\n",
      "2018-01-08T04:52:35.913783: step 2624, loss 0.165878, acc 0.91\n",
      "2018-01-08T04:52:36.064394: step 2625, loss 0.171103, acc 0.92\n",
      "2018-01-08T04:52:36.207771: step 2626, loss 0.146266, acc 0.95\n",
      "2018-01-08T04:52:36.354917: step 2627, loss 0.14291, acc 0.94\n",
      "2018-01-08T04:52:36.496628: step 2628, loss 0.107935, acc 0.97\n",
      "2018-01-08T04:52:36.639593: step 2629, loss 0.169725, acc 0.93\n",
      "2018-01-08T04:52:36.784375: step 2630, loss 0.173891, acc 0.92\n",
      "2018-01-08T04:52:36.929065: step 2631, loss 0.147321, acc 0.96\n",
      "2018-01-08T04:52:37.080765: step 2632, loss 0.174144, acc 0.94\n",
      "2018-01-08T04:52:37.231866: step 2633, loss 0.199929, acc 0.93\n",
      "2018-01-08T04:52:37.371926: step 2634, loss 0.15787, acc 0.95\n",
      "2018-01-08T04:52:37.511043: step 2635, loss 0.173353, acc 0.93\n",
      "2018-01-08T04:52:37.661342: step 2636, loss 0.158385, acc 0.95\n",
      "2018-01-08T04:52:37.811415: step 2637, loss 0.1722, acc 0.95\n",
      "2018-01-08T04:52:37.952481: step 2638, loss 0.14413, acc 0.92\n",
      "2018-01-08T04:52:38.092814: step 2639, loss 0.131921, acc 0.97\n",
      "2018-01-08T04:52:38.232389: step 2640, loss 0.151585, acc 0.94\n",
      "2018-01-08T04:52:38.384246: step 2641, loss 0.15341, acc 0.93\n",
      "2018-01-08T04:52:38.525040: step 2642, loss 0.175709, acc 0.92\n",
      "2018-01-08T04:52:38.674888: step 2643, loss 0.145337, acc 0.95\n",
      "2018-01-08T04:52:38.814536: step 2644, loss 0.145603, acc 0.95\n",
      "2018-01-08T04:52:38.954807: step 2645, loss 0.163043, acc 0.94\n",
      "2018-01-08T04:52:39.095419: step 2646, loss 0.16877, acc 0.94\n",
      "2018-01-08T04:52:39.240804: step 2647, loss 0.163818, acc 0.94\n",
      "2018-01-08T04:52:39.373742: step 2648, loss 0.227809, acc 0.9\n",
      "2018-01-08T04:52:39.512193: step 2649, loss 0.146091, acc 0.95\n",
      "2018-01-08T04:52:39.653002: step 2650, loss 0.123358, acc 0.92\n",
      "2018-01-08T04:52:39.792276: step 2651, loss 0.151593, acc 0.94\n",
      "2018-01-08T04:52:39.934986: step 2652, loss 0.131063, acc 0.96\n",
      "2018-01-08T04:52:40.081483: step 2653, loss 0.140463, acc 0.97\n",
      "2018-01-08T04:52:40.215722: step 2654, loss 0.113371, acc 0.97\n",
      "2018-01-08T04:52:40.302145: step 2655, loss 0.150668, acc 0.909091\n",
      "2018-01-08T04:52:40.444037: step 2656, loss 0.102881, acc 1\n",
      "2018-01-08T04:52:40.591058: step 2657, loss 0.132356, acc 0.96\n",
      "2018-01-08T04:52:40.735084: step 2658, loss 0.161631, acc 0.94\n",
      "2018-01-08T04:52:40.882043: step 2659, loss 0.18185, acc 0.93\n",
      "2018-01-08T04:52:41.023803: step 2660, loss 0.130448, acc 0.95\n",
      "2018-01-08T04:52:41.163150: step 2661, loss 0.163369, acc 0.93\n",
      "2018-01-08T04:52:41.299846: step 2662, loss 0.161318, acc 0.93\n",
      "2018-01-08T04:52:41.433065: step 2663, loss 0.157429, acc 0.97\n",
      "2018-01-08T04:52:41.570003: step 2664, loss 0.164228, acc 0.93\n",
      "2018-01-08T04:52:41.713254: step 2665, loss 0.143723, acc 0.97\n",
      "2018-01-08T04:52:41.859174: step 2666, loss 0.161273, acc 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:52:42.004056: step 2667, loss 0.142627, acc 0.96\n",
      "2018-01-08T04:52:42.152517: step 2668, loss 0.181925, acc 0.91\n",
      "2018-01-08T04:52:42.300772: step 2669, loss 0.142849, acc 0.94\n",
      "2018-01-08T04:52:42.444486: step 2670, loss 0.134793, acc 0.93\n",
      "2018-01-08T04:52:42.593354: step 2671, loss 0.147146, acc 0.95\n",
      "2018-01-08T04:52:42.731298: step 2672, loss 0.167074, acc 0.91\n",
      "2018-01-08T04:52:42.884413: step 2673, loss 0.143293, acc 0.95\n",
      "2018-01-08T04:52:43.035065: step 2674, loss 0.159931, acc 0.94\n",
      "2018-01-08T04:52:43.178943: step 2675, loss 0.163491, acc 0.92\n",
      "2018-01-08T04:52:43.325551: step 2676, loss 0.119552, acc 0.95\n",
      "2018-01-08T04:52:43.466480: step 2677, loss 0.13863, acc 0.97\n",
      "2018-01-08T04:52:43.612188: step 2678, loss 0.163897, acc 0.92\n",
      "2018-01-08T04:52:43.755022: step 2679, loss 0.148721, acc 0.93\n",
      "2018-01-08T04:52:43.902204: step 2680, loss 0.124575, acc 0.97\n",
      "2018-01-08T04:52:44.045251: step 2681, loss 0.161249, acc 0.97\n",
      "2018-01-08T04:52:44.191360: step 2682, loss 0.132951, acc 0.97\n",
      "2018-01-08T04:52:44.328647: step 2683, loss 0.13804, acc 0.95\n",
      "2018-01-08T04:52:44.472800: step 2684, loss 0.107827, acc 0.98\n",
      "2018-01-08T04:52:44.615226: step 2685, loss 0.132681, acc 0.98\n",
      "2018-01-08T04:52:44.761707: step 2686, loss 0.178874, acc 0.92\n",
      "2018-01-08T04:52:44.913880: step 2687, loss 0.134694, acc 0.96\n",
      "2018-01-08T04:52:45.056453: step 2688, loss 0.102299, acc 0.98\n",
      "2018-01-08T04:52:45.201377: step 2689, loss 0.120093, acc 0.97\n",
      "2018-01-08T04:52:45.340672: step 2690, loss 0.140736, acc 0.95\n",
      "2018-01-08T04:52:45.479887: step 2691, loss 0.161333, acc 0.95\n",
      "2018-01-08T04:52:45.623370: step 2692, loss 0.122312, acc 0.94\n",
      "2018-01-08T04:52:45.767398: step 2693, loss 0.123894, acc 0.98\n",
      "2018-01-08T04:52:45.903677: step 2694, loss 0.141576, acc 0.93\n",
      "2018-01-08T04:52:46.046809: step 2695, loss 0.122742, acc 0.93\n",
      "2018-01-08T04:52:46.186735: step 2696, loss 0.140668, acc 0.94\n",
      "2018-01-08T04:52:46.328347: step 2697, loss 0.14759, acc 0.96\n",
      "2018-01-08T04:52:46.469452: step 2698, loss 0.128607, acc 0.97\n",
      "2018-01-08T04:52:46.608874: step 2699, loss 0.164208, acc 0.9\n",
      "2018-01-08T04:52:46.747554: step 2700, loss 0.143774, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:52:47.019080: step 2700, loss 0.250596, acc 0.856413\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2700\n",
      "\n",
      "2018-01-08T04:52:47.222693: step 2701, loss 0.128112, acc 0.96\n",
      "2018-01-08T04:52:47.356956: step 2702, loss 0.161865, acc 0.92\n",
      "2018-01-08T04:52:47.494315: step 2703, loss 0.149609, acc 0.93\n",
      "2018-01-08T04:52:47.629477: step 2704, loss 0.155387, acc 0.92\n",
      "2018-01-08T04:52:47.770136: step 2705, loss 0.15828, acc 0.94\n",
      "2018-01-08T04:52:47.913428: step 2706, loss 0.164359, acc 0.91\n",
      "2018-01-08T04:52:48.050363: step 2707, loss 0.133422, acc 0.95\n",
      "2018-01-08T04:52:48.192355: step 2708, loss 0.141698, acc 0.96\n",
      "2018-01-08T04:52:48.329788: step 2709, loss 0.143892, acc 0.93\n",
      "2018-01-08T04:52:48.466603: step 2710, loss 0.116187, acc 0.97\n",
      "2018-01-08T04:52:48.606543: step 2711, loss 0.153635, acc 0.93\n",
      "2018-01-08T04:52:48.744407: step 2712, loss 0.137871, acc 0.97\n",
      "2018-01-08T04:52:48.891911: step 2713, loss 0.130869, acc 0.95\n",
      "2018-01-08T04:52:49.028465: step 2714, loss 0.124868, acc 0.96\n",
      "2018-01-08T04:52:49.171486: step 2715, loss 0.14201, acc 0.96\n",
      "2018-01-08T04:52:49.310404: step 2716, loss 0.138027, acc 0.93\n",
      "2018-01-08T04:52:49.444121: step 2717, loss 0.152027, acc 0.95\n",
      "2018-01-08T04:52:49.585589: step 2718, loss 0.142398, acc 0.93\n",
      "2018-01-08T04:52:49.725683: step 2719, loss 0.13375, acc 0.93\n",
      "2018-01-08T04:52:49.871728: step 2720, loss 0.143921, acc 0.98\n",
      "2018-01-08T04:52:50.022487: step 2721, loss 0.160022, acc 0.93\n",
      "2018-01-08T04:52:50.167313: step 2722, loss 0.122301, acc 0.94\n",
      "2018-01-08T04:52:50.309517: step 2723, loss 0.146404, acc 0.95\n",
      "2018-01-08T04:52:50.455861: step 2724, loss 0.223858, acc 0.88\n",
      "2018-01-08T04:52:50.598547: step 2725, loss 0.166773, acc 0.92\n",
      "2018-01-08T04:52:50.746748: step 2726, loss 0.13301, acc 0.96\n",
      "2018-01-08T04:52:50.895486: step 2727, loss 0.154245, acc 0.94\n",
      "2018-01-08T04:52:51.044983: step 2728, loss 0.126206, acc 0.96\n",
      "2018-01-08T04:52:51.191957: step 2729, loss 0.12565, acc 0.96\n",
      "2018-01-08T04:52:51.334478: step 2730, loss 0.178235, acc 0.91\n",
      "2018-01-08T04:52:51.484659: step 2731, loss 0.135616, acc 0.95\n",
      "2018-01-08T04:52:51.627866: step 2732, loss 0.156234, acc 0.92\n",
      "2018-01-08T04:52:51.766335: step 2733, loss 0.155485, acc 0.93\n",
      "2018-01-08T04:52:51.912846: step 2734, loss 0.133562, acc 0.95\n",
      "2018-01-08T04:52:52.058250: step 2735, loss 0.157118, acc 0.94\n",
      "2018-01-08T04:52:52.209484: step 2736, loss 0.151068, acc 0.94\n",
      "2018-01-08T04:52:52.351297: step 2737, loss 0.119016, acc 0.96\n",
      "2018-01-08T04:52:52.497198: step 2738, loss 0.116652, acc 0.94\n",
      "2018-01-08T04:52:52.645569: step 2739, loss 0.157984, acc 0.95\n",
      "2018-01-08T04:52:52.794895: step 2740, loss 0.160376, acc 0.92\n",
      "2018-01-08T04:52:52.940565: step 2741, loss 0.149086, acc 0.92\n",
      "2018-01-08T04:52:53.083063: step 2742, loss 0.171503, acc 0.92\n",
      "2018-01-08T04:52:53.232238: step 2743, loss 0.164695, acc 0.93\n",
      "2018-01-08T04:52:53.373639: step 2744, loss 0.154028, acc 0.94\n",
      "2018-01-08T04:52:53.513678: step 2745, loss 0.16492, acc 0.94\n",
      "2018-01-08T04:52:53.666791: step 2746, loss 0.127826, acc 0.94\n",
      "2018-01-08T04:52:53.873888: step 2747, loss 0.153542, acc 0.92\n",
      "2018-01-08T04:52:54.096680: step 2748, loss 0.151148, acc 0.94\n",
      "2018-01-08T04:52:54.334618: step 2749, loss 0.183936, acc 0.92\n",
      "2018-01-08T04:52:54.564976: step 2750, loss 0.149407, acc 0.93\n",
      "2018-01-08T04:52:54.784998: step 2751, loss 0.12846, acc 0.94\n",
      "2018-01-08T04:52:55.018852: step 2752, loss 0.108517, acc 0.98\n",
      "2018-01-08T04:52:55.260335: step 2753, loss 0.139186, acc 0.96\n",
      "2018-01-08T04:52:55.485398: step 2754, loss 0.158171, acc 0.92\n",
      "2018-01-08T04:52:55.710906: step 2755, loss 0.12525, acc 0.96\n",
      "2018-01-08T04:52:55.966815: step 2756, loss 0.117948, acc 0.98\n",
      "2018-01-08T04:52:56.208813: step 2757, loss 0.134313, acc 0.93\n",
      "2018-01-08T04:52:56.439743: step 2758, loss 0.141445, acc 0.96\n",
      "2018-01-08T04:52:56.662859: step 2759, loss 0.148301, acc 0.94\n",
      "2018-01-08T04:52:56.883875: step 2760, loss 0.140671, acc 0.95\n",
      "2018-01-08T04:52:57.106380: step 2761, loss 0.125998, acc 0.95\n",
      "2018-01-08T04:52:57.332647: step 2762, loss 0.132238, acc 0.97\n",
      "2018-01-08T04:52:57.549917: step 2763, loss 0.103996, acc 0.97\n",
      "2018-01-08T04:52:57.767305: step 2764, loss 0.146551, acc 0.92\n",
      "2018-01-08T04:52:57.928372: step 2765, loss 0.157375, acc 0.93\n",
      "2018-01-08T04:52:58.093580: step 2766, loss 0.154177, acc 0.93\n",
      "2018-01-08T04:52:58.246515: step 2767, loss 0.135449, acc 0.97\n",
      "2018-01-08T04:52:58.405557: step 2768, loss 0.132528, acc 0.95\n",
      "2018-01-08T04:52:58.570284: step 2769, loss 0.165268, acc 0.94\n",
      "2018-01-08T04:52:58.726381: step 2770, loss 0.155845, acc 0.93\n",
      "2018-01-08T04:52:58.895417: step 2771, loss 0.189033, acc 0.91\n",
      "2018-01-08T04:52:59.057720: step 2772, loss 0.169556, acc 0.93\n",
      "2018-01-08T04:52:59.213562: step 2773, loss 0.125961, acc 0.95\n",
      "2018-01-08T04:52:59.369999: step 2774, loss 0.127907, acc 0.96\n",
      "2018-01-08T04:52:59.531534: step 2775, loss 0.164716, acc 0.95\n",
      "2018-01-08T04:52:59.689647: step 2776, loss 0.126819, acc 0.98\n",
      "2018-01-08T04:52:59.844994: step 2777, loss 0.147173, acc 0.92\n",
      "2018-01-08T04:53:00.003981: step 2778, loss 0.123644, acc 0.95\n",
      "2018-01-08T04:53:00.163069: step 2779, loss 0.147589, acc 0.93\n",
      "2018-01-08T04:53:00.320107: step 2780, loss 0.144351, acc 0.93\n",
      "2018-01-08T04:53:00.478866: step 2781, loss 0.129498, acc 0.95\n",
      "2018-01-08T04:53:00.646954: step 2782, loss 0.15354, acc 0.95\n",
      "2018-01-08T04:53:00.808953: step 2783, loss 0.151546, acc 0.93\n",
      "2018-01-08T04:53:00.965217: step 2784, loss 0.14483, acc 0.95\n",
      "2018-01-08T04:53:01.130428: step 2785, loss 0.150511, acc 0.97\n",
      "2018-01-08T04:53:01.286815: step 2786, loss 0.153015, acc 0.96\n",
      "2018-01-08T04:53:01.440537: step 2787, loss 0.149266, acc 0.96\n",
      "2018-01-08T04:53:01.594692: step 2788, loss 0.152856, acc 0.94\n",
      "2018-01-08T04:53:01.753584: step 2789, loss 0.154982, acc 0.94\n",
      "2018-01-08T04:53:01.896680: step 2790, loss 0.137161, acc 0.95\n",
      "2018-01-08T04:53:02.041631: step 2791, loss 0.1008, acc 1\n",
      "2018-01-08T04:53:02.192736: step 2792, loss 0.163171, acc 0.89\n",
      "2018-01-08T04:53:02.340391: step 2793, loss 0.136188, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:53:02.481109: step 2794, loss 0.169843, acc 0.94\n",
      "2018-01-08T04:53:02.626632: step 2795, loss 0.120656, acc 0.94\n",
      "2018-01-08T04:53:02.772370: step 2796, loss 0.131662, acc 0.96\n",
      "2018-01-08T04:53:02.918076: step 2797, loss 0.131918, acc 0.96\n",
      "2018-01-08T04:53:03.066566: step 2798, loss 0.149031, acc 0.95\n",
      "2018-01-08T04:53:03.215052: step 2799, loss 0.149877, acc 0.93\n",
      "2018-01-08T04:53:03.361316: step 2800, loss 0.157895, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:53:03.634693: step 2800, loss 0.247213, acc 0.85488\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2800\n",
      "\n",
      "2018-01-08T04:53:03.846842: step 2801, loss 0.121353, acc 0.95\n",
      "2018-01-08T04:53:03.994115: step 2802, loss 0.139773, acc 0.94\n",
      "2018-01-08T04:53:04.143717: step 2803, loss 0.137402, acc 0.93\n",
      "2018-01-08T04:53:04.287276: step 2804, loss 0.14409, acc 0.96\n",
      "2018-01-08T04:53:04.433255: step 2805, loss 0.111611, acc 0.98\n",
      "2018-01-08T04:53:04.580928: step 2806, loss 0.128905, acc 0.96\n",
      "2018-01-08T04:53:04.732417: step 2807, loss 0.169821, acc 0.92\n",
      "2018-01-08T04:53:04.886164: step 2808, loss 0.108889, acc 0.98\n",
      "2018-01-08T04:53:05.036963: step 2809, loss 0.154445, acc 0.94\n",
      "2018-01-08T04:53:05.184221: step 2810, loss 0.129764, acc 0.94\n",
      "2018-01-08T04:53:05.330207: step 2811, loss 0.161053, acc 0.91\n",
      "2018-01-08T04:53:05.477641: step 2812, loss 0.154771, acc 0.94\n",
      "2018-01-08T04:53:05.618071: step 2813, loss 0.166533, acc 0.88\n",
      "2018-01-08T04:53:05.765505: step 2814, loss 0.14551, acc 0.94\n",
      "2018-01-08T04:53:05.922900: step 2815, loss 0.155904, acc 0.94\n",
      "2018-01-08T04:53:06.080799: step 2816, loss 0.135228, acc 0.95\n",
      "2018-01-08T04:53:06.244223: step 2817, loss 0.134805, acc 0.95\n",
      "2018-01-08T04:53:06.402899: step 2818, loss 0.106875, acc 1\n",
      "2018-01-08T04:53:06.565740: step 2819, loss 0.138778, acc 0.96\n",
      "2018-01-08T04:53:06.723154: step 2820, loss 0.134125, acc 0.95\n",
      "2018-01-08T04:53:06.893874: step 2821, loss 0.148298, acc 0.96\n",
      "2018-01-08T04:53:07.050927: step 2822, loss 0.178246, acc 0.91\n",
      "2018-01-08T04:53:07.214901: step 2823, loss 0.17202, acc 0.9\n",
      "2018-01-08T04:53:07.400824: step 2824, loss 0.143035, acc 0.95\n",
      "2018-01-08T04:53:07.560930: step 2825, loss 0.153369, acc 0.96\n",
      "2018-01-08T04:53:07.720968: step 2826, loss 0.173766, acc 0.93\n",
      "2018-01-08T04:53:07.882279: step 2827, loss 0.125959, acc 0.96\n",
      "2018-01-08T04:53:08.043573: step 2828, loss 0.114831, acc 0.97\n",
      "2018-01-08T04:53:08.198085: step 2829, loss 0.117951, acc 0.97\n",
      "2018-01-08T04:53:08.354276: step 2830, loss 0.136161, acc 0.95\n",
      "2018-01-08T04:53:08.517113: step 2831, loss 0.161207, acc 0.9\n",
      "2018-01-08T04:53:08.614854: step 2832, loss 0.147004, acc 0.909091\n",
      "2018-01-08T04:53:08.784893: step 2833, loss 0.150129, acc 0.94\n",
      "2018-01-08T04:53:08.944736: step 2834, loss 0.106347, acc 0.97\n",
      "2018-01-08T04:53:09.105581: step 2835, loss 0.160876, acc 0.94\n",
      "2018-01-08T04:53:09.261314: step 2836, loss 0.11831, acc 0.96\n",
      "2018-01-08T04:53:09.417743: step 2837, loss 0.139065, acc 0.96\n",
      "2018-01-08T04:53:09.577948: step 2838, loss 0.13589, acc 0.94\n",
      "2018-01-08T04:53:09.738986: step 2839, loss 0.152105, acc 0.92\n",
      "2018-01-08T04:53:09.886687: step 2840, loss 0.124932, acc 0.96\n",
      "2018-01-08T04:53:10.032670: step 2841, loss 0.121658, acc 0.97\n",
      "2018-01-08T04:53:10.177308: step 2842, loss 0.157703, acc 0.91\n",
      "2018-01-08T04:53:10.324279: step 2843, loss 0.113436, acc 0.95\n",
      "2018-01-08T04:53:10.472454: step 2844, loss 0.145379, acc 0.93\n",
      "2018-01-08T04:53:10.621985: step 2845, loss 0.179909, acc 0.9\n",
      "2018-01-08T04:53:10.769606: step 2846, loss 0.15, acc 0.97\n",
      "2018-01-08T04:53:10.919886: step 2847, loss 0.136354, acc 0.94\n",
      "2018-01-08T04:53:11.062676: step 2848, loss 0.171717, acc 0.9\n",
      "2018-01-08T04:53:11.208183: step 2849, loss 0.133822, acc 0.97\n",
      "2018-01-08T04:53:11.352200: step 2850, loss 0.128108, acc 0.94\n",
      "2018-01-08T04:53:11.492714: step 2851, loss 0.117628, acc 0.99\n",
      "2018-01-08T04:53:11.639221: step 2852, loss 0.140507, acc 0.94\n",
      "2018-01-08T04:53:11.789135: step 2853, loss 0.141823, acc 0.94\n",
      "2018-01-08T04:53:11.934796: step 2854, loss 0.0921874, acc 0.98\n",
      "2018-01-08T04:53:12.075368: step 2855, loss 0.130406, acc 0.94\n",
      "2018-01-08T04:53:12.219998: step 2856, loss 0.130955, acc 0.97\n",
      "2018-01-08T04:53:12.365704: step 2857, loss 0.137914, acc 0.96\n",
      "2018-01-08T04:53:12.516398: step 2858, loss 0.132867, acc 0.94\n",
      "2018-01-08T04:53:12.661403: step 2859, loss 0.129223, acc 0.95\n",
      "2018-01-08T04:53:12.806648: step 2860, loss 0.135345, acc 0.96\n",
      "2018-01-08T04:53:12.956537: step 2861, loss 0.109182, acc 0.97\n",
      "2018-01-08T04:53:13.107961: step 2862, loss 0.118228, acc 0.97\n",
      "2018-01-08T04:53:13.252310: step 2863, loss 0.163007, acc 0.9\n",
      "2018-01-08T04:53:13.392512: step 2864, loss 0.181438, acc 0.9\n",
      "2018-01-08T04:53:13.531262: step 2865, loss 0.134511, acc 0.96\n",
      "2018-01-08T04:53:13.679266: step 2866, loss 0.157098, acc 0.95\n",
      "2018-01-08T04:53:13.832917: step 2867, loss 0.151241, acc 0.93\n",
      "2018-01-08T04:53:13.986817: step 2868, loss 0.139897, acc 0.96\n",
      "2018-01-08T04:53:14.144290: step 2869, loss 0.0975439, acc 0.98\n",
      "2018-01-08T04:53:14.306595: step 2870, loss 0.154875, acc 0.93\n",
      "2018-01-08T04:53:14.467215: step 2871, loss 0.106308, acc 0.96\n",
      "2018-01-08T04:53:14.626625: step 2872, loss 0.127859, acc 0.95\n",
      "2018-01-08T04:53:14.790463: step 2873, loss 0.141887, acc 0.95\n",
      "2018-01-08T04:53:14.954572: step 2874, loss 0.137494, acc 0.94\n",
      "2018-01-08T04:53:15.116202: step 2875, loss 0.163741, acc 0.88\n",
      "2018-01-08T04:53:15.281004: step 2876, loss 0.145625, acc 0.92\n",
      "2018-01-08T04:53:15.434623: step 2877, loss 0.109774, acc 0.96\n",
      "2018-01-08T04:53:15.596244: step 2878, loss 0.105289, acc 0.98\n",
      "2018-01-08T04:53:15.759382: step 2879, loss 0.120868, acc 0.97\n",
      "2018-01-08T04:53:15.919679: step 2880, loss 0.131035, acc 0.98\n",
      "2018-01-08T04:53:16.080875: step 2881, loss 0.119115, acc 0.97\n",
      "2018-01-08T04:53:16.238577: step 2882, loss 0.141015, acc 0.97\n",
      "2018-01-08T04:53:16.401053: step 2883, loss 0.161991, acc 0.93\n",
      "2018-01-08T04:53:16.559634: step 2884, loss 0.0962547, acc 0.98\n",
      "2018-01-08T04:53:16.718108: step 2885, loss 0.150263, acc 0.92\n",
      "2018-01-08T04:53:16.874683: step 2886, loss 0.125471, acc 0.95\n",
      "2018-01-08T04:53:17.030245: step 2887, loss 0.120137, acc 0.98\n",
      "2018-01-08T04:53:17.183151: step 2888, loss 0.116666, acc 0.96\n",
      "2018-01-08T04:53:17.343342: step 2889, loss 0.124313, acc 0.98\n",
      "2018-01-08T04:53:17.503529: step 2890, loss 0.160521, acc 0.94\n",
      "2018-01-08T04:53:17.679166: step 2891, loss 0.10114, acc 0.96\n",
      "2018-01-08T04:53:17.837363: step 2892, loss 0.149048, acc 0.93\n",
      "2018-01-08T04:53:17.987099: step 2893, loss 0.12213, acc 0.98\n",
      "2018-01-08T04:53:18.137043: step 2894, loss 0.125414, acc 0.97\n",
      "2018-01-08T04:53:18.279586: step 2895, loss 0.0960568, acc 0.98\n",
      "2018-01-08T04:53:18.427368: step 2896, loss 0.139721, acc 0.95\n",
      "2018-01-08T04:53:18.578321: step 2897, loss 0.11523, acc 0.97\n",
      "2018-01-08T04:53:18.722569: step 2898, loss 0.145569, acc 0.95\n",
      "2018-01-08T04:53:18.876240: step 2899, loss 0.159454, acc 0.94\n",
      "2018-01-08T04:53:19.023594: step 2900, loss 0.151888, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:53:19.309192: step 2900, loss 0.252619, acc 0.851303\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-2900\n",
      "\n",
      "2018-01-08T04:53:19.515778: step 2901, loss 0.142935, acc 0.93\n",
      "2018-01-08T04:53:19.666913: step 2902, loss 0.12156, acc 0.96\n",
      "2018-01-08T04:53:19.812349: step 2903, loss 0.128074, acc 0.94\n",
      "2018-01-08T04:53:19.955697: step 2904, loss 0.149409, acc 0.94\n",
      "2018-01-08T04:53:20.099493: step 2905, loss 0.163986, acc 0.93\n",
      "2018-01-08T04:53:20.242088: step 2906, loss 0.13304, acc 0.94\n",
      "2018-01-08T04:53:20.386043: step 2907, loss 0.184102, acc 0.92\n",
      "2018-01-08T04:53:20.530466: step 2908, loss 0.14764, acc 0.93\n",
      "2018-01-08T04:53:20.678612: step 2909, loss 0.113306, acc 0.96\n",
      "2018-01-08T04:53:20.824477: step 2910, loss 0.124026, acc 0.97\n",
      "2018-01-08T04:53:20.975595: step 2911, loss 0.139748, acc 0.94\n",
      "2018-01-08T04:53:21.126045: step 2912, loss 0.143716, acc 0.93\n",
      "2018-01-08T04:53:21.276180: step 2913, loss 0.102278, acc 0.97\n",
      "2018-01-08T04:53:21.420616: step 2914, loss 0.119378, acc 0.96\n",
      "2018-01-08T04:53:21.569525: step 2915, loss 0.145807, acc 0.96\n",
      "2018-01-08T04:53:21.716053: step 2916, loss 0.127485, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:53:21.866779: step 2917, loss 0.124444, acc 0.96\n",
      "2018-01-08T04:53:22.003262: step 2918, loss 0.130993, acc 0.98\n",
      "2018-01-08T04:53:22.149639: step 2919, loss 0.0951692, acc 0.98\n",
      "2018-01-08T04:53:22.285991: step 2920, loss 0.133491, acc 0.96\n",
      "2018-01-08T04:53:22.426591: step 2921, loss 0.111922, acc 0.96\n",
      "2018-01-08T04:53:22.563981: step 2922, loss 0.159619, acc 0.94\n",
      "2018-01-08T04:53:22.701971: step 2923, loss 0.125077, acc 0.96\n",
      "2018-01-08T04:53:22.846146: step 2924, loss 0.14864, acc 0.95\n",
      "2018-01-08T04:53:22.992949: step 2925, loss 0.127244, acc 0.96\n",
      "2018-01-08T04:53:23.140153: step 2926, loss 0.133699, acc 0.96\n",
      "2018-01-08T04:53:23.281482: step 2927, loss 0.143719, acc 0.97\n",
      "2018-01-08T04:53:23.418013: step 2928, loss 0.131074, acc 0.92\n",
      "2018-01-08T04:53:23.554777: step 2929, loss 0.115394, acc 0.97\n",
      "2018-01-08T04:53:23.702403: step 2930, loss 0.144266, acc 0.94\n",
      "2018-01-08T04:53:23.846937: step 2931, loss 0.132989, acc 0.96\n",
      "2018-01-08T04:53:23.986272: step 2932, loss 0.136619, acc 0.96\n",
      "2018-01-08T04:53:24.125445: step 2933, loss 0.121696, acc 0.97\n",
      "2018-01-08T04:53:24.264498: step 2934, loss 0.131168, acc 0.95\n",
      "2018-01-08T04:53:24.406894: step 2935, loss 0.146136, acc 0.93\n",
      "2018-01-08T04:53:24.547912: step 2936, loss 0.128945, acc 0.97\n",
      "2018-01-08T04:53:24.685169: step 2937, loss 0.116411, acc 0.97\n",
      "2018-01-08T04:53:24.828006: step 2938, loss 0.141692, acc 0.95\n",
      "2018-01-08T04:53:24.973731: step 2939, loss 0.114431, acc 0.96\n",
      "2018-01-08T04:53:25.115648: step 2940, loss 0.126969, acc 0.97\n",
      "2018-01-08T04:53:25.253016: step 2941, loss 0.127424, acc 0.97\n",
      "2018-01-08T04:53:25.392946: step 2942, loss 0.125652, acc 0.93\n",
      "2018-01-08T04:53:25.530567: step 2943, loss 0.107658, acc 0.97\n",
      "2018-01-08T04:53:25.677978: step 2944, loss 0.0974507, acc 0.98\n",
      "2018-01-08T04:53:25.821752: step 2945, loss 0.15113, acc 0.95\n",
      "2018-01-08T04:53:25.964475: step 2946, loss 0.109354, acc 0.97\n",
      "2018-01-08T04:53:26.108074: step 2947, loss 0.129604, acc 0.95\n",
      "2018-01-08T04:53:26.253895: step 2948, loss 0.13964, acc 0.97\n",
      "2018-01-08T04:53:26.405767: step 2949, loss 0.132503, acc 0.97\n",
      "2018-01-08T04:53:26.556526: step 2950, loss 0.152626, acc 0.94\n",
      "2018-01-08T04:53:26.706167: step 2951, loss 0.0936797, acc 0.99\n",
      "2018-01-08T04:53:26.852479: step 2952, loss 0.0846797, acc 0.98\n",
      "2018-01-08T04:53:26.994751: step 2953, loss 0.146462, acc 0.96\n",
      "2018-01-08T04:53:27.139798: step 2954, loss 0.129379, acc 0.97\n",
      "2018-01-08T04:53:27.286114: step 2955, loss 0.114665, acc 0.94\n",
      "2018-01-08T04:53:27.430621: step 2956, loss 0.141294, acc 0.95\n",
      "2018-01-08T04:53:27.579891: step 2957, loss 0.141705, acc 0.92\n",
      "2018-01-08T04:53:27.728785: step 2958, loss 0.148878, acc 0.94\n",
      "2018-01-08T04:53:27.889791: step 2959, loss 0.165608, acc 0.9\n",
      "2018-01-08T04:53:28.041002: step 2960, loss 0.106775, acc 0.97\n",
      "2018-01-08T04:53:28.190764: step 2961, loss 0.131773, acc 0.94\n",
      "2018-01-08T04:53:28.340365: step 2962, loss 0.132049, acc 0.96\n",
      "2018-01-08T04:53:28.487271: step 2963, loss 0.114429, acc 0.96\n",
      "2018-01-08T04:53:28.633752: step 2964, loss 0.149779, acc 0.95\n",
      "2018-01-08T04:53:28.776170: step 2965, loss 0.125715, acc 0.96\n",
      "2018-01-08T04:53:28.922882: step 2966, loss 0.1531, acc 0.94\n",
      "2018-01-08T04:53:29.065724: step 2967, loss 0.145242, acc 0.92\n",
      "2018-01-08T04:53:29.210764: step 2968, loss 0.157244, acc 0.9\n",
      "2018-01-08T04:53:29.354925: step 2969, loss 0.177291, acc 0.9\n",
      "2018-01-08T04:53:29.494757: step 2970, loss 0.128721, acc 0.94\n",
      "2018-01-08T04:53:29.636723: step 2971, loss 0.13446, acc 0.99\n",
      "2018-01-08T04:53:29.785592: step 2972, loss 0.118407, acc 0.97\n",
      "2018-01-08T04:53:30.018882: step 2973, loss 0.127786, acc 0.98\n",
      "2018-01-08T04:53:30.244632: step 2974, loss 0.134784, acc 0.94\n",
      "2018-01-08T04:53:30.486444: step 2975, loss 0.155593, acc 0.92\n",
      "2018-01-08T04:53:30.706284: step 2976, loss 0.195936, acc 0.91\n",
      "2018-01-08T04:53:30.933983: step 2977, loss 0.137493, acc 0.94\n",
      "2018-01-08T04:53:31.164892: step 2978, loss 0.153552, acc 0.92\n",
      "2018-01-08T04:53:31.385568: step 2979, loss 0.109324, acc 0.97\n",
      "2018-01-08T04:53:31.616445: step 2980, loss 0.130281, acc 0.93\n",
      "2018-01-08T04:53:31.852011: step 2981, loss 0.129427, acc 0.93\n",
      "2018-01-08T04:53:32.092546: step 2982, loss 0.141672, acc 0.93\n",
      "2018-01-08T04:53:32.324938: step 2983, loss 0.136064, acc 0.93\n",
      "2018-01-08T04:53:32.551266: step 2984, loss 0.114247, acc 0.97\n",
      "2018-01-08T04:53:32.771786: step 2985, loss 0.144885, acc 0.94\n",
      "2018-01-08T04:53:33.026801: step 2986, loss 0.130573, acc 0.96\n",
      "2018-01-08T04:53:33.249113: step 2987, loss 0.144049, acc 0.93\n",
      "2018-01-08T04:53:33.465715: step 2988, loss 0.158087, acc 0.91\n",
      "2018-01-08T04:53:33.688715: step 2989, loss 0.134101, acc 0.97\n",
      "2018-01-08T04:53:33.875724: step 2990, loss 0.127153, acc 0.96\n",
      "2018-01-08T04:53:34.038664: step 2991, loss 0.117296, acc 0.96\n",
      "2018-01-08T04:53:34.198885: step 2992, loss 0.122503, acc 0.95\n",
      "2018-01-08T04:53:34.361212: step 2993, loss 0.168359, acc 0.95\n",
      "2018-01-08T04:53:34.525365: step 2994, loss 0.151135, acc 0.92\n",
      "2018-01-08T04:53:34.693335: step 2995, loss 0.132276, acc 0.98\n",
      "2018-01-08T04:53:34.855838: step 2996, loss 0.119905, acc 0.96\n",
      "2018-01-08T04:53:35.020573: step 2997, loss 0.0991707, acc 0.98\n",
      "2018-01-08T04:53:35.177480: step 2998, loss 0.116625, acc 0.98\n",
      "2018-01-08T04:53:35.333619: step 2999, loss 0.140038, acc 0.94\n",
      "2018-01-08T04:53:35.487553: step 3000, loss 0.159564, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:53:35.798830: step 3000, loss 0.248599, acc 0.862034\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3000\n",
      "\n",
      "2018-01-08T04:53:36.030286: step 3001, loss 0.092559, acc 0.95\n",
      "2018-01-08T04:53:36.186456: step 3002, loss 0.118785, acc 0.96\n",
      "2018-01-08T04:53:36.341970: step 3003, loss 0.111853, acc 0.98\n",
      "2018-01-08T04:53:36.503577: step 3004, loss 0.12285, acc 0.97\n",
      "2018-01-08T04:53:36.662479: step 3005, loss 0.105252, acc 0.98\n",
      "2018-01-08T04:53:36.827370: step 3006, loss 0.129456, acc 0.97\n",
      "2018-01-08T04:53:36.986874: step 3007, loss 0.124394, acc 0.94\n",
      "2018-01-08T04:53:37.144264: step 3008, loss 0.126045, acc 0.94\n",
      "2018-01-08T04:53:37.241901: step 3009, loss 0.151682, acc 0.954545\n",
      "2018-01-08T04:53:37.398929: step 3010, loss 0.134105, acc 0.94\n",
      "2018-01-08T04:53:37.561940: step 3011, loss 0.14091, acc 0.96\n",
      "2018-01-08T04:53:37.719685: step 3012, loss 0.140441, acc 0.92\n",
      "2018-01-08T04:53:37.865500: step 3013, loss 0.128462, acc 0.93\n",
      "2018-01-08T04:53:38.009796: step 3014, loss 0.108785, acc 0.98\n",
      "2018-01-08T04:53:38.158816: step 3015, loss 0.110281, acc 0.97\n",
      "2018-01-08T04:53:38.306115: step 3016, loss 0.0909897, acc 0.97\n",
      "2018-01-08T04:53:38.461343: step 3017, loss 0.137998, acc 0.96\n",
      "2018-01-08T04:53:38.607229: step 3018, loss 0.126402, acc 0.96\n",
      "2018-01-08T04:53:38.758675: step 3019, loss 0.114923, acc 0.97\n",
      "2018-01-08T04:53:38.913074: step 3020, loss 0.119332, acc 0.99\n",
      "2018-01-08T04:53:39.058003: step 3021, loss 0.143559, acc 0.93\n",
      "2018-01-08T04:53:39.206811: step 3022, loss 0.134313, acc 0.96\n",
      "2018-01-08T04:53:39.345654: step 3023, loss 0.0954148, acc 0.99\n",
      "2018-01-08T04:53:39.490448: step 3024, loss 0.102976, acc 0.97\n",
      "2018-01-08T04:53:39.634092: step 3025, loss 0.134045, acc 0.95\n",
      "2018-01-08T04:53:39.783264: step 3026, loss 0.102905, acc 0.97\n",
      "2018-01-08T04:53:39.943687: step 3027, loss 0.111372, acc 0.97\n",
      "2018-01-08T04:53:40.104871: step 3028, loss 0.115559, acc 0.94\n",
      "2018-01-08T04:53:40.259017: step 3029, loss 0.100224, acc 0.97\n",
      "2018-01-08T04:53:40.406638: step 3030, loss 0.136947, acc 0.97\n",
      "2018-01-08T04:53:40.568766: step 3031, loss 0.0863324, acc 0.98\n",
      "2018-01-08T04:53:40.721167: step 3032, loss 0.0948793, acc 0.99\n",
      "2018-01-08T04:53:40.874472: step 3033, loss 0.125977, acc 0.94\n",
      "2018-01-08T04:53:41.029668: step 3034, loss 0.0985079, acc 0.97\n",
      "2018-01-08T04:53:41.182815: step 3035, loss 0.101187, acc 0.96\n",
      "2018-01-08T04:53:41.335868: step 3036, loss 0.112491, acc 0.96\n",
      "2018-01-08T04:53:41.499647: step 3037, loss 0.137253, acc 0.92\n",
      "2018-01-08T04:53:41.646877: step 3038, loss 0.119065, acc 0.95\n",
      "2018-01-08T04:53:41.813536: step 3039, loss 0.113544, acc 0.97\n",
      "2018-01-08T04:53:41.992515: step 3040, loss 0.123238, acc 0.95\n",
      "2018-01-08T04:53:42.166507: step 3041, loss 0.110553, acc 0.96\n",
      "2018-01-08T04:53:42.331787: step 3042, loss 0.134747, acc 0.93\n",
      "2018-01-08T04:53:42.517700: step 3043, loss 0.133369, acc 0.95\n",
      "2018-01-08T04:53:42.695304: step 3044, loss 0.0895343, acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:53:42.887911: step 3045, loss 0.114293, acc 0.94\n",
      "2018-01-08T04:53:43.056594: step 3046, loss 0.116712, acc 0.96\n",
      "2018-01-08T04:53:43.234730: step 3047, loss 0.115272, acc 0.97\n",
      "2018-01-08T04:53:43.408079: step 3048, loss 0.105711, acc 0.98\n",
      "2018-01-08T04:53:43.578393: step 3049, loss 0.122217, acc 0.96\n",
      "2018-01-08T04:53:43.754251: step 3050, loss 0.119276, acc 0.95\n",
      "2018-01-08T04:53:43.920685: step 3051, loss 0.114749, acc 0.95\n",
      "2018-01-08T04:53:44.093458: step 3052, loss 0.11961, acc 0.96\n",
      "2018-01-08T04:53:44.289014: step 3053, loss 0.0868365, acc 1\n",
      "2018-01-08T04:53:44.456847: step 3054, loss 0.1547, acc 0.94\n",
      "2018-01-08T04:53:44.657767: step 3055, loss 0.0857293, acc 0.99\n",
      "2018-01-08T04:53:44.830042: step 3056, loss 0.155266, acc 0.95\n",
      "2018-01-08T04:53:45.002534: step 3057, loss 0.116933, acc 0.95\n",
      "2018-01-08T04:53:45.173427: step 3058, loss 0.117706, acc 0.97\n",
      "2018-01-08T04:53:45.345084: step 3059, loss 0.136007, acc 0.94\n",
      "2018-01-08T04:53:45.519832: step 3060, loss 0.103581, acc 0.96\n",
      "2018-01-08T04:53:45.688248: step 3061, loss 0.0855114, acc 1\n",
      "2018-01-08T04:53:45.854634: step 3062, loss 0.131973, acc 0.93\n",
      "2018-01-08T04:53:46.006713: step 3063, loss 0.118494, acc 0.97\n",
      "2018-01-08T04:53:46.157475: step 3064, loss 0.106361, acc 0.97\n",
      "2018-01-08T04:53:46.304754: step 3065, loss 0.123713, acc 0.94\n",
      "2018-01-08T04:53:46.462521: step 3066, loss 0.116991, acc 0.96\n",
      "2018-01-08T04:53:46.623133: step 3067, loss 0.100929, acc 0.97\n",
      "2018-01-08T04:53:46.778421: step 3068, loss 0.111774, acc 0.95\n",
      "2018-01-08T04:53:46.943636: step 3069, loss 0.11504, acc 0.96\n",
      "2018-01-08T04:53:47.092574: step 3070, loss 0.133083, acc 0.96\n",
      "2018-01-08T04:53:47.252414: step 3071, loss 0.0901034, acc 0.98\n",
      "2018-01-08T04:53:47.403996: step 3072, loss 0.12255, acc 0.96\n",
      "2018-01-08T04:53:47.550637: step 3073, loss 0.106806, acc 0.94\n",
      "2018-01-08T04:53:47.697379: step 3074, loss 0.196678, acc 0.89\n",
      "2018-01-08T04:53:47.844494: step 3075, loss 0.132428, acc 0.95\n",
      "2018-01-08T04:53:47.986113: step 3076, loss 0.114164, acc 0.94\n",
      "2018-01-08T04:53:48.139461: step 3077, loss 0.144077, acc 0.95\n",
      "2018-01-08T04:53:48.295027: step 3078, loss 0.141294, acc 0.96\n",
      "2018-01-08T04:53:48.443438: step 3079, loss 0.102977, acc 0.96\n",
      "2018-01-08T04:53:48.588543: step 3080, loss 0.128691, acc 0.95\n",
      "2018-01-08T04:53:48.735947: step 3081, loss 0.131085, acc 0.95\n",
      "2018-01-08T04:53:48.887801: step 3082, loss 0.116165, acc 0.97\n",
      "2018-01-08T04:53:49.034435: step 3083, loss 0.123902, acc 0.94\n",
      "2018-01-08T04:53:49.183702: step 3084, loss 0.107919, acc 0.95\n",
      "2018-01-08T04:53:49.327320: step 3085, loss 0.113026, acc 0.97\n",
      "2018-01-08T04:53:49.483790: step 3086, loss 0.127314, acc 0.97\n",
      "2018-01-08T04:53:49.642549: step 3087, loss 0.146903, acc 0.92\n",
      "2018-01-08T04:53:49.791658: step 3088, loss 0.109353, acc 0.98\n",
      "2018-01-08T04:53:49.940220: step 3089, loss 0.130189, acc 0.94\n",
      "2018-01-08T04:53:50.084444: step 3090, loss 0.149654, acc 0.92\n",
      "2018-01-08T04:53:50.237594: step 3091, loss 0.112299, acc 0.97\n",
      "2018-01-08T04:53:50.376575: step 3092, loss 0.134094, acc 0.93\n",
      "2018-01-08T04:53:50.520468: step 3093, loss 0.0876276, acc 0.97\n",
      "2018-01-08T04:53:50.661853: step 3094, loss 0.121768, acc 0.96\n",
      "2018-01-08T04:53:50.818409: step 3095, loss 0.110514, acc 0.95\n",
      "2018-01-08T04:53:50.961552: step 3096, loss 0.105103, acc 0.98\n",
      "2018-01-08T04:53:51.112311: step 3097, loss 0.137544, acc 0.93\n",
      "2018-01-08T04:53:51.255065: step 3098, loss 0.116232, acc 0.96\n",
      "2018-01-08T04:53:51.399421: step 3099, loss 0.145075, acc 0.93\n",
      "2018-01-08T04:53:51.558452: step 3100, loss 0.10932, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:53:51.863404: step 3100, loss 0.244024, acc 0.863567\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3100\n",
      "\n",
      "2018-01-08T04:53:52.099716: step 3101, loss 0.123069, acc 0.95\n",
      "2018-01-08T04:53:52.253509: step 3102, loss 0.121403, acc 0.97\n",
      "2018-01-08T04:53:52.410529: step 3103, loss 0.12783, acc 0.95\n",
      "2018-01-08T04:53:52.562939: step 3104, loss 0.125033, acc 0.96\n",
      "2018-01-08T04:53:52.723607: step 3105, loss 0.140633, acc 0.95\n",
      "2018-01-08T04:53:52.911302: step 3106, loss 0.130479, acc 0.95\n",
      "2018-01-08T04:53:53.064568: step 3107, loss 0.149856, acc 0.95\n",
      "2018-01-08T04:53:53.218073: step 3108, loss 0.112824, acc 0.98\n",
      "2018-01-08T04:53:53.378538: step 3109, loss 0.12655, acc 0.94\n",
      "2018-01-08T04:53:53.549203: step 3110, loss 0.132069, acc 0.97\n",
      "2018-01-08T04:53:53.690394: step 3111, loss 0.13549, acc 0.93\n",
      "2018-01-08T04:53:53.852121: step 3112, loss 0.126519, acc 0.96\n",
      "2018-01-08T04:53:54.005101: step 3113, loss 0.106351, acc 0.96\n",
      "2018-01-08T04:53:54.157587: step 3114, loss 0.0924742, acc 0.97\n",
      "2018-01-08T04:53:54.311846: step 3115, loss 0.116632, acc 0.97\n",
      "2018-01-08T04:53:54.453281: step 3116, loss 0.103057, acc 0.98\n",
      "2018-01-08T04:53:54.600239: step 3117, loss 0.13445, acc 0.93\n",
      "2018-01-08T04:53:54.745216: step 3118, loss 0.108503, acc 0.96\n",
      "2018-01-08T04:53:54.906781: step 3119, loss 0.117717, acc 0.98\n",
      "2018-01-08T04:53:55.057964: step 3120, loss 0.126097, acc 0.94\n",
      "2018-01-08T04:53:55.220703: step 3121, loss 0.137506, acc 0.9\n",
      "2018-01-08T04:53:55.372254: step 3122, loss 0.172348, acc 0.89\n",
      "2018-01-08T04:53:55.518410: step 3123, loss 0.120948, acc 0.95\n",
      "2018-01-08T04:53:55.670917: step 3124, loss 0.119178, acc 0.96\n",
      "2018-01-08T04:53:55.825537: step 3125, loss 0.12167, acc 0.96\n",
      "2018-01-08T04:53:55.980526: step 3126, loss 0.11611, acc 0.97\n",
      "2018-01-08T04:53:56.141170: step 3127, loss 0.131706, acc 0.93\n",
      "2018-01-08T04:53:56.297750: step 3128, loss 0.127909, acc 0.95\n",
      "2018-01-08T04:53:56.442979: step 3129, loss 0.121261, acc 0.94\n",
      "2018-01-08T04:53:56.587642: step 3130, loss 0.0984076, acc 0.97\n",
      "2018-01-08T04:53:56.733021: step 3131, loss 0.118892, acc 0.95\n",
      "2018-01-08T04:53:56.885074: step 3132, loss 0.128357, acc 0.94\n",
      "2018-01-08T04:53:57.035198: step 3133, loss 0.119835, acc 0.96\n",
      "2018-01-08T04:53:57.182148: step 3134, loss 0.125118, acc 0.96\n",
      "2018-01-08T04:53:57.325799: step 3135, loss 0.110263, acc 0.96\n",
      "2018-01-08T04:53:57.477264: step 3136, loss 0.15727, acc 0.9\n",
      "2018-01-08T04:53:57.640687: step 3137, loss 0.15123, acc 0.93\n",
      "2018-01-08T04:53:57.843096: step 3138, loss 0.0993122, acc 0.97\n",
      "2018-01-08T04:53:58.084470: step 3139, loss 0.144008, acc 0.94\n",
      "2018-01-08T04:53:58.331236: step 3140, loss 0.0904644, acc 0.98\n",
      "2018-01-08T04:53:58.563005: step 3141, loss 0.12295, acc 0.96\n",
      "2018-01-08T04:53:58.785911: step 3142, loss 0.122377, acc 0.97\n",
      "2018-01-08T04:53:59.009965: step 3143, loss 0.1285, acc 0.96\n",
      "2018-01-08T04:53:59.240736: step 3144, loss 0.142985, acc 0.95\n",
      "2018-01-08T04:53:59.460333: step 3145, loss 0.117482, acc 0.96\n",
      "2018-01-08T04:53:59.680727: step 3146, loss 0.13817, acc 0.94\n",
      "2018-01-08T04:53:59.916081: step 3147, loss 0.104797, acc 0.96\n",
      "2018-01-08T04:54:00.140398: step 3148, loss 0.13621, acc 0.94\n",
      "2018-01-08T04:54:00.379513: step 3149, loss 0.142809, acc 0.95\n",
      "2018-01-08T04:54:00.604764: step 3150, loss 0.11916, acc 0.97\n",
      "2018-01-08T04:54:00.846388: step 3151, loss 0.157217, acc 0.91\n",
      "2018-01-08T04:54:01.066988: step 3152, loss 0.126938, acc 0.94\n",
      "2018-01-08T04:54:01.303690: step 3153, loss 0.109294, acc 0.94\n",
      "2018-01-08T04:54:01.522060: step 3154, loss 0.121834, acc 0.94\n",
      "2018-01-08T04:54:01.748413: step 3155, loss 0.118746, acc 0.95\n",
      "2018-01-08T04:54:01.917563: step 3156, loss 0.0899803, acc 1\n",
      "2018-01-08T04:54:02.094059: step 3157, loss 0.124349, acc 0.99\n",
      "2018-01-08T04:54:02.249005: step 3158, loss 0.130371, acc 0.94\n",
      "2018-01-08T04:54:02.405249: step 3159, loss 0.129205, acc 0.93\n",
      "2018-01-08T04:54:02.563918: step 3160, loss 0.114332, acc 0.97\n",
      "2018-01-08T04:54:02.729679: step 3161, loss 0.0943081, acc 0.98\n",
      "2018-01-08T04:54:02.896689: step 3162, loss 0.137604, acc 0.94\n",
      "2018-01-08T04:54:03.069426: step 3163, loss 0.10673, acc 0.97\n",
      "2018-01-08T04:54:03.225419: step 3164, loss 0.107571, acc 0.95\n",
      "2018-01-08T04:54:03.383831: step 3165, loss 0.0925735, acc 0.98\n",
      "2018-01-08T04:54:03.539864: step 3166, loss 0.132658, acc 0.95\n",
      "2018-01-08T04:54:03.699623: step 3167, loss 0.148481, acc 0.94\n",
      "2018-01-08T04:54:03.857715: step 3168, loss 0.108761, acc 0.97\n",
      "2018-01-08T04:54:04.024555: step 3169, loss 0.107277, acc 0.99\n",
      "2018-01-08T04:54:04.185441: step 3170, loss 0.110581, acc 0.97\n",
      "2018-01-08T04:54:04.349636: step 3171, loss 0.141693, acc 0.94\n",
      "2018-01-08T04:54:04.505210: step 3172, loss 0.1247, acc 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:54:04.664707: step 3173, loss 0.121328, acc 0.95\n",
      "2018-01-08T04:54:04.833590: step 3174, loss 0.0949892, acc 0.99\n",
      "2018-01-08T04:54:04.996691: step 3175, loss 0.124568, acc 0.95\n",
      "2018-01-08T04:54:05.162274: step 3176, loss 0.120018, acc 0.95\n",
      "2018-01-08T04:54:05.320362: step 3177, loss 0.19264, acc 0.91\n",
      "2018-01-08T04:54:05.473140: step 3178, loss 0.119965, acc 0.98\n",
      "2018-01-08T04:54:05.636815: step 3179, loss 0.116155, acc 0.95\n",
      "2018-01-08T04:54:05.798063: step 3180, loss 0.124529, acc 0.95\n",
      "2018-01-08T04:54:05.947136: step 3181, loss 0.138081, acc 0.95\n",
      "2018-01-08T04:54:06.095530: step 3182, loss 0.128935, acc 0.96\n",
      "2018-01-08T04:54:06.238302: step 3183, loss 0.0719659, acc 0.99\n",
      "2018-01-08T04:54:06.384636: step 3184, loss 0.177835, acc 0.9\n",
      "2018-01-08T04:54:06.535113: step 3185, loss 0.0861314, acc 0.99\n",
      "2018-01-08T04:54:06.624150: step 3186, loss 0.166008, acc 0.909091\n",
      "2018-01-08T04:54:06.772463: step 3187, loss 0.118948, acc 0.96\n",
      "2018-01-08T04:54:06.913283: step 3188, loss 0.0999922, acc 0.96\n",
      "2018-01-08T04:54:07.061615: step 3189, loss 0.106913, acc 0.97\n",
      "2018-01-08T04:54:07.206303: step 3190, loss 0.127179, acc 0.93\n",
      "2018-01-08T04:54:07.347772: step 3191, loss 0.114537, acc 0.97\n",
      "2018-01-08T04:54:07.487420: step 3192, loss 0.11155, acc 0.95\n",
      "2018-01-08T04:54:07.634250: step 3193, loss 0.131809, acc 0.95\n",
      "2018-01-08T04:54:07.777031: step 3194, loss 0.103751, acc 0.96\n",
      "2018-01-08T04:54:07.924450: step 3195, loss 0.103166, acc 0.97\n",
      "2018-01-08T04:54:08.069625: step 3196, loss 0.113997, acc 0.97\n",
      "2018-01-08T04:54:08.219834: step 3197, loss 0.119739, acc 0.95\n",
      "2018-01-08T04:54:08.365064: step 3198, loss 0.126778, acc 0.96\n",
      "2018-01-08T04:54:08.514350: step 3199, loss 0.124934, acc 0.98\n",
      "2018-01-08T04:54:08.662151: step 3200, loss 0.134771, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:54:08.976262: step 3200, loss 0.241406, acc 0.859479\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3200\n",
      "\n",
      "2018-01-08T04:54:09.195565: step 3201, loss 0.118069, acc 0.96\n",
      "2018-01-08T04:54:09.343179: step 3202, loss 0.115409, acc 0.95\n",
      "2018-01-08T04:54:09.486862: step 3203, loss 0.0919142, acc 0.98\n",
      "2018-01-08T04:54:09.630388: step 3204, loss 0.126299, acc 0.95\n",
      "2018-01-08T04:54:09.777730: step 3205, loss 0.117704, acc 0.96\n",
      "2018-01-08T04:54:09.916981: step 3206, loss 0.11447, acc 0.95\n",
      "2018-01-08T04:54:10.056584: step 3207, loss 0.141192, acc 0.95\n",
      "2018-01-08T04:54:10.200001: step 3208, loss 0.113327, acc 0.95\n",
      "2018-01-08T04:54:10.341641: step 3209, loss 0.116195, acc 0.97\n",
      "2018-01-08T04:54:10.478272: step 3210, loss 0.126602, acc 0.93\n",
      "2018-01-08T04:54:10.620370: step 3211, loss 0.133993, acc 0.95\n",
      "2018-01-08T04:54:10.758010: step 3212, loss 0.128902, acc 0.94\n",
      "2018-01-08T04:54:10.901519: step 3213, loss 0.0955546, acc 0.96\n",
      "2018-01-08T04:54:11.038256: step 3214, loss 0.101867, acc 0.96\n",
      "2018-01-08T04:54:11.179939: step 3215, loss 0.134455, acc 0.94\n",
      "2018-01-08T04:54:11.317031: step 3216, loss 0.128553, acc 0.97\n",
      "2018-01-08T04:54:11.452135: step 3217, loss 0.0837014, acc 0.98\n",
      "2018-01-08T04:54:11.598886: step 3218, loss 0.103531, acc 0.98\n",
      "2018-01-08T04:54:11.735353: step 3219, loss 0.0973347, acc 0.96\n",
      "2018-01-08T04:54:11.883202: step 3220, loss 0.142773, acc 0.92\n",
      "2018-01-08T04:54:12.022175: step 3221, loss 0.13044, acc 0.93\n",
      "2018-01-08T04:54:12.159701: step 3222, loss 0.0956213, acc 0.97\n",
      "2018-01-08T04:54:12.297431: step 3223, loss 0.16015, acc 0.91\n",
      "2018-01-08T04:54:12.438885: step 3224, loss 0.115177, acc 0.98\n",
      "2018-01-08T04:54:12.574304: step 3225, loss 0.099571, acc 0.96\n",
      "2018-01-08T04:54:12.714466: step 3226, loss 0.104229, acc 0.97\n",
      "2018-01-08T04:54:12.861592: step 3227, loss 0.0778109, acc 0.99\n",
      "2018-01-08T04:54:13.001328: step 3228, loss 0.106014, acc 0.97\n",
      "2018-01-08T04:54:13.145383: step 3229, loss 0.134645, acc 0.94\n",
      "2018-01-08T04:54:13.292532: step 3230, loss 0.0947146, acc 0.98\n",
      "2018-01-08T04:54:13.431442: step 3231, loss 0.103246, acc 0.95\n",
      "2018-01-08T04:54:13.571106: step 3232, loss 0.103738, acc 0.97\n",
      "2018-01-08T04:54:13.711316: step 3233, loss 0.0946293, acc 0.97\n",
      "2018-01-08T04:54:13.860309: step 3234, loss 0.115225, acc 0.96\n",
      "2018-01-08T04:54:14.001154: step 3235, loss 0.153941, acc 0.95\n",
      "2018-01-08T04:54:14.149981: step 3236, loss 0.0844918, acc 0.98\n",
      "2018-01-08T04:54:14.299480: step 3237, loss 0.0907767, acc 0.98\n",
      "2018-01-08T04:54:14.442500: step 3238, loss 0.0905664, acc 0.98\n",
      "2018-01-08T04:54:14.589154: step 3239, loss 0.103432, acc 0.97\n",
      "2018-01-08T04:54:14.733997: step 3240, loss 0.128638, acc 0.95\n",
      "2018-01-08T04:54:14.885286: step 3241, loss 0.123499, acc 0.96\n",
      "2018-01-08T04:54:15.029020: step 3242, loss 0.10846, acc 0.95\n",
      "2018-01-08T04:54:15.172694: step 3243, loss 0.117338, acc 0.96\n",
      "2018-01-08T04:54:15.316494: step 3244, loss 0.100156, acc 0.98\n",
      "2018-01-08T04:54:15.455427: step 3245, loss 0.101847, acc 0.96\n",
      "2018-01-08T04:54:15.599676: step 3246, loss 0.114968, acc 0.96\n",
      "2018-01-08T04:54:15.743297: step 3247, loss 0.10962, acc 0.95\n",
      "2018-01-08T04:54:15.890049: step 3248, loss 0.108054, acc 0.97\n",
      "2018-01-08T04:54:16.034483: step 3249, loss 0.131722, acc 0.95\n",
      "2018-01-08T04:54:16.188654: step 3250, loss 0.0872855, acc 0.98\n",
      "2018-01-08T04:54:16.330503: step 3251, loss 0.108526, acc 0.97\n",
      "2018-01-08T04:54:16.476234: step 3252, loss 0.0764585, acc 1\n",
      "2018-01-08T04:54:16.626707: step 3253, loss 0.124907, acc 0.98\n",
      "2018-01-08T04:54:16.774802: step 3254, loss 0.086539, acc 0.99\n",
      "2018-01-08T04:54:16.934374: step 3255, loss 0.125375, acc 0.97\n",
      "2018-01-08T04:54:17.083917: step 3256, loss 0.156278, acc 0.93\n",
      "2018-01-08T04:54:17.229404: step 3257, loss 0.101884, acc 0.96\n",
      "2018-01-08T04:54:17.377086: step 3258, loss 0.0973456, acc 0.95\n",
      "2018-01-08T04:54:17.521869: step 3259, loss 0.096678, acc 0.98\n",
      "2018-01-08T04:54:17.668631: step 3260, loss 0.115269, acc 0.96\n",
      "2018-01-08T04:54:17.821357: step 3261, loss 0.134236, acc 0.93\n",
      "2018-01-08T04:54:18.049359: step 3262, loss 0.120253, acc 0.94\n",
      "2018-01-08T04:54:18.281137: step 3263, loss 0.123802, acc 0.95\n",
      "2018-01-08T04:54:18.500982: step 3264, loss 0.107951, acc 0.99\n",
      "2018-01-08T04:54:18.728540: step 3265, loss 0.111746, acc 0.99\n",
      "2018-01-08T04:54:18.968988: step 3266, loss 0.0778163, acc 1\n",
      "2018-01-08T04:54:19.204239: step 3267, loss 0.0739418, acc 0.98\n",
      "2018-01-08T04:54:19.422535: step 3268, loss 0.139976, acc 0.92\n",
      "2018-01-08T04:54:19.641076: step 3269, loss 0.0939644, acc 0.99\n",
      "2018-01-08T04:54:19.865914: step 3270, loss 0.0889532, acc 0.99\n",
      "2018-01-08T04:54:20.092904: step 3271, loss 0.114537, acc 0.94\n",
      "2018-01-08T04:54:20.311092: step 3272, loss 0.137415, acc 0.94\n",
      "2018-01-08T04:54:20.532222: step 3273, loss 0.106619, acc 0.98\n",
      "2018-01-08T04:54:20.757553: step 3274, loss 0.12727, acc 0.93\n",
      "2018-01-08T04:54:21.015015: step 3275, loss 0.142458, acc 0.93\n",
      "2018-01-08T04:54:21.242071: step 3276, loss 0.0799211, acc 0.98\n",
      "2018-01-08T04:54:21.468620: step 3277, loss 0.115737, acc 0.95\n",
      "2018-01-08T04:54:21.701028: step 3278, loss 0.106056, acc 0.96\n",
      "2018-01-08T04:54:21.897304: step 3279, loss 0.114739, acc 0.96\n",
      "2018-01-08T04:54:22.061479: step 3280, loss 0.139632, acc 0.92\n",
      "2018-01-08T04:54:22.219264: step 3281, loss 0.108999, acc 0.95\n",
      "2018-01-08T04:54:22.373817: step 3282, loss 0.131924, acc 0.94\n",
      "2018-01-08T04:54:22.535637: step 3283, loss 0.129439, acc 0.94\n",
      "2018-01-08T04:54:22.696364: step 3284, loss 0.111718, acc 0.96\n",
      "2018-01-08T04:54:22.852457: step 3285, loss 0.140878, acc 0.94\n",
      "2018-01-08T04:54:23.017980: step 3286, loss 0.119022, acc 0.96\n",
      "2018-01-08T04:54:23.175956: step 3287, loss 0.116189, acc 0.94\n",
      "2018-01-08T04:54:23.334955: step 3288, loss 0.0917748, acc 0.98\n",
      "2018-01-08T04:54:23.490163: step 3289, loss 0.120207, acc 0.93\n",
      "2018-01-08T04:54:23.651627: step 3290, loss 0.130902, acc 0.94\n",
      "2018-01-08T04:54:23.813667: step 3291, loss 0.101085, acc 0.98\n",
      "2018-01-08T04:54:23.974501: step 3292, loss 0.136283, acc 0.95\n",
      "2018-01-08T04:54:24.132821: step 3293, loss 0.0940081, acc 0.97\n",
      "2018-01-08T04:54:24.293333: step 3294, loss 0.106638, acc 0.97\n",
      "2018-01-08T04:54:24.452538: step 3295, loss 0.0897407, acc 0.98\n",
      "2018-01-08T04:54:24.622229: step 3296, loss 0.0975284, acc 0.97\n",
      "2018-01-08T04:54:24.781895: step 3297, loss 0.125719, acc 0.94\n",
      "2018-01-08T04:54:24.937805: step 3298, loss 0.0904429, acc 0.98\n",
      "2018-01-08T04:54:25.097747: step 3299, loss 0.0951178, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:54:25.259816: step 3300, loss 0.108708, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:54:25.573625: step 3300, loss 0.2425, acc 0.859479\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3300\n",
      "\n",
      "2018-01-08T04:54:25.805311: step 3301, loss 0.12891, acc 0.97\n",
      "2018-01-08T04:54:25.956668: step 3302, loss 0.10014, acc 0.99\n",
      "2018-01-08T04:54:26.114081: step 3303, loss 0.135915, acc 0.94\n",
      "2018-01-08T04:54:26.257906: step 3304, loss 0.141377, acc 0.94\n",
      "2018-01-08T04:54:26.401682: step 3305, loss 0.116475, acc 0.96\n",
      "2018-01-08T04:54:26.551433: step 3306, loss 0.123084, acc 0.94\n",
      "2018-01-08T04:54:26.697531: step 3307, loss 0.107172, acc 0.98\n",
      "2018-01-08T04:54:26.845029: step 3308, loss 0.134796, acc 0.95\n",
      "2018-01-08T04:54:26.990103: step 3309, loss 0.112542, acc 0.96\n",
      "2018-01-08T04:54:27.139751: step 3310, loss 0.107941, acc 0.96\n",
      "2018-01-08T04:54:27.288708: step 3311, loss 0.0720159, acc 1\n",
      "2018-01-08T04:54:27.428888: step 3312, loss 0.11245, acc 0.95\n",
      "2018-01-08T04:54:27.572432: step 3313, loss 0.133559, acc 0.94\n",
      "2018-01-08T04:54:27.719901: step 3314, loss 0.164297, acc 0.91\n",
      "2018-01-08T04:54:27.874007: step 3315, loss 0.0931723, acc 0.97\n",
      "2018-01-08T04:54:28.018363: step 3316, loss 0.113845, acc 0.95\n",
      "2018-01-08T04:54:28.165872: step 3317, loss 0.136876, acc 0.91\n",
      "2018-01-08T04:54:28.317492: step 3318, loss 0.107548, acc 0.98\n",
      "2018-01-08T04:54:28.469179: step 3319, loss 0.101666, acc 0.96\n",
      "2018-01-08T04:54:28.613449: step 3320, loss 0.126933, acc 0.97\n",
      "2018-01-08T04:54:28.761022: step 3321, loss 0.127277, acc 0.95\n",
      "2018-01-08T04:54:28.910010: step 3322, loss 0.109815, acc 0.96\n",
      "2018-01-08T04:54:29.057921: step 3323, loss 0.0973648, acc 0.98\n",
      "2018-01-08T04:54:29.203311: step 3324, loss 0.138893, acc 0.93\n",
      "2018-01-08T04:54:29.340345: step 3325, loss 0.123032, acc 0.95\n",
      "2018-01-08T04:54:29.482900: step 3326, loss 0.139995, acc 0.94\n",
      "2018-01-08T04:54:29.630395: step 3327, loss 0.0916134, acc 0.97\n",
      "2018-01-08T04:54:29.776082: step 3328, loss 0.129249, acc 0.95\n",
      "2018-01-08T04:54:29.916096: step 3329, loss 0.0900515, acc 0.97\n",
      "2018-01-08T04:54:30.056214: step 3330, loss 0.116185, acc 0.96\n",
      "2018-01-08T04:54:30.200014: step 3331, loss 0.098556, acc 0.99\n",
      "2018-01-08T04:54:30.343224: step 3332, loss 0.0822403, acc 0.99\n",
      "2018-01-08T04:54:30.482052: step 3333, loss 0.112594, acc 0.97\n",
      "2018-01-08T04:54:30.620805: step 3334, loss 0.10363, acc 0.98\n",
      "2018-01-08T04:54:30.764524: step 3335, loss 0.108448, acc 0.97\n",
      "2018-01-08T04:54:30.913500: step 3336, loss 0.0878998, acc 0.98\n",
      "2018-01-08T04:54:31.059299: step 3337, loss 0.120804, acc 0.94\n",
      "2018-01-08T04:54:31.194824: step 3338, loss 0.112274, acc 0.97\n",
      "2018-01-08T04:54:31.330003: step 3339, loss 0.100883, acc 0.97\n",
      "2018-01-08T04:54:31.462887: step 3340, loss 0.112034, acc 0.96\n",
      "2018-01-08T04:54:31.608677: step 3341, loss 0.0811025, acc 0.98\n",
      "2018-01-08T04:54:31.745352: step 3342, loss 0.12766, acc 0.95\n",
      "2018-01-08T04:54:31.887972: step 3343, loss 0.103115, acc 0.96\n",
      "2018-01-08T04:54:32.028363: step 3344, loss 0.121402, acc 0.95\n",
      "2018-01-08T04:54:32.179882: step 3345, loss 0.113457, acc 0.96\n",
      "2018-01-08T04:54:32.321424: step 3346, loss 0.105237, acc 0.96\n",
      "2018-01-08T04:54:32.465384: step 3347, loss 0.114357, acc 0.95\n",
      "2018-01-08T04:54:32.605789: step 3348, loss 0.107196, acc 0.97\n",
      "2018-01-08T04:54:32.747572: step 3349, loss 0.119919, acc 0.94\n",
      "2018-01-08T04:54:32.891161: step 3350, loss 0.0974475, acc 0.97\n",
      "2018-01-08T04:54:33.028528: step 3351, loss 0.100526, acc 0.95\n",
      "2018-01-08T04:54:33.171104: step 3352, loss 0.125535, acc 0.94\n",
      "2018-01-08T04:54:33.302275: step 3353, loss 0.132203, acc 0.94\n",
      "2018-01-08T04:54:33.443710: step 3354, loss 0.0801012, acc 0.98\n",
      "2018-01-08T04:54:33.581121: step 3355, loss 0.112087, acc 0.94\n",
      "2018-01-08T04:54:33.719955: step 3356, loss 0.0925299, acc 0.98\n",
      "2018-01-08T04:54:33.871135: step 3357, loss 0.144924, acc 0.94\n",
      "2018-01-08T04:54:34.013885: step 3358, loss 0.159926, acc 0.95\n",
      "2018-01-08T04:54:34.162301: step 3359, loss 0.124893, acc 0.94\n",
      "2018-01-08T04:54:34.305127: step 3360, loss 0.148263, acc 0.94\n",
      "2018-01-08T04:54:34.451173: step 3361, loss 0.0897168, acc 0.98\n",
      "2018-01-08T04:54:34.591927: step 3362, loss 0.0942705, acc 0.97\n",
      "2018-01-08T04:54:34.680500: step 3363, loss 0.104998, acc 0.954545\n",
      "2018-01-08T04:54:34.828218: step 3364, loss 0.149962, acc 0.92\n",
      "2018-01-08T04:54:34.969362: step 3365, loss 0.0947433, acc 0.97\n",
      "2018-01-08T04:54:35.120620: step 3366, loss 0.120344, acc 0.92\n",
      "2018-01-08T04:54:35.269114: step 3367, loss 0.111493, acc 0.96\n",
      "2018-01-08T04:54:35.408145: step 3368, loss 0.118699, acc 0.95\n",
      "2018-01-08T04:54:35.554692: step 3369, loss 0.0990444, acc 0.96\n",
      "2018-01-08T04:54:35.698006: step 3370, loss 0.0716758, acc 1\n",
      "2018-01-08T04:54:35.842431: step 3371, loss 0.0902231, acc 0.98\n",
      "2018-01-08T04:54:35.990356: step 3372, loss 0.100648, acc 0.97\n",
      "2018-01-08T04:54:36.136979: step 3373, loss 0.116836, acc 0.96\n",
      "2018-01-08T04:54:36.280413: step 3374, loss 0.0907526, acc 0.97\n",
      "2018-01-08T04:54:36.417761: step 3375, loss 0.0667839, acc 1\n",
      "2018-01-08T04:54:36.569354: step 3376, loss 0.0987473, acc 0.97\n",
      "2018-01-08T04:54:36.712468: step 3377, loss 0.0830204, acc 0.96\n",
      "2018-01-08T04:54:36.859641: step 3378, loss 0.112101, acc 0.95\n",
      "2018-01-08T04:54:37.010857: step 3379, loss 0.102669, acc 0.96\n",
      "2018-01-08T04:54:37.154953: step 3380, loss 0.105601, acc 0.96\n",
      "2018-01-08T04:54:37.303592: step 3381, loss 0.08752, acc 0.99\n",
      "2018-01-08T04:54:37.445082: step 3382, loss 0.136563, acc 0.92\n",
      "2018-01-08T04:54:37.593934: step 3383, loss 0.134209, acc 0.96\n",
      "2018-01-08T04:54:37.743365: step 3384, loss 0.106024, acc 0.98\n",
      "2018-01-08T04:54:37.937363: step 3385, loss 0.0861322, acc 0.99\n",
      "2018-01-08T04:54:38.156964: step 3386, loss 0.119602, acc 0.96\n",
      "2018-01-08T04:54:38.382316: step 3387, loss 0.0880429, acc 0.96\n",
      "2018-01-08T04:54:38.617198: step 3388, loss 0.0801196, acc 0.98\n",
      "2018-01-08T04:54:38.839849: step 3389, loss 0.10744, acc 0.96\n",
      "2018-01-08T04:54:39.069151: step 3390, loss 0.0984892, acc 0.96\n",
      "2018-01-08T04:54:39.299690: step 3391, loss 0.0990979, acc 0.98\n",
      "2018-01-08T04:54:39.513114: step 3392, loss 0.124682, acc 0.97\n",
      "2018-01-08T04:54:39.745915: step 3393, loss 0.100338, acc 0.96\n",
      "2018-01-08T04:54:39.973386: step 3394, loss 0.125614, acc 0.95\n",
      "2018-01-08T04:54:40.197605: step 3395, loss 0.0818833, acc 0.98\n",
      "2018-01-08T04:54:40.424185: step 3396, loss 0.089595, acc 0.98\n",
      "2018-01-08T04:54:40.650594: step 3397, loss 0.138604, acc 0.96\n",
      "2018-01-08T04:54:40.887135: step 3398, loss 0.135577, acc 0.95\n",
      "2018-01-08T04:54:41.109553: step 3399, loss 0.100205, acc 0.97\n",
      "2018-01-08T04:54:41.344467: step 3400, loss 0.0778479, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:54:41.741447: step 3400, loss 0.24104, acc 0.862034\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3400\n",
      "\n",
      "2018-01-08T04:54:42.007591: step 3401, loss 0.124724, acc 0.94\n",
      "2018-01-08T04:54:42.164270: step 3402, loss 0.0930581, acc 0.96\n",
      "2018-01-08T04:54:42.326037: step 3403, loss 0.0829609, acc 0.97\n",
      "2018-01-08T04:54:42.487474: step 3404, loss 0.106392, acc 0.95\n",
      "2018-01-08T04:54:42.656628: step 3405, loss 0.106121, acc 0.96\n",
      "2018-01-08T04:54:42.819954: step 3406, loss 0.103663, acc 0.97\n",
      "2018-01-08T04:54:42.980286: step 3407, loss 0.0858989, acc 0.97\n",
      "2018-01-08T04:54:43.178152: step 3408, loss 0.136494, acc 0.95\n",
      "2018-01-08T04:54:43.347377: step 3409, loss 0.14741, acc 0.92\n",
      "2018-01-08T04:54:43.502148: step 3410, loss 0.0780274, acc 0.99\n",
      "2018-01-08T04:54:43.662132: step 3411, loss 0.0918886, acc 0.99\n",
      "2018-01-08T04:54:43.826110: step 3412, loss 0.100526, acc 0.98\n",
      "2018-01-08T04:54:43.987197: step 3413, loss 0.102203, acc 0.97\n",
      "2018-01-08T04:54:44.149773: step 3414, loss 0.127231, acc 0.95\n",
      "2018-01-08T04:54:44.313453: step 3415, loss 0.0709904, acc 1\n",
      "2018-01-08T04:54:44.472745: step 3416, loss 0.133118, acc 0.93\n",
      "2018-01-08T04:54:44.632318: step 3417, loss 0.0900153, acc 0.99\n",
      "2018-01-08T04:54:44.787009: step 3418, loss 0.121173, acc 0.91\n",
      "2018-01-08T04:54:44.952253: step 3419, loss 0.108357, acc 0.95\n",
      "2018-01-08T04:54:45.116407: step 3420, loss 0.0817412, acc 0.98\n",
      "2018-01-08T04:54:45.283495: step 3421, loss 0.0918252, acc 0.98\n",
      "2018-01-08T04:54:45.437124: step 3422, loss 0.0927344, acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-08T04:54:45.595816: step 3423, loss 0.1265, acc 0.97\n",
      "2018-01-08T04:54:45.754016: step 3424, loss 0.122018, acc 0.95\n",
      "2018-01-08T04:54:45.914178: step 3425, loss 0.0859995, acc 0.96\n",
      "2018-01-08T04:54:46.062896: step 3426, loss 0.0820111, acc 0.97\n",
      "2018-01-08T04:54:46.213379: step 3427, loss 0.0770813, acc 0.99\n",
      "2018-01-08T04:54:46.360369: step 3428, loss 0.107751, acc 0.95\n",
      "2018-01-08T04:54:46.509881: step 3429, loss 0.108133, acc 0.97\n",
      "2018-01-08T04:54:46.659472: step 3430, loss 0.145213, acc 0.93\n",
      "2018-01-08T04:54:46.812021: step 3431, loss 0.0897287, acc 0.96\n",
      "2018-01-08T04:54:46.963381: step 3432, loss 0.0760395, acc 1\n",
      "2018-01-08T04:54:47.109608: step 3433, loss 0.0928793, acc 0.97\n",
      "2018-01-08T04:54:47.256482: step 3434, loss 0.0942939, acc 0.98\n",
      "2018-01-08T04:54:47.398079: step 3435, loss 0.082912, acc 0.98\n",
      "2018-01-08T04:54:47.545632: step 3436, loss 0.100665, acc 0.98\n",
      "2018-01-08T04:54:47.693403: step 3437, loss 0.120956, acc 0.95\n",
      "2018-01-08T04:54:47.843253: step 3438, loss 0.11144, acc 0.95\n",
      "2018-01-08T04:54:47.991023: step 3439, loss 0.128251, acc 0.94\n",
      "2018-01-08T04:54:48.139121: step 3440, loss 0.0978968, acc 0.99\n",
      "2018-01-08T04:54:48.287600: step 3441, loss 0.141324, acc 0.93\n",
      "2018-01-08T04:54:48.428106: step 3442, loss 0.154128, acc 0.95\n",
      "2018-01-08T04:54:48.577598: step 3443, loss 0.0741392, acc 0.97\n",
      "2018-01-08T04:54:48.719489: step 3444, loss 0.118909, acc 0.97\n",
      "2018-01-08T04:54:48.858964: step 3445, loss 0.115551, acc 0.96\n",
      "2018-01-08T04:54:49.004797: step 3446, loss 0.100333, acc 0.94\n",
      "2018-01-08T04:54:49.153300: step 3447, loss 0.102833, acc 0.98\n",
      "2018-01-08T04:54:49.297641: step 3448, loss 0.107747, acc 0.95\n",
      "2018-01-08T04:54:49.444430: step 3449, loss 0.120616, acc 0.95\n",
      "2018-01-08T04:54:49.589729: step 3450, loss 0.120452, acc 0.95\n",
      "2018-01-08T04:54:49.740096: step 3451, loss 0.0710973, acc 0.98\n",
      "2018-01-08T04:54:49.896788: step 3452, loss 0.0791217, acc 0.97\n",
      "2018-01-08T04:54:50.060536: step 3453, loss 0.133472, acc 0.96\n",
      "2018-01-08T04:54:50.223503: step 3454, loss 0.119979, acc 0.94\n",
      "2018-01-08T04:54:50.382502: step 3455, loss 0.0999061, acc 0.98\n",
      "2018-01-08T04:54:50.543794: step 3456, loss 0.0937265, acc 0.96\n",
      "2018-01-08T04:54:50.710311: step 3457, loss 0.117723, acc 0.97\n",
      "2018-01-08T04:54:50.872176: step 3458, loss 0.132147, acc 0.96\n",
      "2018-01-08T04:54:51.038949: step 3459, loss 0.0981233, acc 0.98\n",
      "2018-01-08T04:54:51.197229: step 3460, loss 0.112869, acc 0.96\n",
      "2018-01-08T04:54:51.353411: step 3461, loss 0.0741394, acc 1\n",
      "2018-01-08T04:54:51.519494: step 3462, loss 0.0887742, acc 0.98\n",
      "2018-01-08T04:54:51.676997: step 3463, loss 0.100099, acc 0.98\n",
      "2018-01-08T04:54:51.835717: step 3464, loss 0.103054, acc 0.96\n",
      "2018-01-08T04:54:51.996345: step 3465, loss 0.0972663, acc 0.97\n",
      "2018-01-08T04:54:52.154588: step 3466, loss 0.120455, acc 0.94\n",
      "2018-01-08T04:54:52.311132: step 3467, loss 0.116849, acc 0.96\n",
      "2018-01-08T04:54:52.469192: step 3468, loss 0.0947279, acc 0.97\n",
      "2018-01-08T04:54:52.634931: step 3469, loss 0.138862, acc 0.94\n",
      "2018-01-08T04:54:52.789564: step 3470, loss 0.0955376, acc 0.97\n",
      "2018-01-08T04:54:52.958487: step 3471, loss 0.0999187, acc 0.96\n",
      "2018-01-08T04:54:53.114850: step 3472, loss 0.0928629, acc 0.98\n",
      "2018-01-08T04:54:53.276521: step 3473, loss 0.0829832, acc 0.98\n",
      "2018-01-08T04:54:53.432792: step 3474, loss 0.0813578, acc 0.97\n",
      "2018-01-08T04:54:53.588249: step 3475, loss 0.105454, acc 0.98\n",
      "2018-01-08T04:54:53.744681: step 3476, loss 0.0963024, acc 0.97\n",
      "2018-01-08T04:54:53.898081: step 3477, loss 0.108305, acc 0.96\n",
      "2018-01-08T04:54:54.046785: step 3478, loss 0.0907266, acc 0.98\n",
      "2018-01-08T04:54:54.191553: step 3479, loss 0.0848983, acc 1\n",
      "2018-01-08T04:54:54.344344: step 3480, loss 0.112547, acc 0.94\n",
      "2018-01-08T04:54:54.530420: step 3481, loss 0.111879, acc 0.98\n",
      "2018-01-08T04:54:54.678527: step 3482, loss 0.102499, acc 0.98\n",
      "2018-01-08T04:54:54.835198: step 3483, loss 0.105733, acc 0.97\n",
      "2018-01-08T04:54:54.981828: step 3484, loss 0.11201, acc 0.96\n",
      "2018-01-08T04:54:55.132300: step 3485, loss 0.107929, acc 0.96\n",
      "2018-01-08T04:54:55.276525: step 3486, loss 0.12837, acc 0.93\n",
      "2018-01-08T04:54:55.418502: step 3487, loss 0.117248, acc 0.95\n",
      "2018-01-08T04:54:55.564723: step 3488, loss 0.0811455, acc 0.98\n",
      "2018-01-08T04:54:55.710687: step 3489, loss 0.110107, acc 0.95\n",
      "2018-01-08T04:54:55.857099: step 3490, loss 0.0954833, acc 0.97\n",
      "2018-01-08T04:54:56.004074: step 3491, loss 0.104892, acc 0.95\n",
      "2018-01-08T04:54:56.154635: step 3492, loss 0.0811055, acc 0.98\n",
      "2018-01-08T04:54:56.294788: step 3493, loss 0.0945176, acc 0.95\n",
      "2018-01-08T04:54:56.437764: step 3494, loss 0.130063, acc 0.93\n",
      "2018-01-08T04:54:56.584410: step 3495, loss 0.109331, acc 0.96\n",
      "2018-01-08T04:54:56.737903: step 3496, loss 0.0916655, acc 0.97\n",
      "2018-01-08T04:54:56.889674: step 3497, loss 0.127704, acc 0.94\n",
      "2018-01-08T04:54:57.030879: step 3498, loss 0.078706, acc 0.99\n",
      "2018-01-08T04:54:57.177518: step 3499, loss 0.0891567, acc 0.98\n",
      "2018-01-08T04:54:57.321676: step 3500, loss 0.121338, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-01-08T04:54:57.616983: step 3500, loss 0.240782, acc 0.862034\n",
      "\n",
      "Saved model checkpoint to /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3500\n",
      "\n",
      "2018-01-08T04:54:57.832016: step 3501, loss 0.124233, acc 0.95\n",
      "2018-01-08T04:54:57.968904: step 3502, loss 0.0701612, acc 0.98\n",
      "2018-01-08T04:54:58.104470: step 3503, loss 0.116332, acc 0.96\n",
      "2018-01-08T04:54:58.241006: step 3504, loss 0.106291, acc 0.96\n",
      "2018-01-08T04:54:58.376778: step 3505, loss 0.105012, acc 0.97\n",
      "2018-01-08T04:54:58.523557: step 3506, loss 0.116894, acc 0.93\n",
      "2018-01-08T04:54:58.666970: step 3507, loss 0.123536, acc 0.95\n",
      "2018-01-08T04:54:58.816248: step 3508, loss 0.109719, acc 0.97\n",
      "2018-01-08T04:54:58.957837: step 3509, loss 0.118176, acc 0.92\n",
      "2018-01-08T04:54:59.096770: step 3510, loss 0.133411, acc 0.93\n",
      "2018-01-08T04:54:59.237256: step 3511, loss 0.0896041, acc 0.97\n",
      "2018-01-08T04:54:59.374115: step 3512, loss 0.0750072, acc 0.99\n",
      "2018-01-08T04:54:59.508062: step 3513, loss 0.159786, acc 0.9\n",
      "2018-01-08T04:54:59.645249: step 3514, loss 0.0874672, acc 0.97\n",
      "2018-01-08T04:54:59.782196: step 3515, loss 0.105798, acc 0.96\n",
      "2018-01-08T04:54:59.919071: step 3516, loss 0.130931, acc 0.95\n",
      "2018-01-08T04:55:00.062206: step 3517, loss 0.117766, acc 0.96\n",
      "2018-01-08T04:55:00.200358: step 3518, loss 0.0816291, acc 0.98\n",
      "2018-01-08T04:55:00.343841: step 3519, loss 0.104212, acc 0.97\n",
      "2018-01-08T04:55:00.478334: step 3520, loss 0.0807247, acc 0.99\n",
      "2018-01-08T04:55:00.618813: step 3521, loss 0.112415, acc 0.94\n",
      "2018-01-08T04:55:00.759268: step 3522, loss 0.11023, acc 0.97\n",
      "2018-01-08T04:55:00.909288: step 3523, loss 0.0642009, acc 0.99\n",
      "2018-01-08T04:55:01.059485: step 3524, loss 0.0975498, acc 0.97\n",
      "2018-01-08T04:55:01.201190: step 3525, loss 0.107875, acc 0.96\n",
      "2018-01-08T04:55:01.339356: step 3526, loss 0.134881, acc 0.91\n",
      "2018-01-08T04:55:01.472985: step 3527, loss 0.113387, acc 0.95\n",
      "2018-01-08T04:55:01.612076: step 3528, loss 0.0919165, acc 0.97\n",
      "2018-01-08T04:55:01.749715: step 3529, loss 0.106833, acc 0.96\n",
      "2018-01-08T04:55:01.893783: step 3530, loss 0.0847812, acc 0.99\n",
      "2018-01-08T04:55:02.043176: step 3531, loss 0.113471, acc 0.94\n",
      "2018-01-08T04:55:02.190501: step 3532, loss 0.0873259, acc 0.97\n",
      "2018-01-08T04:55:02.341396: step 3533, loss 0.101895, acc 0.98\n",
      "2018-01-08T04:55:02.493895: step 3534, loss 0.14708, acc 0.92\n",
      "2018-01-08T04:55:02.643905: step 3535, loss 0.0854409, acc 0.98\n",
      "2018-01-08T04:55:02.795860: step 3536, loss 0.079706, acc 0.98\n",
      "2018-01-08T04:55:02.950653: step 3537, loss 0.105848, acc 0.98\n",
      "2018-01-08T04:55:03.094647: step 3538, loss 0.135592, acc 0.92\n",
      "2018-01-08T04:55:03.244040: step 3539, loss 0.112342, acc 0.94\n",
      "2018-01-08T04:55:03.333143: step 3540, loss 0.104962, acc 0.954545\n"
     ]
    }
   ],
   "source": [
    "# reset function if not below 1 before 50\n",
    "# maybe just run 5 times per parameter change\n",
    "# Training\n",
    "# ============================\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextDNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "                \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # add print epoch\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/hi/Documents/Nanodegree/machine-learning-master/cnn-text-classification-tf_OG/runs/1515404745/checkpoints/model-3500\n",
      "Saving evaluation to submission.csv\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.008586  0.000546  0.990868\n",
      "1  id24541  0.999425  0.000572  0.000004\n",
      "2  id00134  0.029484  0.970295  0.000221\n",
      "3  id27757  0.992481  0.006712  0.000807\n",
      "4  id04081  0.961886  0.005635  0.032479\n"
     ]
    }
   ],
   "source": [
    "# Map data into vocabulary\n",
    "#print(FLAGS.checkpoint_dir)\n",
    "#vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "#print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "#print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "#vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_path = os.path.join(checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "#checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        #### Tensors we want to evaluate\n",
    "        scores = graph.get_operation_by_name(\"output/scores\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "        all_probabilities = None\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions_scores = sess.run([predictions,scores], {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions_scores[0]])\n",
    "            probabilities = softmax(batch_predictions_scores[1])\n",
    "            if all_probabilities is not None:\n",
    "                all_probabilities = np.concatenate([all_probabilities, probabilities])\n",
    "            else:\n",
    "                all_probabilities = probabilities\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df_ids = pd.DataFrame(x_ids, columns = ['id'])\n",
    "df_probs = pd.DataFrame(all_probabilities, columns = ['EAP','HPL','MWS'])\n",
    "df_sub = pd.concat([df_ids,df_probs], axis =1, join='inner')\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"\", \"submission.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "df_sub.to_csv(out_path, index = False)\n",
    "print(df_sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
